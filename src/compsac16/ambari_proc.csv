issue_id,summary,description,summary_proc,description_proc,Surprising,Dormant,Blocker,Security,Performance,Breakage
12,Add additional transition states, add addit transit state,For running a server (daemon)  the current states are: START and STARTED. It would be nice to have transition state: STARTING  and STOPPING., for run server daemon current state start start it would nice transit state start stop,0,0,0,0,0,0,
46,Preserve cluster id  blueprint name  and blueprint revision in agent local disk, preserv cluster id blueprint name blueprint revis agent local disk,After agent execute commands on behave of the controller  agent should store cluster id  blueprint name and blueprint revision in the local disk to keep track of the current deployment state on the agent., after agent execut command behav control agent store cluster id blueprint name blueprint revis local disk keep track current deploy state agent,0,0,0,0,0,0,
92,Agent should retry heartbeat message  if controller did not receive the heartbeat, agent retri heartbeat messag control receiv heartbeat,If the heartbeat was not received correctly by the controller  then it should retry., if heartbeat receiv correctli control retri,0,0,0,0,0,0,
103,Remove agent entities beans from public schema xsd, remov agent entiti bean public schema xsd,Agent entities beans are located ambari-client: org.apache.ambari.common.rest.entities.agent. Hence  schema xsd generation for public rest api will include agent entities beans. We should move it to org.apache.ambari.common.rest.agent to exclude agent entities to expose to public., agent entiti bean locat ambari client org apach ambari common rest entiti agent henc schema xsd gener public rest api includ agent entiti bean we move org apach ambari common rest agent exclud agent entiti expos public,0,0,0,0,0,0,
192,Check for NN safemode during restarts, check nn safemod restart,There is no checks for safemode when we reconfigure the cluster., there check safemod reconfigur cluster,1,0,0,0,0,0,
198,Dependency of templeton on hcat client, depend templeton hcat client,hcat client should be installed at the templeton server node., hcat client instal templeton server node,1,0,0,0,0,0,
199,Remove import of mysql puppet module from manifest., remov import mysql puppet modul manifest,Remove import of mysql puppet module from manifest because this module is deprecated., remov import mysql puppet modul manifest modul deprec,1,0,0,0,0,0,
202,Add check to verify jdk path after install, add check verifi jdk path instal,After the jdk install  we do not validate the path. This causes problems during service start in later stages., after jdk instal valid path thi caus problem servic start later stage,1,0,0,0,0,0,
207,PHP Notice: Undefined variable: manifest in /usr/share/hmc/php/puppet/genmanifest/hostsConfig.php, php notic undefin variabl manifest usr share hmc php puppet genmanifest host config php,Undefined variable used., undefin variabl use,1,0,0,0,0,0,
208,Support filtering hosts based on discovery status, support filter host base discoveri statu,Api to get hosts should allow filtering out bad hosts, api get host allow filter bad host,0,0,0,0,0,0,
217,Alert table on the right needs to be tied visually/verbally to the context/content it is displaying, alert tabl right need tie visual verbal context content display,Currently in Dashboard when service entry is clicked  right side alerts table caption does not indicate that it is showing alerts related to that service. So Cpation of the table should change indicating the corresponding service name., current dashboard servic entri click right side alert tabl caption indic show alert relat servic so cpation tabl chang indic correspond servic name,1,0,0,0,0,0,
222,Remove the word alert from all the Nagios alerts descriptions., remov word alert nagio alert descript,IT is sort of redundant.., it sort redund,1,0,0,0,0,0,
226,Make the daemon names and other field names consistent, make daemon name field name consist,Following names need to be consistent: Hdfs -&gt; HDFS Mapreduce -&gt; MapReduce Zookeeper -&gt; ZooKeeper HADOOP -&gt; Hadoop, follow name need consist hdf gt hdf mapreduc gt map reduc zookeep gt zoo keeper hadoop gt hadoop,1,0,0,0,0,0,
232,Enable LZO should show checkbox instead of text, enabl lzo show checkbox instead text,Currently the enable lzo option shows a text box that needs to be filled with true/false. Changing the UI element to checkbox., current enabl lzo option show text box need fill true fals chang ui element checkbox,1,0,0,0,0,0,
236,Increase puppet agent timeout., increas puppet agent timeout,Puppet agent timeout should be increases as sometimes (possibly due to a bug in ruby) puppet master takes long time to compile and send back the catalog., puppet agent timeout increas sometim possibl due bug rubi puppet master take long time compil send back catalog,0,0,0,0,0,0,
237,Refactor puppet kick loop to easily change retries and timeouts., refactor puppet kick loop easili chang retri timeout,Refactor puppet kick loop to easily change retries and timeouts., refactor puppet kick loop easili chang retri timeout,0,0,0,0,1,0,
245,Support data cleanup if installation fails., support data cleanup instal fail,We need to support data cleanup so that a cluster can be re-installed in case of failures., we need support data cleanup cluster instal case failur,0,0,0,0,0,0,
247,Replace index.php with clusters.php, replac index php cluster php,Like the title says: overwrite index.php with the contents of clusters.php  making sure to add a link to the AddNodesWizard as well., like titl say overwrit index php content cluster php make sure add link add node wizard well,0,0,0,0,0,0,
249,Uninstall support from UI, uninstal support ui,Uninstall/wipeout support from UI., uninstal wipeout support ui,0,0,0,0,0,0,
252,Remove 'Playground' files from HMC, remov playground file hmc,There's a bunch of (temporary playground) files that got wrongly committed to the HMC codebase  so this is to remove all of them and get things into a cleaner state (at least on the face of things)., there bunch temporari playground file got wrongli commit hmc codebas remov get thing cleaner state least face thing,0,0,0,0,0,0,
253,Support uninstall state in mysql modules, support uninstal state mysql modul,Currently  there is no support for uninstall of mysql package., current support uninstal mysql packag,0,0,0,0,0,0,
255,Rename/Relocate files as appropriate, renam reloc file appropri,There's some images in the html/ directory (that should be in images/)  there's .html files whose extension should be changed to .htmli  and such  that would be nice to clean up., there imag html directori imag html file whose extens chang htmli would nice clean,0,0,0,0,0,0,
256,Update hive config to enable authorization, updat hive config enabl author,In /etc/hive/conf/hive-site.xml&lt;property&gt; &lt;name&gt;hive.security.authorization.enabled&lt;/name&gt; &lt;value&gt;true&lt;/value&gt; &lt;description&gt;enable or disable the hive client authorization&lt;/description&gt;&lt;/property&gt;&lt;property&gt; &lt;name&gt;hive.security.authorization.manager&lt;/name&gt; &lt;value&gt;org.apache.hcatalog.security.HdfsAuthorizationProvider&lt;/value&gt; &lt;description&gt;the hive client authorization manager class name. The user defined authorization class should implement interface org.apache.hadoop.hive.ql.security.authorization.HiveAuthorizationProvider. &lt;/description&gt;&lt;/property&gt;In /etc/hive/conf/hive-env.sh  add -export HIVE_AUX_JARS_PATH=/usr/lib/hcatalog/share/hcatalog/hcatalog-0.4.0.jar, in etc hive conf hive site xml lt properti gt lt name gt hive secur author enabl lt name gt lt valu gt true lt valu gt lt descript gt enabl disabl hive client author lt descript gt lt properti gt lt properti gt lt name gt hive secur author manag lt name gt lt valu gt org apach hcatalog secur hdf author provid lt valu gt lt descript gt hive client author manag class name the user defin author class implement interfac org apach hadoop hive ql secur author hive author provid lt descript gt lt properti gt in etc hive conf hive env sh add export hive aux jar path usr lib hcatalog share hcatalog hcatalog jar,0,0,0,0,0,0,
257,Manage services section will have any empty section when no client only components installed, manag servic section empti section client compon instal,On the manage services page we have a section for client only services that have no long running processes.If the user has no such component there is a heading but no content. Maybe we can hide the heading when no such service is present., on manag servic page section client servic long run process if user compon head content mayb hide head servic present,0,0,0,0,0,0,
262,Init Wizard: Advanced Config validation errors can be bypassed, init wizard advanc config valid error bypass,Make the Naigos password (and re-type password) different so as you cause a validation error.This will let the user move on to the next screen by ignoring all other validation errors on this page., make naigo password type password differ caus valid error thi let user move next screen ignor valid error page,0,0,0,0,0,0,
264,Nagios Admin Contact should be checked to ensure it is always an email address, nagio admin contact check ensur alway email address,,,0,0,0,0,0,0,
274,Templeton data on hdfs needs to be readable by all users, templeton data hdf need readabl user,Content of /user/templeton dir-bash-3.2$ hadoop dfs -ls /user/templetonFound 4 items-rw-r--r-- 3 templeton hdfs 107373 2012-05-14 19:53 /user/templeton/hadoop-streaming.jar-rw------- 3 templeton hdfs 35352096 2012-05-14 19:53 /user/templeton/hive.tar.gz-rw------- 3 templeton hdfs 47909478 2012-05-14 19:53 /user/templeton/pig.tar.gz-rw------- 3 templeton hdfs 127652 2012-05-14 19:53 /user/templeton/ugi.jarOnly templeton user can use the jars, content user templeton dir bash hadoop df ls user templeton found item rw r r templeton hdf user templeton hadoop stream jar rw templeton hdf user templeton hive tar gz rw templeton hdf user templeton pig tar gz rw templeton hdf user templeton ugi jar onli templeton user use jar,0,0,0,0,0,0,
276,Update HDFS parameter configuration description, updat hdf paramet configur descript,Updating the short description and tooltip long description as follows:Changing the text that affects WebUI as follows: filesystem -&gt; file system HDFS Append Enabled -&gt; Append enabled HDFS WebHDFS Enabled -&gt; WebHDFS enabled Hadoop maximum Java heap size -&gt; Hadoop maximum Java heap size (MB)  Java Heap Size for slave daemons -&gt; Maximum Java heap size for daemons such as Balancer in MB (-Xmx)   NameNode maximum Java heap size -&gt; NameNode initial Java heap size (MB)  Java Heap Size for NameNode -&gt; Initial and minimum Java heap size for NameNode in MB (-Xms)   Hadoop Young Generation heap size -&gt; NameNode new generation size (MB)  Maximum size for New Generation for java heap size -&gt; Default size of Java new generation in MB for NameNode (-XX:NewSize)   DataNode Java heap size -&gt; DataNode maximum Java heap size (MB)  Java Heap Size for DataNode -&gt; Maximum Java heap size for DataNode in MB (-Xmx), updat short descript tooltip long descript follow chang text affect web ui follow filesystem gt file system hdf append enabl gt append enabl hdf web hdf enabl gt web hdf enabl hadoop maximum java heap size gt hadoop maximum java heap size mb java heap size slave daemon gt maximum java heap size daemon balanc mb xmx name node maximum java heap size gt name node initi java heap size mb java heap size name node gt initi minimum java heap size name node mb xm hadoop young gener heap size gt name node new gener size mb maximum size new gener java heap size gt default size java new gener mb name node xx new size data node java heap size gt data node maximum java heap size mb java heap size data node gt maximum java heap size data node mb xmx,0,0,0,0,0,0,
280,Cleanup of utilities, cleanup util,We need an api for the UI to figure out the status of the cluster., we need api ui figur statu cluster,0,0,0,0,0,0,
283,Fixup review and deploy rendering, fixup review deploy render,Currently render of the review and deploy page is messy., current render review deploy page messi,0,0,0,0,0,0,
287,Add link to uninstall on index page, add link uninstal index page,Uninstall link is now hooked into the index page., uninstal link hook index page,0,0,0,0,0,0,
290,Comment in addNodesWizardInit.js., comment add node wizard init js,Comment in addNodesWizardInit.js., comment add node wizard init js,0,0,0,0,0,0,
292,HTML being spewed in the Review+Deploy page., html spew review deploy page,HTML being spewed in the Review+Deploy page., html spew review deploy page,0,0,0,0,0,0,
300,Change the status message (success/error) location so that it shows below the page summary box  rather than above  more better visibility, chang statu messag success error locat show page summari box rather better visibl,,,0,0,0,0,0,0,
302,regionservers config in the hbase only has localhost in it, regionserv config hbase localhost,/etc/hbase/conf/regionserver should correctly populate the slaves list, etc hbase conf regionserv correctli popul slave list,0,0,0,0,0,0,
304,Upgrade to yui-3.5.1, upgrad yui,,,0,0,0,0,0,0,
310,Externalize message resources for the welcome page.  Update styles on various pages., extern messag resourc welcom page updat style variou page,,,0,0,0,0,0,0,
312,Uninstall's wipe flag should be correctly passed to puppet, uninstal wipe flag correctli pass puppet,,,0,0,0,0,0,0,
316,Grid mount points page doesn't let one pass with only a custom mount point, grid mount point page let one pass custom mount point,,,0,0,0,0,0,0,
319,Scale puppet master to large number of nodes., scale puppet master larg number node,Scale puppet master to large number of nodes., scale puppet master larg number node,0,0,0,0,0,0,
323,During any process in the cluster initialization wizard  if the user goes back to the '1 Create Cluster' tab  the user is stuck., dure process cluster initi wizard user goe back creat cluster tab user stuck,,,0,0,0,0,0,0,
326,Dependencies should be added only during install phase, depend ad instal phase,Host level dependencies are being set in running stage as well  which is redundant. It can be assumed that dependencies were installed at install stage., host level depend set run stage well redund it assum depend instal instal stage,0,0,0,0,0,0,
330,Provide a way to resume if browser crashes/is closed during the deploy-in-progress, provid way resum browser crash close deploy progress,Currently when the browser is closed one cannot view the install in progress. They will have to look at the logs on the hmc machine to figure out whats going on. It would be nice if we can provide a way to get back to the install progress., current browser close one cannot view instal progress they look log hmc machin figur what go it would nice provid way get back instal progress,0,0,0,0,0,0,
335,Redundant downloads even though the artifacts are already installed, redund download even though artifact alreadi instal,Artifacts (such as mysql-connector.zip  hive.tar.gz  pig.tar.gz  ext.zip) are being downloaded even though they are previously installed leading to additional execution time., artifact mysql connector zip hive tar gz pig tar gz ext zip download even though previous instal lead addit execut time,0,0,0,0,0,0,
338,Cluster status update needs to happen for all stages of installation wizard., cluster statu updat need happen stage instal wizard,Cluster status should be updated in db for each stage of the installation wizard. This is used to enable restart of browser and also showing status of the cluster on the index page., cluster statu updat db stage instal wizard thi use enabl restart browser also show statu cluster index page,0,0,0,0,0,0,
339,Making transitionToNextStage more robust, make transit to next stage robust,If the currentStage is null  we should not proceed with a transition., if current stage null proceed transit,0,0,0,0,0,0,
349,Logging in case of error during uninstall needs to be fixed., log case error uninstal need fix,Logs disappear post uninstall because the transaction is also cleaned from the db., log disappear post uninstal transact also clean db,0,0,0,0,0,0,
352,Add flow control - force redirects to appropriate pages based on cluster configuration status for better usability, add flow control forc redirect appropri page base cluster configur statu better usabl,If no cluster has been set up yet  redirect to the welcome page.If a cluster is being configured (but has not gone thru deployment)  redirect to Step 1 of the cluster init wizard.If a cluster is being deployed  redirect to the deployment progress page.If a cluster has gone thru deployment but failed  redirect to the re-install page.If a cluster has gone thru deployment and succeed  do not perform any forced redirect., if cluster set yet redirect welcom page if cluster configur gone thru deploy redirect step cluster init wizard if cluster deploy redirect deploy progress page if cluster gone thru deploy fail redirect instal page if cluster gone thru deploy succeed perform forc redirect,0,0,0,0,0,0,
357,Redesign master service assignment page so that it takes up less vertical space, redesign master servic assign page take less vertic space,,,0,0,0,0,0,0,
362,Create lock file as part of rpm install, creat lock file part rpm instal,Lock file is being created as part of create cluster. This is brittle and needs to be done as part of the rpm install., lock file creat part creat cluster thi brittl need done part rpm instal,0,0,0,0,0,0,
366,Package up the fonts/ subdirectory in the HMC RPM, packag font subdirectori hmc rpm,The new fonts/ subdirectory used for the buttons on the ManageServices page isn't packaged up., the new font subdirectori use button manag servic page packag,0,0,0,0,0,0,
369,Improve Service Management page and general popup styling, improv servic manag page gener popup style,,,0,0,0,0,0,0,
371,Mysql packages not being sent during install and uninstall, mysql packag sent instal uninstal,Mysql packages not being sent during install and uninstall., mysql packag sent instal uninstal,0,0,0,0,0,0,
377,Uninstall does not handle component dependencies., uninstal handl compon depend,Uninstall does not handle component dependencies., uninstal handl compon depend,0,0,0,0,0,0,
386,On Single Node install when install all the components the recommended num for Map/Reduce Tasks is too high, on singl node instal instal compon recommend num map reduc task high,Use lower count of maps/reduce slots for a single node install., use lower count map reduc slot singl node instal,0,0,0,0,0,0,
393,ZooKeeper myid files not existent on ZK install., zoo keeper myid file exist zk instal,ZooKeeper myid files not existent on ZK install., zoo keeper myid file exist zk instal,0,0,0,0,0,0,
394,Add nodes fails to find node in db, add node fail find node db,Host not found in db error on assign nodes page, host found db error assign node page,0,0,0,0,0,0,
399,Cannot uninstall - the page hangs with the spinning icon, cannot uninstal page hang spin icon,,,0,0,0,0,0,0,
401,Manual config changes for nn get reset on stop/start from hmc, manual config chang nn get reset stop start hmc,Manual config changes for nn get reset on stop/start from hmc, manual config chang nn get reset stop start hmc,0,0,0,0,0,0,
402,Completing successful add node takes one to initialize cluster page starting from scratch, complet success add node take one initi cluster page start scratch,,,0,0,0,0,0,0,
404,Unify the top nav for both Monitoring and Cluster Management, unifi top nav monitor cluster manag,,,0,0,0,0,0,0,
414,Add rpm spec for hmc agent., add rpm spec hmc agent,Add rpm spec for hmc agent., add rpm spec hmc agent,0,0,0,0,0,0,
415,Reset service back to original state after reconfiguration, reset servic back origin state reconfigur,,,0,0,0,0,0,0,
420,Improve style on error log popups, improv style error log popup,,,0,0,0,0,0,0,
426,Reinstall of cluster after failure to install results in failure, reinstal cluster failur instal result failur,According to Bikas:I tried to install a single node cluster. That failed for some random issue.I was presented an option to reinstall.I clicked on that option and it asked me to uninstall.I chose uninstall and wipe data option.Uninstall failed.This happened because the install did not complete in the first place.Thu May 24 17:45:28 -0400 2012 /Stage17/Hdp-oozie::Service/Hdp-oozie::Service::Createsymlinks/usr/lib/oozie/oozie-server/lib/mapred-site.xml/File/usr/lib/oozie/oozie-server/lib/mapred-site.xml/ensure (err): change from absent to present failed: Could not set 'present on ensure: No such file or directory - /usr/lib/oozie/oozie-server/lib/mapred-site.xml at /etc/puppet/agent/modules/hdp-oozie/manifests/service.pp:61, accord bika i tri instal singl node cluster that fail random issu i present option reinstal i click option ask uninstal i chose uninstal wipe data option uninstal fail thi happen instal complet first place thu may stage hdp oozi servic hdp oozi servic createsymlink usr lib oozi oozi server lib mapr site xml file usr lib oozi oozi server lib mapr site xml ensur err chang absent present fail could set present ensur no file directori usr lib oozi oozi server lib mapr site xml etc puppet agent modul hdp oozi manifest servic pp,0,0,0,0,0,0,
429,Fix bug with jmx parsing on HBase., fix bug jmx pars h base,Fix bug with jmx parsing on HBase., fix bug jmx pars h base,0,0,0,0,0,0,
433,Using service stop instead of killall for uninstall, use servic stop instead killal uninstal,,,0,0,0,0,0,0,
435,Uninstall needs to update status for failure., uninstal need updat statu failur,This is to enable the routing layer to redirect appropriately., thi enabl rout layer redirect appropri,0,0,0,0,0,0,
442,Duplicate definition: Class[Hdp-hbase::Regionserver::Enable-ganglia], duplic definit class hdp hbase regionserv enabl ganglia,Duplicate definition: Class&#91;Hdp-hbase::Regionserver::Enable-ganglia&#93; is already defined; cannot redefine at /etc/puppet/agent/modules/hdp-ganglia/manifests/monitor.pp:37 on node ip-10-64-19-248.ec2.internal, duplic definit class hdp hbase regionserv enabl ganglia alreadi defin cannot redefin etc puppet agent modul hdp ganglia manifest monitor pp node ip ec intern,0,0,0,0,0,0,
449,Post cluster install/deploy the URL hmc/html/initializeCluster.php should be disabled, post cluster instal deploy url hmc html initi cluster php disabl,Install a cluster. Then go to http://&lt;host&gt;/hmc/html/initializeCluster.php URL. You get a page enter cluster name. If you enter a cluster name  existing install is wiped out. We need to disable this URL on an installed cluster.Only way to enable this should be uninstall., instal cluster then go http lt host gt hmc html initi cluster php url you get page enter cluster name if enter cluster name exist instal wipe we need disabl url instal cluster onli way enabl uninstal,0,0,0,0,0,0,
450,Boldify/Redify restart HMC message when nagios/ganglia is on the hmc host, boldifi redifi restart hmc messag nagio ganglia hmc host,,,0,0,0,0,0,0,
455,nagios shows service status critical if hbase is not installed, nagio show servic statu critic hbase instal,,,0,0,0,0,0,0,
466,Add nodes page alerts removed in case of adding duplicate nodes, add node page alert remov case ad duplic node,Alerts that are being shown needs to be changed to be compatible with current implementation of showing errors. A link now appears to 'Show the duplicate nodes' which shows the duplicate nodes., alert shown need chang compat current implement show error a link appear show duplic node show duplic node,0,0,0,0,0,0,
467,Fix hive stop to escape $., fix hive stop escap,,,0,0,0,0,0,0,
468,Post-Install Add Nodes - update progress title and success/error messages to reflect what it's actually doing/has done, post instal add node updat progress titl success error messag reflect actual done,,,0,0,0,0,0,0,
475,Add missing JS file for making post cluster install Add Nodes work, add miss js file make post cluster instal add node work,,,0,0,0,0,0,0,
482,Show the same welcome page to the user if the user starts configuring a cluster but has not started deploy yet, show welcom page user user start configur cluster start deploy yet,,,0,0,0,0,0,0,
486,Add Node installs MySQL Server for Hive, add node instal my sql server hive,Adding slave nodes post cluster install will install MySQL Server if Hive was selected as a service at the time of cluster installation., ad slave node post cluster instal instal my sql server hive select servic time cluster instal,0,0,0,0,0,0,
488,Manage service needs a way to recover from terminated browser sessions, manag servic need way recov termin browser session,We need to block users from being able to start/stop services when we have a batch of start/stop activities are already in progress.Do something similar to deployment progress display and bring the user back to that modal status display which will end when we detect success/failure., we need block user abl start stop servic batch start stop activ alreadi progress do someth similar deploy progress display bring user back modal statu display end detect success failur,0,0,0,0,0,0,
491,Service Reconfiguration screens should respect the 'reconfigurable' attributes set in ConfigProperties table, servic reconfigur screen respect reconfigur attribut set config properti tabl,Some service config parameters are editable (can be customized on initial install)  but cannot be reconfigured post cluster install.This info is stored in ConfigProperties table  but the Service Reconfiguration screens are allowing these non-reconfigurable parameters to be changed., some servic config paramet edit custom initi instal cannot reconfigur post cluster instal thi info store config properti tabl servic reconfigur screen allow non reconfigur paramet chang,0,0,0,0,0,0,
492,make support for os check a bit more robust, make support os check bit robust,,,0,0,0,0,0,0,
493,Add rack_info as column in Hosts table, add rack info column host tabl,,,0,0,0,0,0,0,
494,Fix node assignments not not allow slaves on master., fix node assign allow slave master,,,0,0,0,1,0,0,
501,Speed up page load/reload times, speed page load reload time,,,0,0,0,0,0,0,
508,Support Resume For Add Nodes, support resum for add node,Just like for Manage Services + Deploy + Uninstall., just like manag servic deploy uninstal,0,0,0,0,0,0,
510,Modify the router to force redirection to 'Add Nodes Progress' popup, modifi router forc redirect add node progress popup,,,0,0,0,0,0,0,
512,Fix puppet manifests for tarball downloads via rpms., fix puppet manifest tarbal download via rpm,,,0,0,0,0,0,0,
519,update to fix the ganglia monitor_and_server anchor problem, updat fix ganglia monitor server anchor problem,update to fix the ganglia monitor_and_server anchor problem, updat fix ganglia monitor server anchor problem,0,0,0,0,0,0,
528,Fix oozie smoke test failure, fix oozi smoke test failur,Oozie smoke test failing with 'Error: E0301 : E0301: Invalid resource &#91;usr/lib/oozie/conf&#93;', oozi smoke test fail error e e invalid resourc usr lib oozi conf,0,0,0,0,0,0,
529,Fix Advanced Config: HDFS reserved space is in bytes. Too many bytes to count., fix advanc config hdf reserv space byte too mani byte count,Simplify user input., simplifi user input,0,0,0,0,0,0,
539,Create a spec file with less dependencies for HMC, creat spec file less depend hmc,Simplify process for users that want to use different dependencies for PHP and Ruby.e.g PHP-5.3  ruby-1.8.7, simplifi process user want use differ depend php rubi e g php rubi,0,0,0,0,0,0,
543,Rpm naming needs to be corrected., rpm name need correct,Rpm naming needs to be corrected., rpm name need correct,0,0,0,0,0,0,
544,Templeton configs for pig archive not correct in HMC, templeton config pig archiv correct hmc,From Arpit:The configs for pig in templeton are wrong.The deployed configs are&lt;property&gt; &lt;name&gt;templeton.pig.archive&lt;/name&gt; &lt;value&gt;hdfs:///apps/templeton/&lt;/value&gt; &lt;description&gt;The path to the Pig archive.&lt;/description&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;templeton.pig.path&lt;/name&gt; &lt;value&gt;/pig-0.9.2/bin/pig&lt;/value&gt; &lt;description&gt;The path to the Pig executable.&lt;/description&gt; &lt;/property&gt;They are missing the pig tar ball. For example they should be&lt;property&gt; &lt;name&gt;templeton.pig.archive&lt;/name&gt; &lt;value&gt;hdfs:///apps/templeton/pig.tar.gz&lt;/value&gt; &lt;description&gt;The path to the Pig archive.&lt;/description&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;templeton.pig.path&lt;/name&gt; &lt;value&gt;pig.tar.gz/pig-0.9.2/bin/pig&lt;/value&gt; &lt;description&gt;The path to the Pig executable.&lt;/description&gt; &lt;/property&gt;So both the properties are missing 'pig-0.9.2.tar.gz', from arpit the config pig templeton wrong the deploy config lt properti gt lt name gt templeton pig archiv lt name gt lt valu gt hdf app templeton lt valu gt lt descript gt the path pig archiv lt descript gt lt properti gt lt properti gt lt name gt templeton pig path lt name gt lt valu gt pig bin pig lt valu gt lt descript gt the path pig execut lt descript gt lt properti gt they miss pig tar ball for exampl lt properti gt lt name gt templeton pig archiv lt name gt lt valu gt hdf app templeton pig tar gz lt valu gt lt descript gt the path pig archiv lt descript gt lt properti gt lt properti gt lt name gt templeton pig path lt name gt lt valu gt pig tar gz pig bin pig lt valu gt lt descript gt the path pig execut lt descript gt lt properti gt so properti miss pig tar gz,0,0,0,0,0,0,
546,Puppet fails to install 32-bit JDK properly on RHEL6, puppet fail instal bit jdk properli rhel,mkdir -p /usr/jdk32 ; chmod +x /tmp/HDP-artifacts//jdk-6u31-linux-i586.bin; cd /usr/jdk32 ; echo A | /tmp/HDP-artifacts//jdk-6u31-linux-i586.bin -noregister 2&gt;&amp;1Unpacking...Checksumming...Extracting.../var/www/html/downloads/jdk-6u31-linux-i586.bin: ./install.sfx.1794: /lib/ld-linux.so.2: bad ELF interpreter: No such file or directoryFailed to extract the files. Please refer to the Troubleshooting section ofthe Installation Instructions on the download page for more information.Puppet should ensure glibc.i686 is installed before trying to install jdk., mkdir p usr jdk chmod x tmp hdp artifact jdk u linux bin cd usr jdk echo a tmp hdp artifact jdk u linux bin noregist gt amp unpack checksum extract var www html download jdk u linux bin instal sfx lib ld linux bad elf interpret no file directori fail extract file pleas refer troubleshoot section ofth instal instruct download page inform puppet ensur glibc instal tri instal jdk,0,0,0,0,0,0,
547,Change os type check during node bootstrap to allow RHEL6 or CentOS6 nodes, chang os type check node bootstrap allow rhel cent os node,hmc/php/frontend/addNodes/verifyAndUpdateNodesInfo.php hardcodes checks to only allow rhel5 or centos5 nodes., hmc php frontend add node verifi and updat node info php hardcod check allow rhel cento node,0,0,1,0,0,0,
548,Puppet agent install script should use correct epel repo, puppet agent instal script use correct epel repo,hmc/ShellScripts/puppet_agent_install.sh hardcodes to epel-release rpm for CentOS-5.4. Should be more intelligent to handle different OS types., hmc shell script puppet agent instal sh hardcod epel releas rpm cent os should intellig handl differ os type,0,0,1,0,0,0,
550,Add support to jump to a specified state in the wizard for development purposes, add support jump specifi state wizard develop purpos,,,0,0,0,0,0,0,
552,Update README to point to trunk, updat readm point trunk,We need to fix the README to point to trunk after the merge of ambari-186 branch., we need fix readm point trunk merg ambari branch,0,0,0,0,0,0,
565,Remove YUI source files from SVN, remov yui sourc file svn,Currently YUI 3.5.1 source files are in SVN.We don't really need to have them checked into SVN as we don't track changes. We are pre-concatenating and minifying the YUI js files  so we don't need the files to run Ambari anyhow.We should just remove them from SVN and instead just checkin the tarball., current yui sourc file svn we realli need check svn track chang we pre concaten minifi yui js file need file run ambari anyhow we remov svn instead checkin tarbal,0,0,0,0,0,0,
566,Update documentation, updat document,Modify pom.xml to create output files under ../site  rather than ./target.Make minor wording modifications., modifi pom xml creat output file site rather target make minor word modif,0,0,0,0,0,0,
569,Nagios install fails on RHEL6, nagio instal fail rhel,Puppet layer  when trying to install nagios  tries to install php-pecl-json when it is not needed and fails when trying to do so. On RHEL6  php-5.3 is default which has the json module built into it., puppet layer tri instal nagio tri instal php pecl json need fail tri on rhel php default json modul built,0,0,1,0,0,0,
570,Consolidate head tags for organization and combine CSS files for faster load, consolid head tag organ combin css file faster load,,,0,0,0,0,1,0,
573,Puppet error: Cannot reassign variable zookeeper_hosts at modules/hdp/manifests/params.pp:47, puppet error cannot reassign variabl zookeep host modul hdp manifest param pp,Seems like a typo: $zookeeper_hosts = hdp_default('zookeeper_hosts')Should be $public_zookeeper_hosts, seem like typo zookeep host hdp default zookeep host should public zookeep host,0,0,0,0,0,0,
576,In Custom config for Nagios: emails with multiple periods before the '@' fails validation, in custom config nagio email multipl period fail valid,,,0,0,0,0,0,0,
578,Custom Config page: don't allow form submission if there are client-side validation errors, custom config page allow form submiss client side valid error,Currently the button looks disabled when there are client-side validation errors  but it is clickable. This is a problem because there is no server-side validation to make sure that the passwords match., current button look disabl client side valid error clickabl thi problem server side valid make sure password match,0,0,0,1,0,0,
581,special characters in hosts files created on some common windows editors causes issues, special charact host file creat common window editor caus issu,,,0,0,0,0,0,0,
587,Rat compliance patch, rat complianc patch,Adding apache license to all the files within Ambari for rat tool compliance., ad apach licens file within ambari rat tool complianc,0,0,0,0,0,0,
588,Externalize the manager service name and point the Help link to a valid URL, extern manag servic name point help link valid url,The master role assignment page (Step 5 of install) and the cluster topology summary page are referencing the manager service name as 'HMC Server'. This needs to be changed to 'Ambari' server.The post-install success message on a single-node install has the same issue.Currently  the Help link in the top nav launches a new tab and loads the page that the user is on. Instead  load the Help page on Ambari project website (for now  we'll point it to the Install Guide as a placeholder)., the master role assign page step instal cluster topolog summari page referenc manag servic name hmc server thi need chang ambari server the post instal success messag singl node instal issu current help link top nav launch new tab load page user instead load help page ambari project websit point instal guid placehold,0,0,0,0,0,0,
591,License header for PHP files should use PHP comments  not HTML comments, licens header php file use php comment html comment,Some PHP files have the HTML-style comments. This is problematic sincethe license headers are becoming part of the HTML response. Worse yet  the headers are repeated multiple times in the response. This can also cause unexpected behavior., some php file html style comment thi problemat sinceth licens header becom part html respons wors yet header repeat multipl time respons thi also caus unexpect behavior,0,0,0,0,0,0,
592,Add a link to NOTICE file on every page, add link notic file everi page,This is so that stuff with different compatible licenses can be attributed appropriately., thi stuff differ compat licens attribut appropri,0,0,0,0,0,0,
597,Remove /usr/bin/php dependency from the rpm's, remov usr bin php depend rpm,,,0,0,0,0,0,0,
600,Fix lzo installs to work correctly on RHEL6, fix lzo instal work correctli rhel,,,0,0,1,0,0,0,
607,Increase puppet timeouts to handle single-node installs timing out, increas puppet timeout handl singl node instal time,,,0,0,0,0,1,0,
614,The database set up script has a duplicate definition of AmbariConfig so install fails, the databas set script duplic definit ambari config instal fail,,,0,0,0,0,0,0,
615,Eliminate redundant and unused definition for the columns in the table ConfigProperties, elimin redund unus definit column tabl config properti,ConfigProperties table has a column named 'display_type'.There's also a JSON-encoded 'display_attributes' column.The 'display_attributes' column has the attributes 'isPassword'  'displayType'  and 'noDisplay'. This is duplicate information is already stored by the 'display_type' column so these attributes are not needed.Upon inspecting the code  these attributes are not used so changing them have no effect. We should simply get rid of these attributes., config properti tabl column name display type there also json encod display attribut column the display attribut column attribut password display type display thi duplic inform alreadi store display type column attribut need upon inspect code attribut use chang effect we simpli get rid attribut,0,0,0,0,0,0,
628,hdp-nagios and hdp-monitoring has wrong configuration file location  also owner:group permissions are wrong., hdp nagio hdp monitor wrong configur file locat also owner group permiss wrong,Suse environment has wrong configuration location for hdp-dashboard and hdp-nagios. Also  owner:group permissions were wrongly set to root:root instead of 'wwwrun' and group is 'www, suse environ wrong configur locat hdp dashboard hdp nagio also owner group permiss wrongli set root root instead wwwrun group www,0,0,0,1,0,0,
633,Fix invalid HTML markup on Monitoring Dashboard, fix invalid html markup monitor dashboard,,,0,0,0,0,0,0,
635,Add Nodes Progress: for partial failure that lets the user continue  display an orange bar rather than a red bar in the progress popup, add node progress partial failur let user continu display orang bar rather red bar progress popup,,,0,0,0,0,0,0,
636,Support for Hadoop Security (front-end changes), support hadoop secur front end chang,,,0,0,0,0,0,0,
638,Weirdness with Custom Config page when the user goes back to previous stages, weird custom config page user goe back previou stage,Issue 1. Going back and forth between different stages in the Cluster Install Wizard  it is possible to get into a state where Custom Config form has the submit button disabled but no field errors are shown.Steps to replicate: Go up to Stage 6  but do not submit the form Go back to Stage 3 Go up to Stage 6 again. No field errors are shown when they should be.Issue 2. Custom Config stage is skipped once you get to 'Review and Deploy' and go back to a stage preceding Custom Config.Steps to replicate: Go thru the Cluster Install Wizard up to Stage 7 ('Review and Deploy')  but do not submit the form. Go back to Stage 4 or 5. Once you submit the form on Stage 5  Stage 6 is skipped and goes directly to Stage 7. If you go back to Stage 3 or earlier  then Stage 6 will not be skipped., issu go back forth differ stage cluster instal wizard possibl get state custom config form submit button disabl field error shown step replic go stage submit form go back stage go stage no field error shown issu custom config stage skip get review deploy go back stage preced custom config step replic go thru cluster instal wizard stage review deploy submit form go back stage onc submit form stage stage skip goe directli stage if go back stage earlier stage skip,0,0,0,0,0,0,
668,Ambari should install yum priorities on all nodes to ensure main repo is picked first, ambari instal yum prioriti node ensur main repo pick first,Depending on which main hadoop repo is used to setup the cluster  sometimes  wrong packages may be pulled from add-on repos even if the main repo has a higher priority and has the same package. This can create problems by incompatible dependencies getting installed at times., depend main hadoop repo use setup cluster sometim wrong packag may pull add repo even main repo higher prioriti packag thi creat problem incompat depend get instal time,0,0,0,0,0,0,
675,Make puppet generate more logs on command failures, make puppet gener log command failur,,,0,0,0,0,0,0,
689,Fix ambari agent init.d scripts and the bootstrapping., fix ambari agent init script bootstrap,,,0,0,0,0,0,0,
701,Ambari does not handle a pre-setup user-supplied Hive Metastore, ambari handl pre setup user suppli hive metastor,Ambari treats a Hive Metastore as something that it still needs to install and setup even though it may not have the necessary permissions to do so., ambari treat hive metastor someth still need instal setup even though may necessari permiss,0,0,0,0,0,0,
1072,Change text on alerts 'about XX hours ago', chang text alert xx hour ago,Alerts should have text like :OK for about 17 hoursWARN for about 17 hoursCRIT for about 17 hours, alert text like ok hour warn hour crit hour,1,0,0,0,0,0,
1081,HDFS disk capacity on dashboard is seen as negative number, hdf disk capac dashboard seen neg number,Sometimes disk capacity calculations end up in negative numbers., sometim disk capac calcul end neg number,1,0,0,0,0,0,
1085,Remove files from ambari-web that were not meant to be checked in, remov file ambari web meant check,ambari-web/node_modules  ambari-web/public  ambari-web/ambari.iml were not meant to be checked in. Must remove., ambari web node modul ambari web public ambari web ambari iml meant check must remov,0,0,0,0,0,0,
1092,dashboard > Summary > capacity pie chart keeps changing colors, dashboard summari capac pie chart keep chang color,,,0,0,0,0,0,0,
1096,Create heatmap legend entries for missing data/invalid hosts, creat heatmap legend entri miss data invalid host,Currently we dont fill anything which is ambiguous., current dont fill anyth ambigu,1,0,0,0,0,0,
1098,Switching services does not update various UI elements, switch servic updat variou ui element,When you switch services in the UI  sometimes alerts are mismatched compared to selected service. Also the highlights in left-bar are not working., when switch servic ui sometim alert mismatch compar select servic also highlight left bar work,1,0,0,0,0,0,
1102,Error handling when errors are encountered during preparation for deploy, error handl error encount prepar deploy,Currently  if any errors are encountered during preparation for deploy  the user is taken to the deploy page and the hosts will be shown as 'Waiting' but nothing happens. This is bad UX.At a minimum  we should prevent the user from proceeding and display an appropriate error message if any error is encountered after 'Deploy' is clicked  but before we transition to Step 9.We should also think about how a user can recover from this situation.At this point  the deploy has not initiated  but certain API calls may have succeeded  so we may have incomplete info in the database. Currently there's no convenient way to 'rollback'.We can either ask the user to clean the slate by reinitializing the database and try again (should succeed if the original problem was temporary).We can also build more logic in the UI to retry  check if records already exist  etc..., current error encount prepar deploy user taken deploy page host shown wait noth happen thi bad ux at minimum prevent user proceed display appropri error messag error encount deploy click transit step we also think user recov situat at point deploy initi certain api call may succeed may incomplet info databas current conveni way rollback we either ask user clean slate reiniti databas tri succeed origin problem temporari we also build logic ui retri check record alreadi exist etc,1,0,0,0,0,0,
1103,Need to be able to reliably recover from the case when the browser is closed during deploy (Step 8 post submission  Step 9) of the wizard, need abl reliabl recov case browser close deploy step post submiss step wizard,Need to be able to reliably recover from the case when the browser is closed during deploy (Step 8 post submission  Step 9) of the wizard.Even after submitting  Step 10  its taking to Step 9 after browser restart. Ideally it should take to monitoring page., need abl reliabl recov case browser close deploy step post submiss step wizard even submit step take step browser restart ideal take monitor page,1,0,0,0,0,0,
1106,User-specified custom configs (such as hdfs-site.xml overrides) should be persisted to maintain what the user specified, user specifi custom config hdf site xml overrid persist maintain user specifi,,,0,0,0,0,0,0,
1113,Install Wizard: Confirm host stuck at Preparing stage, instal wizard confirm host stuck prepar stage,With the install wizard went to assign slaves page successfully  returned back to Welcome page  reentered data and then the UI got stuck at Confirm hosts page., with instal wizard went assign slave page success return back welcom page reenter data ui got stuck confirm host page,0,0,0,0,0,0,
1115,Host component live status is broken, host compon live statu broken,When datanode is stopped on a host  its status keeps jumping., when datanod stop host statu keep jump,0,0,0,0,0,0,
1123,Ambari heatmaps and host information shows infinity for disk space used, ambari heatmap host inform show infin disk space use,When cluster is started after install  it has disk_total of null which results in Infinity., when cluster start instal disk total null result infin,1,0,0,0,0,0,
1125,Graphs 'degrade' over time, graph degrad time,Leave a page with graphs open for several minutes.The graphs become really coarse.It probably has something to do with the fact that Ganglia only provides 6-minute averages beyond the first 61 minutes (first 61 minutes  Ganglia keeps 15-second samples) and we may not be exactly querying for the last 60 minutes., leav page graph open sever minut the graph becom realli coars it probabl someth fact ganglia provid minut averag beyond first minut first minut ganglia keep second sampl may exactli queri last minut,0,0,0,0,0,0,
1142,On Notification Popup  clicking 'go to nagios UI' doesn't load nagios UI, on notif popup click go nagio ui load nagio ui,1) Cause notification to occur (for example  stop oozie)2) On dashboard  click notification icon3) On the notification popup  click 'go to nagios web UI'4) nothing happens.This issue is on IE9  Safari and Chrome. Works fine on Firefox., caus notif occur exampl stop oozi on dashboard click notif icon on notif popup click go nagio web ui noth happen thi issu ie safari chrome work fine firefox,0,0,0,0,0,0,
1143,tmpfs filesystem being added to the list in the dir used by Ambari, tmpf filesystem ad list dir use ambari,I saw this on a sles cluster. On EC2 they have a tmpfs mounted and Ambari picked it up.Not sure what the ideal solution is but i feel tmpfs should not be included in the available mount points.Also the tmpfs is being used in various directories that will have to change during the install.Attached screenshots., i saw sle cluster on ec tmpf mount ambari pick not sure ideal solut feel tmpf includ avail mount point also tmpf use variou directori chang instal attach screenshot,0,0,0,0,0,0,
1150,Installer Wizard - Retry feature in Deploy step (Step 9) is broken, instal wizard retri featur deploy step step broken,,,0,0,0,0,0,0,
1151,Reconfigure fails silently; it's not firing any API calls due to a JS error, reconfigur fail silent fire api call due js error,,,0,0,0,0,0,0,
1153,Host jams in status 'Preparing' if host name is wrong, host jam statu prepar host name wrong,,,0,0,0,0,0,0,
1154,The check boxes to check/uncheck one of the members in a multi artifact graphs is not very readable. It should be more apparent on which one the user clicked on, the check box check uncheck one member multi artifact graph readabl it appar one user click,,,0,0,0,0,0,0,
1159,Check the log/run dir locations to make sure its an abs path, check log run dir locat make sure ab path,,,0,0,0,0,0,0,
1164,Disk Info Metrics and memory usage sometimes do not show up for an hour or so., disk info metric memori usag sometim show hour,Disk Info Metrics and memory usage sometimes do not show up for an hour or so., disk info metric memori usag sometim show hour,0,0,0,0,0,0,
1181,Clean up table header UI for sorting and filter clear 'x' for Hosts table, clean tabl header ui sort filter clear x host tabl,,,0,0,0,0,0,0,
1182,Clean up table header UI for sorting and filter clear 'x' for Jobs table, clean tabl header ui sort filter clear x job tabl,,,0,0,0,0,0,0,
1184,After adding hosts  the host count shown in the Dashboard is incorrect, after ad host host count shown dashboard incorrect,,,0,0,0,0,0,0,
1190,Detailed log view dialogs are not center-aligned, detail log view dialog center align,,,0,0,0,0,0,0,
1196,Automatically update host-level popup info/logs, automat updat host level popup info log,For host-level popup info/logs that appear during the deploy step (Step 9) of the Install and Add Hosts Wizards  the information shown is not automatically updated based on the latest values in the models when the background poller updates the models.Currently  the user needs to close the popup and then re-open to see the updated info., for host level popup info log appear deploy step step instal add host wizard inform shown automat updat base latest valu model background poller updat model current user need close popup open see updat info,0,0,0,0,0,0,
1207,Remove /hdp as the httpd conf for any of the nagios urls - should replace it with ambarinagios or something else., remov hdp httpd conf nagio url replac ambarinagio someth els,Remove /hdp as the httpd conf for any of the nagios urls - should replace it with ambarinagios or something else., remov hdp httpd conf nagio url replac ambarinagio someth els,0,0,0,0,0,0,
1210,Allow capacity scheduler to be attached to host role configs for CS configurability in the API's., allow capac schedul attach host role config cs configur api,Allow capacity scheduler to be attached to host role configs for CS configurability in the API's., allow capac schedul attach host role config cs configur api,0,0,0,0,0,0,
1214,In any starts fails  'warn' the host and the overall install, in start fail warn host overal instal,If any service start fails  'warn' the host and 'warn' the overall install.Allow the other start tasks to complete.Allow the user to click next., if servic start fail warn host warn overal instal allow start task complet allow user click next,0,0,0,0,0,0,
1216,Add filters module, add filter modul,,,0,0,0,0,0,0,
1218,Refactor Job Browser User filter, refactor job browser user filter,,,0,0,0,0,0,0,
1223,Confirm Hosts page: It looks like hosts disappear if you are on 'Fail' filter and click on 'Retry Failed' button, confirm host page it look like host disappear fail filter click retri fail button,,,0,0,0,0,0,0,
1224,Drop the 'all' option from Hosts > Component Filter and Jobs > Users Filter, drop option host compon filter job user filter,,,0,0,0,0,0,0,
1229,Dashboard - make disk usage pie chart in HDFS summary easier to understand, dashboard make disk usag pie chart hdf summari easier understand,,,0,0,0,0,0,0,
1231,Replace sudo with su in the ambari setup script since ambari server setup is already run as root., replac sudo su ambari setup script sinc ambari server setup alreadi run root,Replace sudo with su in the ambari setup script since ambari server setup is already run as root., replac sudo su ambari setup script sinc ambari server setup alreadi run root,0,0,0,0,0,0,
1233,Directory permissions on httpd /var/www/cgi-bin should not be touched by Ambari., directori permiss httpd var www cgi bin touch ambari,Directory permissions on httpd /var/www/cgi-bin should not be touched by Ambari., directori permiss httpd var www cgi bin touch ambari,0,0,0,1,0,0,
1235,Host health indicator should have a tooltip showing details, host health indic tooltip show detail,,,0,0,0,0,0,0,
1244,Install Options - line up the Target Hosts section with the rest of the page, instal option line target host section rest page,,,0,0,0,0,0,0,
1253,Use ember-precompiler-brunch npm plugin, use ember precompil brunch npm plugin,Use the Ember Handlebars precompiler plugin (ember-precompiler-brunch) that's in the NPM registry so that we can specify a specific plugin version to use in package.json  rather than the current plugin that Ambari Web uses (which is not in the NPM registry - it is retrieved from a git repo and Ambari Web can break if the plugin gets updated)., use ember handlebar precompil plugin ember precompil brunch npm registri specifi specif plugin version use packag json rather current plugin ambari web use npm registri retriev git repo ambari web break plugin get updat,0,0,0,0,0,0,
1259,Fix the host roles live status not go back to INSTALLED if it was in START_FAILED state., fix host role live statu go back instal start fail state,Fix the host roles live status not go back to INSTALLED if it was in START_FAILED state., fix host role live statu go back instal start fail state,0,0,0,0,0,0,
1260,Remove hard coded JMX port mappings, remov hard code jmx port map,The JMXPropertyProvider contains a map of component names to ports ... JMX_PORTS.put('NAMENODE'  '50070'); JMX_PORTS.put('DATANODE'  '50075'); JMX_PORTS.put('JOBTRACKER'  '50030'); JMX_PORTS.put('TASKTRACKER'  '50060'); JMX_PORTS.put('HBASE_MASTER'  '60010');These ports can change in configuration. Need to create the mapping dynamically.This is required for secure HDP cluster to work., the jmx properti provid contain map compon name port jmx port put namenod jmx port put datanod jmx port put jobtrack jmx port put tasktrack jmx port put hbase master these port chang configur need creat map dynam thi requir secur hdp cluster work,0,0,0,0,0,0,
1264,Service graphs refresh with spinners, servic graph refresh spinner,Service graphs are refreshing with spinners  rather than simply shifting to the left.See the attached movie clip., servic graph refresh spinner rather simpli shift left see attach movi clip,0,0,0,0,0,0,
1265,Job Browser - Filter by Input  output and duration, job browser filter input output durat,&lt;expression&gt; ::= [&lt;operator&gt;] [&lt;whitespace&gt;]* &lt;value&gt; [&lt;duration-unit&gt; | &lt;io-unit&gt;]&lt;operator&gt; ::= '&gt;' | '&lt;' | '='&lt;whitespace&gt; ::= ' '&lt;value&gt; :== int | float&lt;duration-unit&gt; ::= 's' | 'm' | 'h'&lt;io-unit&gt; ::= 'k' | 'kb' | 'm' | 'mb' | 'g' | 'gb'If &lt;operator&gt; is ommitted  '=' is assumed.If &lt;duration-unit&gt; is omitted  's' is assumed.If &lt;io-unit&gt; is omitted  'k' is assumed.&lt;duration-unit&gt; and &lt;io-unit&gt; are case-insensitive., lt express gt lt oper gt lt whitespac gt lt valu gt lt durat unit gt lt io unit gt lt oper gt gt lt lt whitespac gt lt valu gt int float lt durat unit gt h lt io unit gt k kb mb g gb if lt oper gt ommit assum if lt durat unit gt omit assum if lt io unit gt omit k assum lt durat unit gt lt io unit gt case insensit,0,0,0,0,0,0,
1266,Agent checks packages as part of host check but doesn't tell which ones are needed or conflicting, agent check packag part host check tell one need conflict,Agent checks packages as part of host check but doesn't tell which ones are needed or conflicting, agent check packag part host check tell one need conflict,0,0,0,0,0,0,
1267,Store example Hive Queries somewhere in Ambari that's easily accessible for demo/test purposes, store exampl hive queri somewher ambari easili access demo test purpos,Store example Hive Queries somewhere in Ambari that's easily accessible for demo/test purposes, store exampl hive queri somewher ambari easili access demo test purpos,0,0,0,0,0,0,
1271,On Confirm Hosts page  add a link to show the Host Checks popup in the success message, on confirm host page add link show host check popup success messag,,,0,0,0,0,0,0,
1273,Edit User: No error message is shown when the user does not enter the correct 'old password', edit user no error messag shown user enter correct old password,No error message is shown when the user does not enter the correct 'old password' when editing an user., no error messag shown user enter correct old password edit user,0,0,0,0,0,0,
1274,Shrink top nav height, shrink top nav height,Navbar's height should be shrunk to ~40px., navbar height shrunk px,0,0,0,0,0,0,
1275,Incorrect displaying 'Background operations' window after changing state of component, incorrect display background oper window chang state compon,Steps:1. Change state (start/stop operations are preferable) for lot of components so that the number of background operations was approximately 20-30;2. Change size of browser (800-900 px);3. Go to down of page;4. Change state for next component;5. Confirm changing;Result:'Background operations' window was opened  but it 'OK' button is not visible.Expected result:'Background operations' window contains scroll bar for all background operations. 'OK' button is available for using., step chang state start stop oper prefer lot compon number background oper approxim chang size browser px go page chang state next compon confirm chang result background oper window open ok button visibl expect result background oper window contain scroll bar background oper ok button avail use,0,0,0,0,0,0,
1277,Failing build due to url moved on Suse., fail build due url move suse,Failing build due to url moved on Suse. Looks like centos5 and 6 handle redirection all fine but doesnt look like the maven plugin handles that on SUSE., fail build due url move suse look like cento handl redirect fine doesnt look like maven plugin handl suse,0,0,0,0,0,0,
1279,Make sure that Ambari Web renders all elements correctly when the browser width is 1024px or narrower, make sure ambari web render element correctli browser width px narrow,Current behavior is to dynamically layout elements as the browser viewport size changes. This causes elements to overlap  go outside the bounding box  positioned in a weird way  etc.Let's set minimum width to be 1024px (or maybe 980px to account for the scrollbar) and make sure that all elements are laid out correctly.Bigger than 1024px should still utilize more screen space as it does today., current behavior dynam layout element browser viewport size chang thi caus element overlap go outsid bound box posit weird way etc let set minimum width px mayb px account scrollbar make sure element laid correctli bigger px still util screen space today,0,0,0,0,0,0,
1299,Bootstrap can hang indefinitely, bootstrap hang indefinit,I was bootstrapping 4 nodes. One of them got stuck in 'Installing' phase and won't time out even after ~30 minutes.It seems like starting of ambari-agent was hanging., i bootstrap node one got stuck instal phase time even minut it seem like start ambari agent hang,0,0,0,0,0,0,
1300,Service status / host component status can get stuck in the green blinking state if stop fails - no further operation can be performed, servic statu host compon statu get stuck green blink state stop fail oper perform,This happens when a service/host component is running  but stop fails. This leaves the desired_state in the INSTALLED (i.e.  STOPPED) state  while the live state is STARTED.Currently  UI assumes when desired_state==INSTALLED and state==STARTED  it is STARTING. This was a trick to get around the problem of backend live state update lagging and to make UI more responsive., thi happen servic host compon run stop fail thi leav desir state instal e stop state live state start current ui assum desir state instal state start start thi trick get around problem backend live state updat lag make ui respons,0,0,0,0,0,0,
1305,Make sure that Ambari Web renders all elements correctly when the browser width is 1024px or narrower (refactor), make sure ambari web render element correctli browser width px narrow refactor,Current behavior is to dynamically layout elements as the browser viewport size changes. This causes elements to overlap  go outside the bounding box  positioned in a weird way  etc.Let's set minimum width to be 1024px (or maybe 980px to account for the scrollbar) and make sure that all elements are laid out correctly.Bigger than 1024px should still utilize more screen space as it does today., current behavior dynam layout element browser viewport size chang thi caus element overlap go outsid bound box posit weird way etc let set minimum width px mayb px account scrollbar make sure element laid correctli bigger px still util screen space today,0,0,1,0,0,0,
1308,Properly display Apps page aggregate summary and data table when there are no data to be shown, properli display app page aggreg summari data tabl data shown,,,0,0,0,0,0,0,
1309,Remove all text from Apps views  controllers  templates to messages.js, remov text app view control templat messag js,,,0,0,0,0,0,0,
1311,Host health indicator should have a tooltip showing few details (refactoring), host health indic tooltip show detail refactor,,,0,0,0,0,0,0,
1321,Switching out of Jobs page does not launch popup anymore, switch job page launch popup anymor,On the jobs page click on Job-X and see the popup with all the job information. Now switch to Services page and come back to the jobs page. Now clicking on Job-X will not launch popup., on job page click job x see popup job inform now switch servic page come back job page now click job x launch popup,0,0,0,0,0,0,
1330,Cluster missing hosts after successful install and restart., cluster miss host success instal restart,Cluster missing hosts after successful install and restart. This bug got introduced due to my patch in AMBARI-1301., cluster miss host success instal restart thi bug got introduc due patch ambari,0,0,0,0,0,0,
1333,Add username validation for Ambari local users, add usernam valid ambari local user,,,0,0,0,0,0,0,
1335,Show validation error when the user specifies target hosts that are already part of the cluster, show valid error user specifi target host alreadi part cluster,,,0,0,0,0,0,0,
1337,Refactor Job Browser filter, refactor job browser filter,,,0,0,0,0,0,0,
1340,Enhance Install/Start/Test progress display, enhanc instal start test progress display,When we are showing progress for cluster install  it takes a while for any of the host-level or overall progress to show any progress completion percentage (they can get stuck at 0% for several minutes). This is because host-level completion is not displayed until the first install task for the task is complete. Instead  we should advance percentage complete when the tasks changes the status (from PENDING-&gt;QUEUED-&gt;IN_PROGRESS) to better reflect the install progress to the end user., when show progress cluster instal take host level overal progress show progress complet percentag get stuck sever minut thi host level complet display first instal task task complet instead advanc percentag complet task chang statu pend gt queu gt in progress better reflect instal progress end user,0,0,0,0,0,0,
1343,Service Check fails after secure install due to wrong kinit path, servic check fail secur instal due wrong kinit path,For manually installed kdc  the kinit path in the puppet script is /usr/kerberos/bin/kinitActual location:root@ip-10-38-13-250 data]# whereis kinitkinit: /usr/bin/kiniterr: /Stage&#91;2&#93;/Hdp-hbase::Hbase::Service_check/Hdp-hadoop::Exec-hadoop&#91;hbase::service_check::test&#93;/Hdp::Exec&#91;/usr/kerberos/bin/kinit -kt /etc/security/keytabs/hdfs.headless.keytab hdfs; hadoop --config /etc/hadoop/conf fs -test -e /apps/hbase/data/usertable&#93;/Exec&#91;/usr/kerberos/bin/kinit -kt /etc/security/keytabs/hdfs.headless.keytab hdfs; hadoop --config /etc/hadoop/conf fs -test -e /apps/hbase/data/usertable&#93;: Failed to call refresh: Could not find command '/usr/kerberos/bin/kinit', for manual instal kdc kinit path puppet script usr kerbero bin kinit actual locat root ip data wherei kinitkinit usr bin kiniterr stage hdp hbase hbase servic check hdp hadoop exec hadoop hbase servic check test hdp exec usr kerbero bin kinit kt etc secur keytab hdf headless keytab hdf hadoop config etc hadoop conf fs test e app hbase data usert exec usr kerbero bin kinit kt etc secur keytab hdf headless keytab hdf hadoop config etc hadoop conf fs test e app hbase data usert fail call refresh could find command usr kerbero bin kinit,1,0,0,0,0,0,
1348,Externalize strings to messages.js, extern string messag js,Externalize string resources to messages.js., extern string resourc messag js,1,0,0,0,0,0,
1351,Provide consistent ordering of hosts in heatmap, provid consist order host heatmap,,,0,0,0,0,0,0,
1354,'No alerts' badge on the Host Detail page should be green  not red, no alert badg host detail page green red,,,0,0,0,0,0,0,
1359,App Browser rows colours should alternate from dark grey to light grey and back, app browser row colour altern dark grey light grey back,,,0,0,0,0,0,0,
1360,Mouse cursor hover behavior is strange on Job Browser, mous cursor hover behavior strang job browser,When hovering the mouse cursor around Show All  Filtered  the mouse cursor changes to the 'hand' icon as expected. To the right  the cursor turns into a hand even when hovering over areas where there's no link. Hovering over 'Clear filters'  the cursor does not turn into the 'hand'., when hover mous cursor around show all filter mous cursor chang hand icon expect to right cursor turn hand even hover area link hover clear filter cursor turn hand,0,0,0,0,0,0,
1373,Since there is the ability to log in to Ambari Web as different users the current user should be indicated, sinc abil log ambari web differ user current user indic,,,0,0,0,0,0,0,
1376,Wrong calculation of duration filter on apps page, wrong calcul durat filter app page,Wrong calculation for the duration filter if we enter just number  without h  m or s to specify the unit. By default we should take seconds as the default unit., wrong calcul durat filter enter number without h specifi unit by default take second default unit,0,0,0,0,0,0,
1432,Ambari Agent registration hangs due to Acceptor bug in Jetty for not reading through accepted connections., ambari agent registr hang due acceptor bug jetti read accept connect,Ambari Agent registration hangs due to Acceptor bug in Jetty for not reading through accepted connections., ambari agent registr hang due acceptor bug jetti read accept connect,0,0,0,0,0,0,
1441,Validation for username used in service configs is broken, valid usernam use servic config broken,,,0,0,0,1,0,0,
1449,Failure popup shown for reconfiguring HDFS when MapReduce is not installed, failur popup shown reconfigur hdf map reduc instal,,,0,0,0,0,0,0,
1450,Remove hard-coded stack version, remov hard code stack version,,,0,0,0,0,0,0,
1456,Cannot proceed after bootstrapping in some cases due to a run-time error while running host checks, cannot proceed bootstrap case due run time error run host check,,,0,0,0,0,0,0,
1460,Optimize query call for retrieving host information, optim queri call retriev host inform,Every 15 seconds  Ambari Web makes a call to retrieve information about all hosts in the cluster. With 400+ nodes  this call retrieves about 10MB of info  since the API returns last_agent_env  which is host check results used during bootstrap and makes the payload huge.By optimizing the query string for this API call  we can cut down on the payload by more than 90%., everi second ambari web make call retriev inform host cluster with node call retriev mb info sinc api return last agent env host check result use bootstrap make payload huge by optim queri string api call cut payload,0,0,0,0,1,0,
1461,Optimize query for getting service and host component status back from the server, optim queri get servic host compon statu back server,The API response for getting service/host component status back from the server is unnecessarily big due to nagios_alerts being included as part of the response. This optimization can cut the payload in half or more  and also eases load on the server since it does not have to get Nagios alerts as part of fulfilling the API call., the api respons get servic host compon statu back server unnecessarili big due nagio alert includ part respons thi optim cut payload half also eas load server sinc get nagio alert part fulfil api call,0,0,0,0,1,0,
1465,Minimize Read and Write locks for createHosts, minim read write lock creat host,Invocation count and exec time for ClustersImpl.mapHostToCluser very highorg.apache.ambari.server.state.cluster.ClustersImpl.mapHostToCluster(String  String) Time(ms): 252 925 Avg Time(ms): 5 269 Own Time(ms): 211Invocation Count: 48Each time host is mapped to cluster we refresh the entity manager. This results in the createHosts call taking excess of 10 minutes, invoc count exec time cluster impl map host to cluser highorg apach ambari server state cluster cluster impl map host to cluster string string time ms avg time ms own time ms invoc count each time host map cluster refresh entiti manag thi result creat host call take excess minut,1,0,0,0,1,0,
1466,Optimize ganglia rrd script to be able to respond within reasonable time to queries made by the UI., optim ganglia rrd script abl respond within reason time queri made ui,Optimize ganglia rrd script to be able to respond within reasonable time to queries made by the UI., optim ganglia rrd script abl respond within reason time queri made ui,0,0,0,0,1,0,
1473,Further optimization of querying host information from the server, further optim queri host inform server,Further work on optimizing query for getting host information on top of BUG-1460., further work optim queri get host inform top bug,0,0,0,0,0,0,
1486,Fix TestHostName to take care of issues when gethostname and getfqdn do not match., fix test host name take care issu gethostnam getfqdn match,Fix TestHostName to take care of issues when gethostname and getfqdn do not match., fix test host name take care issu gethostnam getfqdn match,0,0,0,0,0,0,
1487,Fix alerts at host level if MapReduce is not selected not to alert for tasktrackers not running., fix alert host level map reduc select alert tasktrack run,Fix alerts at host level if MapReduce is not selected not to alert for tasktrackers not running., fix alert host level map reduc select alert tasktrack run,1,0,0,0,0,0,
1488,Nagios script causes unwanted  Datanode logs., nagio script caus unwant datanod log,Nagios script causes unwanted Datanode logs., nagio script caus unwant datanod log,1,0,0,0,0,0,
1489,Add hadoop-lzo to be one of the rpms to check for before installation., add hadoop lzo one rpm check instal,Add hadoop-lzo to be one of the rpms to check for before installation., add hadoop lzo one rpm check instal,0,0,0,0,0,0,
1496,Make all service properties reconfigurable., make servic properti reconfigur,,,0,0,0,0,0,0,
1497,Fix start up option for ambari-server where there is a missing space., fix start option ambari server miss space,Fix start up option for ambari-server where there is a missing space., fix start option ambari server miss space,0,0,0,0,0,0,
1499,Add hosts is broken, add host broken,This is due to performance enhancements for querying hosts.Hosts/disk_info for the hosts being added is expected in the Add Hosts wizard  but it is now missing., thi due perform enhanc queri host host disk info host ad expect add host wizard miss,0,0,0,0,1,0,
1505,Hosts page: add filtering by host status, host page add filter host statu,Show the host status filter at the top of the Hosts table  showing the status and the number of hosts in that status., show host statu filter top host tabl show statu number host statu,0,0,0,0,0,0,
1519,Ambari Web goes back and forth between frozen and usable state peridocially on a large cluster, ambari web goe back forth frozen usabl state peridoci larg cluster,The background polling to update the live status of services and host components runs every 6 seconds. When this happens  the whole UI freezes for several seconds periodically., the background poll updat live statu servic host compon run everi second when happen whole ui freez sever second period,0,0,0,0,1,0,
1520,Alerts take around 20-30 seconds to show up everytime you refresh the dashboard., alert take around second show everytim refresh dashboard,Alerts take around 20-30 seconds to show up everytime you refresh the dashboard., alert take around second show everytim refresh dashboard,0,0,0,0,1,0,
1536,Hosts page layout fixes, host page layout fix,1.Use Bootstrap-style tooltip for the hover tooltip on the host status dots as well as the disk usage bars. The current tooltip does not show up unless you rest your mouse cursor for more than a second - users will simply assume there's no tooltip hover when quickly moving the mouse cursor over them.2.We can kill some space between the status dots and the hostnames in the table as well.3.Also  there is too much margin at the top compared to the rest of the pages., use bootstrap style tooltip hover tooltip host statu dot well disk usag bar the current tooltip show unless rest mous cursor second user simpli assum tooltip hover quickli move mous cursor we kill space statu dot hostnam tabl well also much margin top compar rest page,0,0,0,0,0,0,
1537,Constrain the width of all wizard popups, constrain width wizard popup,Currently  the popup for any wizard resizes to fill up the width of the browser width with some horizontal margin. Vertically  it resizes to fit the height of the content; in some cases  it looks really silly on a wide screen.We should specify the max width of the wizard popup. Ambari Web supports two width configurations (wide and narrow)  so the wizard popup should look good in these two configurations., current popup wizard resiz fill width browser width horizont margin vertic resiz fit height content case look realli silli wide screen we specifi max width wizard popup ambari web support two width configur wide narrow wizard popup look good two configur,0,0,0,0,0,0,
1559,Jobs failed count always returns 0 in the jobtracker API metrics, job fail count alway return jobtrack api metric,See the attachments  but after running both successful and failed MapReduce jobs  the jobs submitted count includes all jobs  the jobs successful appears correct  but the jobs failed count is still 0., see attach run success fail map reduc job job submit count includ job job success appear correct job fail count still,0,0,0,0,0,0,
1580,Stack Upgrade Wizard - resume upon page refresh / login, stack upgrad wizard resum upon page refresh login,,,0,0,0,0,0,0,
1581,Host progress popup - generic component for showing progress on async operations, host progress popup gener compon show progress async oper,This is for the popup that shows up upon clicking already-performed or currently-in-progress async operations.We should take what we already have for host-level popup in Step 9 as a base and make it reusable  with a new higher level showing all hosts., thi popup show upon click alreadi perform current progress async oper we take alreadi host level popup step base make reusabl new higher level show host,0,0,0,0,0,0,
1582,Cannot start hadoop services after hdfs re-configuration and amabri server restart., cannot start hadoop servic hdf configur amabri server restart,Cannot start hadoop services after several restarts since the agents., cannot start hadoop servic sever restart sinc agent,0,0,0,0,0,0,
1597,Templeton smoke test fails for secure cluster, templeton smoke test fail secur cluster,Templeton start fails due to multiple errors.[0;36mnotice: /Stage&#91;2&#93;/Hdp-templeton::Copy-hdfs-directories/Hdp-hadoop::Hdfs::Copyfromlocal&#91;/usr/lib/hadoop/contrib/streaming/hadoop-streaming*.jar&#93;/Hdp-hadoop::Exec-hadoop&#91;fs -copyFromLocal /usr/lib/hadoop/contrib/streaming/hadoop-streaming*.jar /apps/webhcat/hadoop-streaming.jar&#93;/Hdp::Exec&#91;/usr/bin/kinit -kt /etc/security/keytabs/hcat.headless.keytab hcat; hadoop --config /etc/hadoop/conf fs -copyFromLocal /usr/lib/hadoop/contrib/streaming/hadoop-streaming*.jar /apps/webhcat/hadoop-streaming.jar&#93;/Exec&#91;/usr/bin/kinit -kt /etc/security/keytabs/hcat.headless.keytab hcat; hadoop --config /etc/hadoop/conf fs -copyFromLocal /usr/lib/hadoop/contrib/streaming/hadoop-streaming*.jar /apps/webhcat/hadoop-streaming.jar&#93;/returns: kinit: Client not found in Kerberos database while getting initial credentials^[[0m, templeton start fail due multipl error mnotic stage hdp templeton copi hdf directori hdp hadoop hdf copyfromloc usr lib hadoop contrib stream hadoop stream jar hdp hadoop exec hadoop fs copi from local usr lib hadoop contrib stream hadoop stream jar app webhcat hadoop stream jar hdp exec usr bin kinit kt etc secur keytab hcat headless keytab hcat hadoop config etc hadoop conf fs copi from local usr lib hadoop contrib stream hadoop stream jar app webhcat hadoop stream jar exec usr bin kinit kt etc secur keytab hcat headless keytab hcat hadoop config etc hadoop conf fs copi from local usr lib hadoop contrib stream hadoop stream jar app webhcat hadoop stream jar return kinit client found kerbero databas get initi credenti,0,0,0,0,0,0,
1621,Config/Reconfig UI should not allow certain configs to have host-level overrides, config reconfig ui allow certain config host level overrid,,,0,0,0,0,0,0,
1631,Security Wizard - integrate host progress popup, secur wizard integr host progress popup,,,0,0,0,0,0,0,
1641,Some map and reduce task metrics are missing for the tasktrackers in the API, some map reduc task metric miss tasktrack api,With Ambari 1.2.2  I can?? get the metrics.mapred object to show up for the tasktracker component. Our code is hitting the URL: http://sdll4474.labs.teradata.com:8080/api/v1/clusters/sdll4474.labs.teradata.com/services/MAPREDUCE/components/TASKTRACKER?fields=host_components/*. Here?? one of the objects in the host_components array. Note that the data in metrics.mapred.tasktracker is providing some data I??e never seen before. In previous versions we say properties such as reduces_running  reduceTaskSlots  maps_running  etc. in this object. { 'href' : 'http://aster39h1.td.teradata.com:8080/api/v1/clusters/aster39h1.td.teradata.com/hosts/byn001-17/host_components/TASKTRACKER'  'HostRoles' : { 'cluster_name' : 'aster39h1.td.teradata.com'  'desired_state' : 'STARTED'  'state' : 'STARTED'  'component_name' : 'TASKTRACKER'  'service_name' : 'MAPREDUCE'  'host_name' : 'byn001-17' }  'metrics' : { 'boottime' : 1.360089758E9  'process' : { 'proc_total' : 845.211111111  'proc_run' : 0.0 }  'rpc' : { 'rpcAuthorizationSuccesses' : 9  'SentBytes' : 6842  'rpcAuthorizationFailures' : 0  'ReceivedBytes' : 26187  'NumOpenConnections' : 0  'callQueueLen' : 0  'RpcQueueTime_num_ops' : 59  'rpcAuthenticationSuccesses' : 0  'RpcProcessingTime_num_ops' : 59  'rpcAuthenticationFailures' : 0  'RpcProcessingTime_avg_time' : 0.6666666666666666  'RpcQueueTime_avg_time' : 0.0 }  'mapred' : { 'shuffleOutput' : { 'shuffle_success_outputs' : 1  'shuffle_handler_busy_percent' : 0.0  'shuffle_output_bytes' : 1400  'shuffle_failed_outputs' : 0  'shuffle_exceptions_caught' : 0 }  'tasktracker' : { 'ConfigVersion' : 'default'  'HttpPort' : 50060  'TasksInfoJson' : '{/'running/':0 /'failed/':0 /'commit_pending/':0}'  'JobTrackerUrl' : 'aster39h1.td.teradata.com:50300'  'Healthy' : true  'Version' : '1.1.2.22  r'  'Hostname' : 'byn001-17'  'RpcPort' : 48526 } }  'ugi' : { 'loginFailure_num_ops' : 0  'loginSuccess_num_ops' : 0  'loginSuccess_avg_time' : 0.0  'loginFailure_avg_time' : 0.0 }  'disk' : { 'disk_total' : 36841.767  'disk_free' : 36776.9775333  'part_max_used' : 70.7 }  'cpu' : { 'cpu_speed' : 1999.0  'cpu_wio' : 0.0  'cpu_num' : 24.0  'cpu_idle' : 99.8886111111  'cpu_nice' : 0.0  'cpu_aidle' : 0.0  'cpu_system' : 0.1  'cpu_user' : 0.0227777777778 }  'rpcdetailed' : { 'getTask_avg_time' : 1.0  'ping_avg_time' : 0.0  'done_avg_time' : 1.0  'getProtocolVersion_avg_time' : 0.0  'getMapCompletionEvents_avg_time' : 0.0  'done_num_ops' : 9  'getMapCompletionEvents_num_ops' : 6  'canCommit_num_ops' : 6  'ping_num_ops' : 2  'commitPending_avg_time' : 1.0  'statusUpdate_num_ops' : 15  'statusUpdate_avg_time' : 1.0  'getTask_num_ops' : 9  'getProtocolVersion_num_ops' : 9  'commitPending_num_ops' : 3  'canCommit_avg_time' : 0.0 }  'load' : { 'load_fifteen' : 0.0  'load_one' : 0.0  'load_five' : 0.0 }  'jvm' : { 'memHeapCommittedM' : 100.4375  'NonHeapMemoryUsed' : 25214472  'logFatal' : 0  'threadsWaiting' : 17  'gcCount' : 122400  'threadsBlocked' : 0  'HeapMemoryUsed' : 77617416  'logWarn' : 0  'logError' : 0  'HeapMemoryMax' : 954466304  'memNonHeapCommittedM' : 26.125  'memNonHeapUsedM' : 24.046394  'gcTimeMillis' : 81352  'NonHeapMemoryMax' : 136314880  'logInfo' : 3  'memHeapUsedM' : 73.81818  'threadsNew' : 0  'threadsTerminated' : 0  'maxMemoryM' : 758.4375  'threadsTimedWaiting' : 10  'threadsRunnable' : 6 }  'network' : { 'pkts_out' : 111.684472222  'bytes_in' : 1428.83666667  'bytes_out' : 23201.8668056  'pkts_in' : 13.9853333333 }  'memory' : { 'mem_total' : 1.31854096E8  'swap_free' : 6291448.0  'mem_buffers' : 574794.711111  'mem_shared' : 0.0  'swap_total' : 6291448.0  'mem_cached' : 6061952.85556  'mem_free' : 1.23072573378E8 } }  'component' : [ { 'href' : 'http://aster39h1.td.teradata.com:8080/api/v1/clusters/aster39h1.td.teradata.com/services/MAPREDUCE/components/TASKTRACKER'  'ServiceComponentInfo' : { 'cluster_name' : 'aster39h1.td.teradata.com'  'component_name' : 'TASKTRACKER'  'service_name' : 'MAPREDUCE' } } ] }, with ambari i get metric mapr object show tasktrack compon our code hit url http sdll lab teradata com api v cluster sdll lab teradata com servic mapreduc compon tasktrack field host compon here one object host compon array note data metric mapr tasktrack provid data i e never seen in previou version say properti reduc run reduc task slot map run etc object href http aster h td teradata com api v cluster aster h td teradata com host byn host compon tasktrack host role cluster name aster h td teradata com desir state start state start compon name tasktrack servic name mapreduc host name byn metric boottim e process proc total proc run rpc rpc author success sent byte rpc author failur receiv byte num open connect call queue len rpc queue time num op rpc authent success rpc process time num op rpc authent failur rpc process time avg time rpc queue time avg time mapr shuffl output shuffl success output shuffl handler busi percent shuffl output byte shuffl fail output shuffl except caught tasktrack config version default http port task info json run fail commit pend job tracker url aster h td teradata com healthi true version r hostnam byn rpc port ugi login failur num op login success num op login success avg time login failur avg time disk disk total disk free part max use cpu cpu speed cpu wio cpu num cpu idl cpu nice cpu aidl cpu system cpu user rpcdetail get task avg time ping avg time done avg time get protocol version avg time get map complet event avg time done num op get map complet event num op commit num op ping num op commit pend avg time statu updat num op statu updat avg time get task num op get protocol version num op commit pend num op commit avg time load load fifteen load one load five jvm mem heap commit m non heap memori use log fatal thread wait gc count thread block heap memori use log warn log error heap memori max mem non heap commit m mem non heap use m gc time milli non heap memori max log info mem heap use m thread new thread termin max memori m thread time wait thread runnabl network pkt byte byte pkt memori mem total e swap free mem buffer mem share swap total mem cach mem free e compon href http aster h td teradata com api v cluster aster h td teradata com servic mapreduc compon tasktrack servic compon info cluster name aster h td teradata com compon name tasktrack servic name mapreduc,0,0,0,0,0,0,
1645,Undo should not be allowed on component hosts, undo allow compon host,,,0,0,0,0,0,0,
1666,Oozie properties for principal and keytab not read from oozie-site, oozi properti princip keytab read oozi site,Oozie principal is create by the UI as oozie/${hostname}@realm.comPuppet script has a bug that does not read this property and uses default 'oozie' as the principal, oozi princip creat ui oozi hostnam realm com puppet script bug read properti use default oozi princip,0,0,0,0,0,0,
1667,Starting all services fails on secure cluster (excluding HBase and ZooKeeper), start servic fail secur cluster exclud h base zoo keeper,HDFS service check failure leads to this.After the failure of the 'HDFS service check' task  stage fails. But HDFS comes up.The Ambari server hostname for secure cluster: ec2-54-234-164-5.compute-1.amazonaws.com, hdf servic check failur lead after failur hdf servic check task stage fail but hdf come the ambari server hostnam secur cluster ec comput amazonaw com,0,0,0,0,0,0,
1702,Ambari/GSInstallers need to set the value of mapred.jobtracker.completeuserjobs.maximum, ambari gs instal need set valu mapr jobtrack completeuserjob maximum,mapred.jobtracker.completeuserjobs.maximum is currently set to 0. This causes issues with failed(/successful) jobs from pig and other job submitters because the references to the failed job is cleared up asap in jobtracker preventing one from accessing the failure reason. This value should be bumped up to about 100 according to the MapReduce team., mapr jobtrack completeuserjob maximum current set thi caus issu fail success job pig job submitt refer fail job clear asap jobtrack prevent one access failur reason thi valu bump accord map reduc team,0,0,0,0,0,0,
1724,Agent has it hard-coded that HDP repo file can only be downloaded once, agent hard code hdp repo file download,Upgrade requires agents to download the repo file anytime., upgrad requir agent download repo file anytim,0,0,0,0,0,0,
1726,It seems upgrades available at the FE is hard-coded to 1.3.0, it seem upgrad avail fe hard code,In order to test upgrade  I modified the available stack definitions to allow upgrade from 1.2.0 to 1.2.2 and removed upgrade to 1.3.0. However  the FE still says '(Upgrade available: HDP-1.3.0)'. However  http://server:8080/api/v1/stacks2/HDP/versions/1.3.0/ has min_upgrade_version as null.{ 'href' : 'http://server:8080/api/v1/stacks2/HDP/versions/1.3.0/'  'Versions' : { 'stack_version' : '1.3.0'  'stack_name' : 'HDP'  'min_upgrade_version' : null } ..., in order test upgrad i modifi avail stack definit allow upgrad remov upgrad howev fe still say upgrad avail hdp howev http server api v stack hdp version min upgrad version null href http server api v stack hdp version version stack version stack name hdp min upgrad version null,0,0,0,0,0,0,
1739,HBase and Zk failed to start on secure install, h base zk fail start secur instal,Error during starting hbase and zookeeper during secure install.Incorrectly parsed Files:hbase-env.shzookeeper-env.sh, error start hbase zookeep secur instal incorrectli pars file hbase env shzookeep env sh,0,0,0,0,0,0,
1752,Backend support for MySQL and Oracle for Oozie and Hive, backend support my sql oracl oozi hive,Add ability to use Oracle DB for Hive and Oozie in Ambari, add abil use oracl db hive oozi ambari,0,0,0,0,0,0,
1757,Add support for Stack 1.2.2 to Ambari, add support stack ambari,Add support for Stack 1.2.2 to Ambari., add support stack ambari,1,0,0,0,0,0,
1764,Unable to get all tasks from more than one request_id by one request, unabl get task one request id one request,When trying to get tasks from more than one request_id returns tasks only for one.request 'api/v1/clusters/mycluster/requests?Requests/id=1|Requests/id=2'returns:{'href' : 'http://ec2-23-20-223-127.compute-1.amazonaws.com:8080/api/v1/clusters/mycluster/requests?Requests/id=1|Requests/id=2' 'items' : [{'href' : 'http://ec2-23-20-223-127.compute-1.amazonaws.com:8080/api/v1/clusters/mycluster/requests/2' 'Requests' :{ 'id' : 2  'cluster_name' : 'mycluster' }}]}, when tri get task one request id return task one request api v cluster myclust request request id request id return href http ec comput amazonaw com api v cluster myclust request request id request id item href http ec comput amazonaw com api v cluster myclust request request id cluster name myclust,0,0,0,0,0,0,
1770,Hue installation fails due to manifest errors, hue instal fail due manifest error,Hue installation fails due to following errors:1. Change in the name of rpm bundle2. Empty values in the hue.ini sections, hue instal fail due follow error chang name rpm bundl empti valu hue ini section,1,0,0,0,0,0,
1775,Security wizard - Javascript error is thrown when zooKeeper is included as a secure service., secur wizard javascript error thrown zoo keeper includ secur servic,,,0,0,0,0,0,0,
1789,Stopping and then Starting all services doesn't start NameNode, stop start servic start name node,Security wizard stops all services  applies configuration and then starts all services.Sometimes it has been noticed that the action to stop all service completes successfully but the action to start all services never sends the task to start NameNode., secur wizard stop servic appli configur start servic sometim notic action stop servic complet success action start servic never send task start name node,1,0,0,0,0,0,
1791,Can not specify request context for smoke test request, can specifi request context smoke test request,Regarding BUG-3509 when we send request to server likeapi/v1/clusters/cl1/services/HDFS/actions/HDFS_SERVICE_CHECKRequest Method:POST Form Data:{'RequestInfo':{'context':'Smoke Test'}}This request is not setting request_context., regard bug send request server likeapi v cluster cl servic hdf action hdf servic check request method post form data request info context smoke test thi request set request context,0,0,0,0,0,0,
1816,Security wizard: Add missing secure configs to Hbase service and make 'zookeeper' as default primary name for zookeeper principal., secur wizard add miss secur config hbase servic make zookeep default primari name zookeep princip,,,1,0,0,1,0,0,
1818,HBase master shuts down immediately after start in a secure cluster., h base master shut immedi start secur cluster,HBase master shuts down immediately after start in a secure cluster.Wrong settings in the hbase_master_jaas. Need to replace the 'HOST with actual fqdn, h base master shut immedi start secur cluster wrong set hbase master jaa need replac host actual fqdn,0,0,0,0,0,0,
1837,Few core-site properties vanished after seemingly benign reconfiguration., few core site properti vanish seemingli benign reconfigur,UI metadata properties are not being reconfigured and retained on saving services., ui metadata properti reconfigur retain save servic,1,0,0,0,0,0,
1838,Cluster Management > Services > MapReduce > Config throws JS error and the page comes up blank, cluster manag servic map reduc config throw js error page come blank,,,1,0,0,0,0,0,
1840,For global properties show restart for appropriate services only, for global properti show restart appropri servic,Currently when one service changes a property which doesnt effect all other services  the rest of the services are marked for restart. This is because the global tags are changed. Using work done in AMBARI-1797  only appropriate services should be marked as needing restart., current one servic chang properti doesnt effect servic rest servic mark restart thi global tag chang use work done ambari appropri servic mark need restart,1,0,0,0,0,0,
1847,Make single PUT call for multiple host overrides, make singl put call multipl host overrid,Currently when we save host-overrides configuration  we have to do PUT of the delta on each host.This is problematic if admin provides an exception to 100 hosts. This will require 100 PUT calls which is expensive. There is a bulk update mechanism  but that requires the same content for all 100 hosts. This will not be the case if any hosts have other properties that are overridden.To save on network calls  we have to make one PUT call via the new API provided by AMBARI-1844., current save host overrid configur put delta host thi problemat admin provid except host thi requir put call expens there bulk updat mechan requir content host thi case host properti overridden to save network call make one put call via new api provid ambari,1,0,0,0,0,0,
1849,Cosmetic problems on HBase Dashboard, cosmet problem h base dashboard,,,0,0,0,0,0,0,
1855,Capacity Scheduler: when adding a new queue  populate fields, capac schedul ad new queue popul field,When creating a new queue  populate all fields with default values except for:Queue NameCapacityMax CapacityUsersGroupsAdmin UsersAdmin Groups, when creat new queue popul field default valu except queue name capac max capac user group admin user admin group,1,0,0,0,0,0,
1859,Cannot load Nagios Alerts due to 400 Bad Request, cannot load nagio alert due bad request,Given the API call: http://dev.hortonworks.com:8080/api/v1/clusters/test403_2/host_components?HostRoles/component_name=NAGIOS_SERVER&amp;fields=HostRoles/nagios_alertsThe feedback is:{ 'status' : 400  'message' : 'The properties [HostRoles/nagios_alerts] specified in the request or predicate are not supported for the resource type HostComponent.' }, given api call http dev hortonwork com api v cluster test host compon host role compon name nagio server amp field host role nagio alert the feedback statu messag the properti host role nagio alert specifi request predic support resourc type host compon,1,0,0,1,0,0,
1860,Master broken - Cannot deploy services, master broken cannot deploy servic,Datanode install fails on multi-node clusters because the following configuration property:$ambari_db_rca_url= 'jdbc:postgresql://localhost/ambarirca'Ensure all of these variables are wired up:'ambari_db_rca_url''ambari_db_rca_driver''ambari_db_rca_username''ambari_db_rca_password', datanod instal fail multi node cluster follow configur properti ambari db rca url jdbc postgresql localhost ambarirca ensur variabl wire ambari db rca url ambari db rca driver ambari db rca usernam ambari db rca password,0,0,0,0,0,0,
1870,ambari-agent RPM claims ownership of /usr/sbin, ambari agent rpm claim ownership usr sbin,This may impact other versions  only branch-1.2 was changed.The ambari-agent.spec (generated from rpm-maven-plugin) claims ownership of /usr/sbin $ grep sbin target/rpm/ambari-agent/SPECS/ambari-agent.spec | grep attr%attr(755 root root) /usr/sbinThis is a problem because the filesystem RPM owns /usr/sbin.According to rpm-maven-plugin documentation&#91;0&#93;  this is because the only file under /usr/sbin is ambari-agent and'directoryIncludedIf the value is true then the attribute string will be written for the directory if the sources identify all of the files in the directory (that is  no other mapping contributed files to the directory). This is the default behavior.'The 'no other mapping contributed files to the directory' bit is important.The solution is to add directoryInclude=false to the mapping.&#91;0&#93; http://mojo.codehaus.org/rpm-maven-plugin/map-params.html, thi may impact version branch chang the ambari agent spec gener rpm maven plugin claim ownership usr sbin grep sbin target rpm ambari agent spec ambari agent spec grep attr attr root root usr sbin thi problem filesystem rpm own usr sbin accord rpm maven plugin document file usr sbin ambari agent directori includ if valu true attribut string written directori sourc identifi file directori map contribut file directori thi default behavior the map contribut file directori bit import the solut add directori includ fals map http mojo codehau org rpm maven plugin map param html,0,0,0,0,0,0,
1872,Ambari FE is not setting proper value for fs.checkpoint.edits.dir, ambari fe set proper valu fs checkpoint edit dir,,,1,0,0,0,0,0,
1873,HUE pid and log dir labels are flip flopped, hue pid log dir label flip flop,,,0,0,0,0,0,0,
1876,Capacity Scheduler: implement user/group and admin user/group validation rules, capac schedul implement user group admin user group valid rule,,,1,0,0,0,0,0,
1877,Reassign Master Wizard  Step 2: prevent proceed next without changing target host, reassign master wizard step prevent proceed next without chang target host,,,1,0,0,0,0,0,
1880,stacks2 API uses 'type' to refer to config tags and no longer exposes 'filename' as a property, stack api use type refer config tag longer expos filenam properti,FE needs to be modified to not to use 'filename' in stacks API to get the config tags. The new property is 'type'., fe need modifi use filenam stack api get config tag the new properti type,1,0,0,0,0,0,
1881,API to map global properties to services is partially complete., api map global properti servic partial complet,API to map global properties to services is partially complete. The API gives 'global.xml' to approximately 47 properties. However the 'global' site has many more properties - so I dont know if some are fully missed., api map global properti servic partial complet the api give global xml approxim properti howev global site mani properti i dont know fulli miss,1,0,0,0,1,0,
1891,Impossibility to scroll metric window after browser width changing, imposs scroll metric window browser width chang,Open detailed view of any metric diagram on Services page  for example 'Total Space Utilization'. After that change width of browser less than width of detailed view of metric diagram. Right arrow for graph time paging is invisible and we can't to resolve this problem using horizontal scrollbar., open detail view metric diagram servic page exampl total space util after chang width browser less width detail view metric diagram right arrow graph time page invis resolv problem use horizont scrollbar,0,0,0,0,0,0,
1896,Disable editing Capacity Scheduler on host configs, disabl edit capac schedul host config,Go to Configs tab of Host.Result: Queues in Capacity Scheduler category are editable.Expected: Queues in Capacity Scheduler category shouldn't be editable., go config tab host result queue capac schedul categori edit expect queue capac schedul categori edit,1,0,0,0,0,0,
1904,Update default stack version to 1.3.0, updat default stack version,,,1,0,0,0,0,0,
1912,HBase master doesn't come up after disabling security., h base master come disabl secur,,,1,0,0,0,0,0,
1915,Client install tasks are shown twice in install progress popup, client instal task shown twice instal progress popup,Client is re-configured by re-installing all client only hosts. This results in multiple tasks for client install in the UI.API response:{ 'href' : 'http://ec2-50-17-99-21.compute-1.amazonaws.com:8080/api/v1/clusters/yusaku/requests/2'  'Requests' : { 'id' : 2  'cluster_name' : 'yusaku' }  'tasks' : [ { 'href' : 'http://ec2-50-17-99-21.compute-1.amazonaws.com:8080/api/v1/clusters/yusaku/requests/2/tasks/43'  'Tasks' : { 'exit_code' : 0  'stdout' : 'notice: /Stage[2]/Hdp-oozie/Configgenerator::Configfile[oozie-site]/File[/etc/oozie/conf/oozie-site.xml]/content: content changed '{md5}eaf59cc452c92e64b559071586150a08' to '{md5}cb15303aab1c384d19c102a6ce650ed2'/nnotice: Finished catalog run in 2.04 seconds'  'status' : 'COMPLETED'  'stderr' : 'none'  'host_name' : 'ip-10-40-19-235.ec2.internal'  'id' : 43  'cluster_name' : 'yusaku'  'attempt_cnt' : 1  'request_id' : 2  'command' : 'INSTALL'  'role' : 'OOZIE_CLIENT'  'start_time' : 1365743356709  'stage_id' : 1 } }  { 'href' : 'http://ec2-50-17-99-21.compute-1.amazonaws.com:8080/api/v1/clusters/yusaku/requests/2/tasks/51'  'Tasks' : { 'exit_code' : 999  'stdout' : ''  'status' : 'QUEUED'  'stderr' : ''  'host_name' : 'ip-10-40-19-235.ec2.internal'  'id' : 51  'cluster_name' : 'yusaku'  'attempt_cnt' : 1  'request_id' : 2  'command' : 'START'  'role' : 'GANGLIA_MONITOR'  'start_time' : 1365743407043  'stage_id' : 2 } }  { 'href' : 'http://ec2-50-17-99-21.compute-1.amazonaws.com:8080/api/v1/clusters/yusaku/requests/2/tasks/41'  'Tasks' : { 'exit_code' : 0  'stdout' : 'notice: Finished catalog run in 2.22 seconds'  'status' : 'COMPLETED'  'stderr' : 'none'  'host_name' : 'ip-10-40-19-235.ec2.internal'  'id' : 41  'cluster_name' : 'yusaku'  'attempt_cnt' : 1  'request_id' : 2  'command' : 'INSTALL'  'role' : 'HCAT'  'start_time' : 1365743356684  'stage_id' : 1 } }  { 'href' : 'http://ec2-50-17-99-21.compute-1.amazonaws.com:8080/api/v1/clusters/yusaku/requests/2/tasks/54'  'Tasks' : { 'exit_code' : 999  'stdout' : ''  'status' : 'QUEUED'  'stderr' : ''  'host_name' : 'ip-10-80-81-236.ec2.internal'  'id' : 54  'cluster_name' : 'yusaku'  'attempt_cnt' : 1  'request_id' : 2  'command' : 'START'  'role' : 'GANGLIA_SERVER'  'start_time' : 1365743407078  'stage_id' : 2 } }  { 'href' : 'http://ec2-50-17-99-21.compute-1.amazonaws.com:8080/api/v1/clusters/yusaku/requests/2/tasks/55'  'Tasks' : { 'exit_code' : 999  'stdout' : ''  'status' : 'QUEUED'  'stderr' : ''  'host_name' : 'ip-10-80-81-236.ec2.internal'  'id' : 55  'cluster_name' : 'yusaku'  'attempt_cnt' : 1  'request_id' : 2  'command' : 'START'  'role' : 'NAGIOS_SERVER'  'start_time' : 1365743407100  'stage_id' : 2 } }  { 'href' : 'http://ec2-50-17-99-21.compute-1.amazonaws.com:8080/api/v1/clusters/yusaku/requests/2/tasks/50'  'Tasks' : { 'exit_code' : 999  'stdout' : ''  'status' : 'QUEUED'  'stderr' : ''  'host_name' : 'ip-10-40-19-235.ec2.internal'  'id' : 50  'cluster_name' : 'yusaku'  'attempt_cnt' : 1  'request_id' : 2  'command' : 'START'  'role' : 'DATANODE'  'start_time' : 1365743407032  'stage_id' : 2 } }  { 'href' : 'http://ec2-50-17-99-21.compute-1.amazonaws.com:8080/api/v1/clusters/yusaku/requests/2/tasks/67'  'Tasks' : { 'exit_code' : 999  'stdout' : ''  'status' : 'PENDING'  'stderr' : ''  'host_name' : 'ip-10-40-19-235.ec2.internal'  'id' : 67  'cluster_name' : 'yusaku'  'attempt_cnt' : 0  'request_id' : 2  'command' : 'START'  'role' : 'HBASE_REGIONSERVER'  'start_time' : -1  'stage_id' : 4 } }  { 'href' : 'http://ec2-50-17-99-21.compute-1.amazonaws.com:8080/api/v1/clusters/yusaku/requests/2/tasks/47'  'Tasks' : { 'exit_code' : 0  'stdout' : 'warning: Dynamic lookup of $hadoop_heapsize is deprecated. Support will be removed in Puppet 2.8. Use a fully-qualified variable name (e.g.  $classname::variable) or parameterized classes./nnotice: /Stage[2]/Hdp-hive/Configgenerator::Configfile[hive-site]/File[/etc/hive/conf/hive-site.xml]/content: content changed '{md5}29d1def766d4aadfddbf38db13a2712e' to '{md5}2d26829fd012bf5f195e760fc8eeb7f9'/nnotice: Finished catalog run in 2.84 seconds'  'status' : 'COMPLETED'  'stderr' : 'none'  'host_name' : 'ip-10-80-81-236.ec2.internal'  'id' : 47  'cluster_name' : 'yusaku'  'attempt_cnt' : 1  'request_id' : 2  'command' : 'INSTALL'  'role' : 'HIVE_CLIENT'  'start_time' : 1365743356758  'stage_id' : 1 } }  { 'href' : 'http://ec2-50-17-99-21.compute-1.amazonaws.com:8080/api/v1/clusters/yusaku/requests/2/tasks/45'  'Tasks' : { 'exit_code' : 0  'stdout' : 'notice: Finished catalog run in 2.24 seconds'  'status' : 'COMPLETED'  'stderr' : 'none'  'host_name' : 'ip-10-40-19-235.ec2.internal'  'id' : 45  'cluster_name' : 'yusaku'  'attempt_cnt' : 1  'request_id' : 2  'command' : 'INSTALL'  'role' : 'SQOOP'  'start_time' : 1365743356733  'stage_id' : 1 } }  { 'href' : 'http://ec2-50-17-99-21.compute-1.amazonaws.com:8080/api/v1/clusters/yusaku/requests/2/tasks/44'  'Tasks' : { 'exit_code' : 0  'stdout' : 'notice: Finished catalog run in 2.13 seconds'  'status' : 'COMPLETED'  'stderr' : 'none'  'host_name' : 'ip-10-40-19-235.ec2.internal'  'id' : 44  'cluster_name' : 'yusaku'  'attempt_cnt' : 1  'request_id' : 2  'command' : 'INSTALL'  'role' : 'PIG'  'start_time' : 1365743356721  'stage_id' : 1 } }  { 'href' : 'http://ec2-50-17-99-21.compute-1.amazonaws.com:8080/api/v1/clusters/yusaku/requests/2/tasks/56'  'Tasks' : { 'exit_code' : 999  'stdout' : ''  'status' : 'QUEUED'  'stderr' : ''  'host_name' : 'ip-10-80-81-236.ec2.internal'  'id' : 56  'cluster_name' : 'yusaku'  'attempt_cnt' : 1  'request_id' : 2  'command' : 'START'  'role' : 'NAMENODE'  'start_time' : 1365743407109  'stage_id' : 2 } }  { 'href' : 'http://ec2-50-17-99-21.compute-1.amazonaws.com:8080/api/v1/clusters/yusaku/requests/2/tasks/52'  'Tasks' : { 'exit_code' : 999  'stdout' : ''  'status' : 'QUEUED'  'stderr' : ''  'host_name' : 'ip-10-40-19-235.ec2.internal'  'id' : 52  'cluster_name' : 'yusaku'  'attempt_cnt' : 1  'request_id' : 2  'command' : 'START'  'role' : 'ZOOKEEPER_SERVER'  'start_time' : 1365743407062  'stage_id' : 2 } }  { 'href' : 'http://ec2-50-17-99-21.compute-1.amazonaws.com:8080/api/v1/clusters/yusaku/requests/2/tasks/42'  'Tasks' : { 'exit_code' : 0  'stdout' : 'warning: Dynamic lookup of $hadoop_heapsize is deprecated. Support will be removed in Puppet 2.8. Use a fully-qualified variable name (e.g.  $classname::variable) or parameterized classes./nnotice: /Stage[2]/Hdp-hive/Configgenerator::Configfile[hive-site]/File[/etc/hive/conf/hive-site.xml]/content: content changed '{md5}27e517fec40f6157f75eb3116d5387bf' to '{md5}54ff14d0a6d9968e900f28853125b294'/nnotice: Finished catalog run in 2.28 seconds'  'status' : 'COMPLETED'  'stderr' : 'none'  'host_name' : 'ip-10-40-19-235.ec2.internal'  'id' : 42  'cluster_name' : 'yusaku'  'attempt_cnt' : 1  'request_id' : 2  'command' : 'INSTALL'  'role' : 'HIVE_CLIENT'  'start_time' : 1365743356697  'stage_id' : 1 } }  { 'href' : 'http://ec2-50-17-99-21.compute-1.amazonaws.com:8080/api/v1/clusters/yusaku/requests/2/tasks/63'  'Tasks' : { 'exit_code' : 999  'stdout' : ''  'status' : 'PENDING'  'stderr' : ''  'host_name' : 'ip-10-80-81-236.ec2.internal'  'id' : 63  'cluster_name' : 'yusaku'  'attempt_cnt' : 0  'request_id' : 2  'command' : 'START'  'role' : 'HBASE_MASTER'  'start_time' : -1  'stage_id' : 3 } }  { 'href' : 'http://ec2-50-17-99-21.compute-1.amazonaws.com:8080/api/v1/clusters/yusaku/requests/2/tasks/62'  'Tasks' : { 'exit_code' : 999  'stdout' : ''  'status' : 'PENDING'  'stderr' : ''  'host_name' : 'ip-10-40-19-235.ec2.internal'  'id' : 62  'cluster_name' : 'yusaku'  'attempt_cnt' : 0  'request_id' : 2  'command' : 'START'  'role' : 'TASKTRACKER'  'start_time' : -1  'stage_id' : 3 } }  { 'href' : 'http://ec2-50-17-99-21.compute-1.amazonaws.com:8080/api/v1/clusters/yusaku/requests/2/tasks/46'  'Tasks' : { 'exit_code' : 0  'stdout' : 'notice: Finished catalog run in 2.77 seconds'  'status' : 'COMPLETED'  'stderr' : 'none'  'host_name' : 'ip-10-80-81-236.ec2.internal'  'id' : 46  'cluster_name' : 'yusaku'  'attempt_cnt' : 1  'request_id' : 2  'command' : 'INSTALL'  'role' : 'HCAT'  'start_time' : 1365743356744  'stage_id' : 1 } }  { 'href' : 'http://ec2-50-17-99-21.compute-1.amazonaws.com:8080/api/v1/clusters/yusaku/requests/2/tasks/58'  'Tasks' : { 'exit_code' : 999  'stdout' : ''  'status' : 'QUEUED'  'stderr' : ''  'host_name' : 'ip-10-85-70-140.ec2.internal'  'id' : 58  'cluster_name' : 'yusaku'  'attempt_cnt' : 1  'request_id' : 2  'command' : 'START'  'role' : 'GANGLIA_MONITOR'  'start_time' : 1365743407125  'stage_id' : 2 } }  { 'href' : 'http://ec2-50-17-99-21.compute-1.amazonaws.com:8080/api/v1/clusters/yusaku/requests/2/tasks/60'  'Tasks' : { 'exit_code' : 999  'stdout' : ''  'status' : 'QUEUED'  'stderr' : ''  'host_name' : 'ip-10-85-70-140.ec2.internal'  'id' : 60  'cluster_name' : 'yusaku'  'attempt_cnt' : 1  'request_id' : 2  'command' : 'START'  'role' : 'ZOOKEEPER_SERVER'  'start_time' : 1365743407164  'stage_id' : 2 } }  { 'href' : 'http://ec2-50-17-99-21.compute-1.amazonaws.com:8080/api/v1/clusters/yusaku/requests/2/tasks/69'  'Tasks' : { 'exit_code' : 999  'stdout' : ''  'status' : 'PENDING'  'stderr' : ''  'host_name' : 'ip-10-85-70-140.ec2.internal'  'id' : 69  'cluster_name' : 'yusaku'  'attempt_cnt' : 0  'request_id' : 2  'command' : 'START'  'role' : 'OOZIE_SERVER'  'start_time' : -1  'stage_id' : 4 } }  { 'href' : 'http://ec2-50-17-99-21.compute-1.amazonaws.com:8080/api/v1/clusters/yusaku/requests/2/tasks/64'  'Tasks' : { 'exit_code' : 999  'stdout' : ''  'status' : 'PENDING'  'stderr' : ''  'host_name' : 'ip-10-85-70-140.ec2.internal'  'id' : 64  'cluster_name' : 'yusaku'  'attempt_cnt' : 0  'request_id' : 2  'command' : 'START'  'role' : 'HIVE_METASTORE'  'start_time' : -1  'stage_id' : 3 } }  { 'href' : 'http://ec2-50-17-99-21.compute-1.amazonaws.com:8080/api/v1/clusters/yusaku/requests/2/tasks/48'  'Tasks' : { 'exit_code' : 0  'stdout' : 'warning: Dynamic lookup of $service_state at /var/lib/ambari-agent/puppet/modules/hdp-hadoop/manifests/init.pp:213 is deprecated. Support will be removed in Puppet 2.8. Use a fully-qualified variable name (e.g.  $classname::variable) or parameterized classes./nwarning: Dynamic lookup of $tasktracker_port is deprecated. Support will be removed in Puppet 2.8. Use a fully-qualified variable name (e.g.  $classname::variable) or parameterized classes./nwarning: Dynamic lookup of $ambari_db_rca_url is deprecated. Support will be removed in Puppet 2.8. Use a fully-qualified variable name (e.g.  $classname::variable) or parameterized classes./nwarning: Dynamic lookup of $ambari_db_rca_driver is deprecated. Support will be removed in Puppet 2.8. Use a fully-qualified variable name (e.g.  $classname::variable) or parameterized classes./nwarning: Dynamic lookup of $ambari_db_rca_username is deprecated. Support will be removed in Puppet 2.8. Use a fully-qualified variable name (e.g.  $classname::variable) or parameterized classes./nwarning: Dynamic lookup of $ambari_db_rca_password is deprecated. Support will be removed in Puppet 2.8. Use a fully-qualified variable name (e.g.  $classname::variable) or parameterized classes./nnotice: /Stage[2]/Hdp-hadoop::Initialize/Configgenerator::Configfile[core-site]/File[/etc/hadoop/conf/core-site.xml]/content: content changed '{md5}95bdcddd064261ac3a00d8c0a7f79fee' to '{md5}d6f5b9646bf280e915e3b0d42ed622a9'/nnotice: /Stage[2]/Hdp-hadoop::Initialize/Configgenerator::Configfile[hdfs-site]/File[/etc/hadoop/conf/hdfs-site.xml]/content: content changed '{md5}5f83b57cbac46a0b7007ed94720a8c3b' to '{md5}e0e38c4dc10fc81b12637e34796ced70'/nnotice: /Stage[2]/Hdp-hadoop::Initialize/Configgenerator::Configfile[mapred-site]/File[/etc/hadoop/conf/mapred-site.xml]/content: content changed '{md5}42b54b8e096eafa7ba53c8f5b53bda3e' to '{md5}409f4680bc7871db2864afecbcefdb36'/nnotice: Finished catalog run in 3.67 seconds'  'status' : 'COMPLETED'  'stderr' : 'none'  'host_name' : 'ip-10-80-81-236.ec2.internal'  'id' : 48  'cluster_name' : 'yusaku'  'attempt_cnt' : 1  'request_id' : 2  'command' : 'INSTALL'  'role' : 'MAPREDUCE_CLIENT'  'start_time' : 1365743356769  'stage_id' : 1 } }  { 'href' : 'http://ec2-50-17-99-21.compute-1.amazonaws.com:8080/api/v1/clusters/yusaku/requests/2/tasks/70'  'Tasks' : { 'exit_code' : 999  'stdout' : ''  'status' : 'PENDING'  'stderr' : ''  'host_name' : 'ip-10-85-70-140.ec2.internal'  'id' : 70  'cluster_name' : 'yusaku'  'attempt_cnt' : 0  'request_id' : 2  'command' : 'START'  'role' : 'WEBHCAT_SERVER'  'start_time' : -1  'stage_id' : 5 } }  { 'href' : 'http://ec2-50-17-99-21.compute-1.amazonaws.com:8080/api/v1/clusters/yusaku/requests/2/tasks/61'  'Tasks' : { 'exit_code' : 999  'stdout' : ''  'status' : 'PENDING'  'stderr' : ''  'host_name' : 'ip-10-40-19-235.ec2.internal'  'id' : 61  'cluster_name' : 'yusaku'  'attempt_cnt' : 0  'request_id' : 2  'command' : 'START'  'role' : 'HBASE_MASTER'  'start_time' : -1  'stage_id' : 3 } }  { 'href' : 'http://ec2-50-17-99-21.compute-1.amazonaws.com:8080/api/v1/clusters/yusaku/requests/2/tasks/66'  'Tasks' : { 'exit_code' : 999  'stdout' : ''  'status' : 'PENDING'  'stderr' : ''  'host_name' : 'ip-10-85-70-140.ec2.internal'  'id' : 66  'cluster_name' : 'yusaku'  'attempt_cnt' : 0  'request_id' : 2  'command' : 'START'  'role' : 'SECONDARY_NAMENODE'  'start_time' : -1  'stage_id' : 3 } }  { 'href' : 'http://ec2-50-17-99-21.compute-1.amazonaws.com:8080/api/v1/clusters/yusaku/requests/2/tasks/49'  'Tasks' : { 'exit_code' : 0  'stdout' : 'notice: /Stage[2]/Hdp-oozie/Configgenerator::Configfile[oozie-site]/File[/etc/oozie/conf/oozie-site.xml]/content: content changed '{md5}001c4940080d2ea315a9720676f1bcad' to '{md5}282f1e354fe7f9da0f6de43425a40d40'/nnotice: Finished catalog run in 2.24 seconds'  'status' : 'COMPLETED'  'stderr' : 'none'  'host_name' : 'ip-10-80-81-236.ec2.internal'  'id' : 49  'cluster_name' : 'yusaku'  'attempt_cnt' : 1  'request_id' : 2  'command' : 'INSTALL'  'role' : 'OOZIE_CLIENT'  'start_time' : 1365743356796  'stage_id' : 1 } }  { 'href' : 'http://ec2-50-17-99-21.compute-1.amazonaws.com:8080/api/v1/clusters/yusaku/requests/2/tasks/53'  'Tasks' : { 'exit_code' : 999  'stdout' : ''  'status' : 'QUEUED'  'stderr' : ''  'host_name' : 'ip-10-80-81-236.ec2.internal'  'id' : 53  'cluster_name' : 'yusaku'  'attempt_cnt' : 1  'request_id' : 2  'command' : 'START'  'role' : 'GANGLIA_MONITOR'  'start_time' : 1365743407070  'stage_id' : 2 } }  { 'href' : 'http://ec2-50-17-99-21.compute-1.amazonaws.com:8080/api/v1/clusters/yusaku/requests/2/tasks/68'  'Tasks' : { 'exit_code' : 999  'stdout' : ''  'status' : 'PENDING'  'stderr' : ''  'host_name' : 'ip-10-85-70-140.ec2.internal'  'id' : 68  'cluster_name' : 'yusaku'  'attempt_cnt' : 0  'request_id' : 2  'command' : 'START'  'role' : 'HIVE_SERVER'  'start_time' : -1  'stage_id' : 4 } }  { 'href' : 'http://ec2-50-17-99-21.compute-1.amazonaws.com:8080/api/v1/clusters/yusaku/requests/2/tasks/65'  'Tasks' : { 'exit_code' : 999  'stdout' : ''  'status' : 'PENDING'  'stderr' : ''  'host_name' : 'ip-10-85-70-140.ec2.internal'  'id' : 65  'cluster_name' : 'yusaku'  'attempt_cnt' : 0  'request_id' : 2  'command' : 'START'  'role' : 'JOBTRACKER'  'start_time' : -1  'stage_id' : 3 } }  { 'href' : 'http://ec2-50-17-99-21.compute-1.amazonaws.com:8080/api/v1/clusters/yusaku/requests/2/tasks/57'  'Tasks' : { 'exit_code' : 999  'stdout' : ''  'status' : 'QUEUED'  'stderr' : ''  'host_name' : 'ip-10-80-81-236.ec2.internal'  'id' : 57  'cluster_name' : 'yusaku'  'attempt_cnt' : 1  'request_id' : 2  'command' : 'START'  'role' : 'ZOOKEEPER_SERVER'  'start_time' : 1365743407117  'stage_id' : 2 } }  { 'href' : 'http://ec2-50-17-99-21.compute-1.amazonaws.com:8080/api/v1/clusters/yusaku/requests/2/tasks/59'  'Tasks' : { 'exit_code' : 999  'stdout' : ''  'status' : 'QUEUED'  'stderr' : ''  'host_name' : 'ip-10-85-70-140.ec2.internal'  'id' : 59  'cluster_name' : 'yusaku'  'attempt_cnt' : 1  'request_id' : 2  'command' : 'START'  'role' : 'MYSQL_SERVER'  'start_time' : 1365743407156  'stage_id' : 2 } } ]}, client configur instal client host thi result multipl task client instal ui api respons href http ec comput amazonaw com api v cluster yusaku request request id cluster name yusaku task href http ec comput amazonaw com api v cluster yusaku request task task exit code stdout notic stage hdp oozi configgener configfil oozi site file etc oozi conf oozi site xml content content chang md eaf cc c e b md cb aab c c ce ed nnotic finish catalog run second statu complet stderr none host name ip ec intern id cluster name yusaku attempt cnt request id command instal role oozi client start time stage id href http ec comput amazonaw com api v cluster yusaku request task task exit code stdout statu queu stderr host name ip ec intern id cluster name yusaku attempt cnt request id command start role ganglia monitor start time stage id href http ec comput amazonaw com api v cluster yusaku request task task exit code stdout notic finish catalog run second statu complet stderr none host name ip ec intern id cluster name yusaku attempt cnt request id command instal role hcat start time stage id href http ec comput amazonaw com api v cluster yusaku request task task exit code stdout statu queu stderr host name ip ec intern id cluster name yusaku attempt cnt request id command start role ganglia server start time stage id href http ec comput amazonaw com api v cluster yusaku request task task exit code stdout statu queu stderr host name ip ec intern id cluster name yusaku attempt cnt request id command start role nagio server start time stage id href http ec comput amazonaw com api v cluster yusaku request task task exit code stdout statu queu stderr host name ip ec intern id cluster name yusaku attempt cnt request id command start role datanod start time stage id href http ec comput amazonaw com api v cluster yusaku request task task exit code stdout statu pend stderr host name ip ec intern id cluster name yusaku attempt cnt request id command start role hbase regionserv start time stage id href http ec comput amazonaw com api v cluster yusaku request task task exit code stdout warn dynam lookup hadoop heapsiz deprec support remov puppet use fulli qualifi variabl name e g classnam variabl parameter class nnotic stage hdp hive configgener configfil hive site file etc hive conf hive site xml content content chang md def aadfddbf db e md fd bf f e fc eeb f nnotic finish catalog run second statu complet stderr none host name ip ec intern id cluster name yusaku attempt cnt request id command instal role hive client start time stage id href http ec comput amazonaw com api v cluster yusaku request task task exit code stdout notic finish catalog run second statu complet stderr none host name ip ec intern id cluster name yusaku attempt cnt request id command instal role sqoop start time stage id href http ec comput amazonaw com api v cluster yusaku request task task exit code stdout notic finish catalog run second statu complet stderr none host name ip ec intern id cluster name yusaku attempt cnt request id command instal role pig start time stage id href http ec comput amazonaw com api v cluster yusaku request task task exit code stdout statu queu stderr host name ip ec intern id cluster name yusaku attempt cnt request id command start role namenod start time stage id href http ec comput amazonaw com api v cluster yusaku request task task exit code stdout statu queu stderr host name ip ec intern id cluster name yusaku attempt cnt request id command start role zookeep server start time stage id href http ec comput amazonaw com api v cluster yusaku request task task exit code stdout warn dynam lookup hadoop heapsiz deprec support remov puppet use fulli qualifi variabl name e g classnam variabl parameter class nnotic stage hdp hive configgener configfil hive site file etc hive conf hive site xml content content chang md e fec f f eb bf md ff e f b nnotic finish catalog run second statu complet stderr none host name ip ec intern id cluster name yusaku attempt cnt request id command instal role hive client start time stage id href http ec comput amazonaw com api v cluster yusaku request task task exit code stdout statu pend stderr host name ip ec intern id cluster name yusaku attempt cnt request id command start role hbase master start time stage id href http ec comput amazonaw com api v cluster yusaku request task task exit code stdout statu pend stderr host name ip ec intern id cluster name yusaku attempt cnt request id command start role tasktrack start time stage id href http ec comput amazonaw com api v cluster yusaku request task task exit code stdout notic finish catalog run second statu complet stderr none host name ip ec intern id cluster name yusaku attempt cnt request id command instal role hcat start time stage id href http ec comput amazonaw com api v cluster yusaku request task task exit code stdout statu queu stderr host name ip ec intern id cluster name yusaku attempt cnt request id command start role ganglia monitor start time stage id href http ec comput amazonaw com api v cluster yusaku request task task exit code stdout statu queu stderr host name ip ec intern id cluster name yusaku attempt cnt request id command start role zookeep server start time stage id href http ec comput amazonaw com api v cluster yusaku request task task exit code stdout statu pend stderr host name ip ec intern id cluster name yusaku attempt cnt request id command start role oozi server start time stage id href http ec comput amazonaw com api v cluster yusaku request task task exit code stdout statu pend stderr host name ip ec intern id cluster name yusaku attempt cnt request id command start role hive metastor start time stage id href http ec comput amazonaw com api v cluster yusaku request task task exit code stdout warn dynam lookup servic state var lib ambari agent puppet modul hdp hadoop manifest init pp deprec support remov puppet use fulli qualifi variabl name e g classnam variabl parameter class nwarn dynam lookup tasktrack port deprec support remov puppet use fulli qualifi variabl name e g classnam variabl parameter class nwarn dynam lookup ambari db rca url deprec support remov puppet use fulli qualifi variabl name e g classnam variabl parameter class nwarn dynam lookup ambari db rca driver deprec support remov puppet use fulli qualifi variabl name e g classnam variabl parameter class nwarn dynam lookup ambari db rca usernam deprec support remov puppet use fulli qualifi variabl name e g classnam variabl parameter class nwarn dynam lookup ambari db rca password deprec support remov puppet use fulli qualifi variabl name e g classnam variabl parameter class nnotic stage hdp hadoop initi configgener configfil core site file etc hadoop conf core site xml content content chang md bdcddd ac c f fee md f b bf e e b ed nnotic stage hdp hadoop initi configgener configfil hdf site file etc hadoop conf hdf site xml content content chang md f b cbac b ed c b md e e c dc fc b e ced nnotic stage hdp hadoop initi configgener configfil mapr site file etc hadoop conf mapr site xml content content chang md b b e eafa ba c f b bda e md f bc db afecbcefdb nnotic finish catalog run second statu complet stderr none host name ip ec intern id cluster name yusaku attempt cnt request id command instal role mapreduc client start time stage id href http ec comput amazonaw com api v cluster yusaku request task task exit code stdout statu pend stderr host name ip ec intern id cluster name yusaku attempt cnt request id command start role webhcat server start time stage id href http ec comput amazonaw com api v cluster yusaku request task task exit code stdout statu pend stderr host name ip ec intern id cluster name yusaku attempt cnt request id command start role hbase master start time stage id href http ec comput amazonaw com api v cluster yusaku request task task exit code stdout statu pend stderr host name ip ec intern id cluster name yusaku attempt cnt request id command start role secondari namenod start time stage id href http ec comput amazonaw com api v cluster yusaku request task task exit code stdout notic stage hdp oozi configgener configfil oozi site file etc oozi conf oozi site xml content content chang md c ea f bcad md f e fe f da f de nnotic finish catalog run second statu complet stderr none host name ip ec intern id cluster name yusaku attempt cnt request id command instal role oozi client start time stage id href http ec comput amazonaw com api v cluster yusaku request task task exit code stdout statu queu stderr host name ip ec intern id cluster name yusaku attempt cnt request id command start role ganglia monitor start time stage id href http ec comput amazonaw com api v cluster yusaku request task task exit code stdout statu pend stderr host name ip ec intern id cluster name yusaku attempt cnt request id command start role hive server start time stage id href http ec comput amazonaw com api v cluster yusaku request task task exit code stdout statu pend stderr host name ip ec intern id cluster name yusaku attempt cnt request id command start role jobtrack start time stage id href http ec comput amazonaw com api v cluster yusaku request task task exit code stdout statu queu stderr host name ip ec intern id cluster name yusaku attempt cnt request id command start role zookeep server start time stage id href http ec comput amazonaw com api v cluster yusaku request task task exit code stdout statu queu stderr host name ip ec intern id cluster name yusaku attempt cnt request id command start role mysql server start time stage id,1,0,0,0,0,0,
1917,Ambari Core-Site.xml Missing Property for LZO (enabled) - io.compression.codecs, ambari core site xml miss properti lzo enabl io compress codec,,,1,0,0,0,0,0,
1919,JobTracker History Server failed to come up on 1.3.0 stack and the request for service stall is stalled, job tracker histori server fail come stack request servic stall stall,Attempted to install a cluster with 1.3.0 stack.Service install was all green  but JobTracker History Server failed to come up.The request for the service start is stalled  with no tasks in QUEUED or IN_PROGRESS state  but with some tasks in PENDING state., attempt instal cluster stack servic instal green job tracker histori server fail come the request servic start stall task queu in progress state task pend state,0,0,0,0,0,0,
1923,Allow for users to customize Nagios user accounts, allow user custom nagio user account,Allow for customers to install the users used for Nagios., allow custom instal user use nagio,1,0,0,0,0,0,
1924,Allow for users to customize Ganglia gmetad + gmond user accounts, allow user custom ganglia gmetad gmond user account,Allow customization of ganglia gmetad and gmond users.For reference: looks like this is available in gsInstaller so the pattern exists to follow for Ambari impl. Defaults to nobody/nobody, allow custom ganglia gmetad gmond user for refer look like avail gs instal pattern exist follow ambari impl default nobodi nobodi,1,0,0,0,0,0,
1933,Test failure : testCascadeDeleteStages, test failur test cascad delet stage,mvn clean install produces the following failure ...testCascadeDeleteStages(org.apache.ambari.server.actionmanager.TestActionManager):Exception [EclipseLink-4002] (Eclipse Persistence Services -2.4.0.v20120608-r11652):org.eclipse.persistence.exceptions.DatabaseException(..), mvn clean instal produc follow failur test cascad delet stage org apach ambari server actionmanag test action manag except eclips link eclips persist servic v r org eclips persist except databas except,0,0,0,0,0,0,
1934,Security vulnerability with Ganglia and Nagios, secur vulner ganglia nagio,Ganglia Issue : Unspecified vulnerability in Ganglia Web before 3.5.1 allows remote attackers to execute arbitrary PHP code via unknown attack vectors. http://ganglia.info/?p=549 Ganglia Web 3.5.1 Release ??Security Advisory There is a security issue in Ganglia Web going back to at least 3.1.7 which can lead to arbitrary script being executed with web user privileges possibly leading to a machine compromise. Issue has been fixed in the latest version of Ganglia Web which can be downloaded from https://sourceforge.net/projects/ganglia/files/ganglia-web/3.5.1/ Solution: Need to get upgraded rpms with the Ganglia Web version 3.5.7 which has the fix for this vulnerability.Nagios: Multiple stack-based buffer overflows in the get_history function in history.cgi in Nagios Core before 3.4.4  and Icinga 1.6.x before 1.6.2  1.7.x before 1.7.4  and 1.8.x before 1.8.4  might allow remote attackers to execute arbitrary code via a long (1) host_name variable (host parameter) or (2) svc_description variable. http://www.nagios.org/projects/nagioscore/history/core-3x http://lists.grok.org.uk/pipermail/full-disclosure/2012-December/089125.html Vulnerable software and versions - nagios:nagios:3.4.3 and previous versions, ganglia issu unspecifi vulner ganglia web allow remot attack execut arbitrari php code via unknown attack vector http ganglia info p ganglia web releas secur advisori there secur issu ganglia web go back least lead arbitrari script execut web user privileg possibl lead machin compromis issu fix latest version ganglia web download http sourceforg net project ganglia file ganglia web solut need get upgrad rpm ganglia web version fix vulner nagio multipl stack base buffer overflow get histori function histori cgi nagio core icinga x x x might allow remot attack execut arbitrari code via long host name variabl host paramet svc descript variabl http www nagio org project nagioscor histori core x http list grok org uk pipermail full disclosur decemb html vulner softwar version nagio nagio previou version,1,0,0,1,0,0,
1944,All Service Smoke tests fail when run with service start, all servic smoke test fail run servic start,has_key(): expects the first argument to be a hash  got '' which is of type String at /var/lib/ambari-agent/puppet/modules/hdp/manifests/init.pp:38 on node ip-10-38-25-227.ec2.internalsite-pp#12.04.2013 03:16:38import '/var/lib/ambari-agent/puppet/modules/hdp/manifests/*.pp'import '/var/lib/ambari-agent/puppet/modules/hdp-hadoop/manifests/*.pp'import '/var/lib/ambari-agent/puppet/modules/hdp-hbase/manifests/*.pp'import '/var/lib/ambari-agent/puppet/modules/hdp-zookeeper/manifests/*.pp'import '/var/lib/ambari-agent/puppet/modules/hdp-oozie/manifests/*.pp'import '/var/lib/ambari-agent/puppet/modules/hdp-pig/manifests/*.pp'import '/var/lib/ambari-agent/puppet/modules/hdp-sqoop/manifests/*.pp'import '/var/lib/ambari-agent/puppet/modules/hdp-templeton/manifests/*.pp'import '/var/lib/ambari-agent/puppet/modules/hdp-hive/manifests/*.pp'import '/var/lib/ambari-agent/puppet/modules/hdp-hcat/manifests/*.pp'import '/var/lib/ambari-agent/puppet/modules/hdp-mysql/manifests/*.pp'import '/var/lib/ambari-agent/puppet/modules/hdp-monitor-webserver/manifests/*.pp'import '/var/lib/ambari-agent/puppet/modules/hdp-repos/manifests/*.pp'$ambari_db_rca_password= ['mapred']$nagios_server_host= ['ip-10-38-25-227.ec2.internal']$ambari_db_rca_url= ['jdbc:postgresql://ip-10-38-25-227.ec2.internal/ambarirca']$webhcat_server_host= ['ip-10-38-25-227.ec2.internal']$hbase_rs_hosts= ['ip-10-38-25-227.ec2.internal']$slave_hosts= ['ip-10-38-25-227.ec2.internal']$namenode_host= ['ip-10-38-25-227.ec2.internal']$ganglia_server_host= ['ip-10-38-25-227.ec2.internal']$hbase_master_hosts= ['ip-10-38-25-227.ec2.internal']$hive_mysql_host= ['ip-10-38-25-227.ec2.internal']$oozie_server= ['ip-10-38-25-227.ec2.internal']$ambari_db_rca_driver= ['org.postgresql.Driver']$zookeeper_hosts= ['ip-10-38-25-227.ec2.internal']$jtnode_host= ['ip-10-38-25-227.ec2.internal']$ambari_db_rca_username= ['mapred']$hive_server_host= ['ip-10-38-25-227.ec2.internal']node /default/ { stage{1 :} -&gt; stage{2 :}class {'hdp': stage =&gt; 1}class {'hdp-zookeeper::quorum::service_check': stage =&gt; 2}}, key expect first argument hash got type string var lib ambari agent puppet modul hdp manifest init pp node ip ec internalsit pp import var lib ambari agent puppet modul hdp manifest pp import var lib ambari agent puppet modul hdp hadoop manifest pp import var lib ambari agent puppet modul hdp hbase manifest pp import var lib ambari agent puppet modul hdp zookeep manifest pp import var lib ambari agent puppet modul hdp oozi manifest pp import var lib ambari agent puppet modul hdp pig manifest pp import var lib ambari agent puppet modul hdp sqoop manifest pp import var lib ambari agent puppet modul hdp templeton manifest pp import var lib ambari agent puppet modul hdp hive manifest pp import var lib ambari agent puppet modul hdp hcat manifest pp import var lib ambari agent puppet modul hdp mysql manifest pp import var lib ambari agent puppet modul hdp monitor webserv manifest pp import var lib ambari agent puppet modul hdp repo manifest pp ambari db rca password mapr nagio server host ip ec intern ambari db rca url jdbc postgresql ip ec intern ambarirca webhcat server host ip ec intern hbase rs host ip ec intern slave host ip ec intern namenod host ip ec intern ganglia server host ip ec intern hbase master host ip ec intern hive mysql host ip ec intern oozi server ip ec intern ambari db rca driver org postgresql driver zookeep host ip ec intern jtnode host ip ec intern ambari db rca usernam mapr hive server host ip ec intern node default stage gt stage class hdp stage gt class hdp zookeep quorum servic check stage gt,0,0,0,0,0,0,
1947,Oozie Smoke test fails with errors on the start services/install page., oozi smoke test fail error start servic instal page,Ozzie smoke tests fail, ozzi smoke test fail,0,0,0,0,0,0,
1948,System logs are not present on tasktracker, system log present tasktrack,Run a mapreduce job.Find the attempt id and look for system logs on tasktracker.http://ec2-54-224-138-78.compute-1.amazonaws.com:50060/tasklog?attemptid=attempt_201304121816_0003_m_000000_0Actual result:The syslogs are not present here.Only stdout and stderr logs are present., run mapreduc job find attempt id look system log tasktrack http ec comput amazonaw com tasklog attemptid attempt actual result the syslog present onli stdout stderr log present,1,0,0,0,0,0,
1952,hadoop dependency version for ambari-log4j is hardcoded  making it regular expression based to pick latest from the repository., hadoop depend version ambari log j hardcod make regular express base pick latest repositori,Ambari-log4j has hardcoded hadoop-core and hadoop-tools dependency. Make it version as regular expression to pick from the range from 1.0 &lt;= x &lt; 2.0.Also  updating the repository url., ambari log j hardcod hadoop core hadoop tool depend make version regular express pick rang lt x lt also updat repositori url,1,0,0,0,0,0,
1956,Wrong install status shown in Add Service Wizard, wrong instal statu shown add servic wizard,Upon master component install failure  the host status becomes 'warning' instead of 'failed' for Add Service Wizard., upon master compon instal failur host statu becom warn instead fail add servic wizard,1,0,0,0,0,0,
1957,Hosts table: whether the alert filter is in effect or not is not clear, host tabl whether alert filter effect clear,Currently  when the red badge inside the Hosts tab is clicked  it shows hosts that have at least one alert and no other hosts are displayed.The fact that the alert filter is in effect is not clear to the user and causes confusion., current red badg insid host tab click show host least one alert host display the fact alert filter effect clear user caus confus,0,0,0,0,0,0,
1966,Client install tasks are shown twice in progress popup during start phase of install wizard (update API call to include params/reconfigure_client), client instal task shown twice progress popup start phase instal wizard updat api call includ param reconfigur client,,,1,0,0,0,0,0,
1978,Deploying HDP-1.3.0 results in several alerts - is it related to hard-coded port, deploy hdp result sever alert relat hard code port,TaskTracker  RegionServer  HBase master process down because check_tcp failure.Looks like the hadoop-services.cfg.erb has:check_command check_tcp!&lt;%=scope.function_hdp_template_var('jtnode_port')%&gt;!-w 1 -c 1and looks like the ports are not getting replaced and end up being empty., task tracker region server h base master process check tcp failur look like hadoop servic cfg erb check command check tcp lt scope function hdp templat var jtnode port gt w c look like port get replac end empti,0,0,0,0,0,0,
1980,When nagios is unavailable  return null instead of throwing an Exception, when nagio unavail return null instead throw except,NAGIOS_SERVER alerts are retrieved from the nagios server when requesting the host_component. There is a SystemException thrown in the case of an IOException  which propagates as a 500 error for the entire request.In this case  set the nagios_alerts element to null instead of the 500 error., nagio server alert retriev nagio server request host compon there system except thrown case io except propag error entir request in case set nagio alert element null instead error,0,0,0,0,0,0,
1988,Hostname pattern expression is broken, hostnam pattern express broken,dev[01-03].domain.com expanded to:dev1.domain.comdev2.domain.comdev3.domain.comShould be:dev01.domain.comdev02.domain.comdev03.domain.com, dev domain com expand dev domain comdev domain comdev domain com should dev domain comdev domain comdev domain com,0,0,0,0,0,0,
1997,Filtered hosts get out of sync with the filter selection, filter host get sync filter select,1. Browse to Hosts2. Click one of the filters3. Browse to Services and back to Hosts4. The filter links show All as the current filter but the filter didn't reset and still shows a sub-set of hosts, brows host click one filter brows servic back host the filter link show all current filter filter reset still show sub set host,0,0,0,0,0,0,
1998,Action buttons on host details page not formatted properly on Firefox, action button host detail page format properli firefox,,,0,0,0,0,0,0,
1999,Clicking on Cancel on the Service Config page should not reload the entire app, click cancel servic config page reload entir app,When Cancel is clicked on the Service Config page  simply reload the config (not the entire app)., when cancel click servic config page simpli reload config entir app,1,0,0,0,0,0,
2001,Filtering on Jobs table does not work under certain situations, filter job tabl work certain situat,,,1,0,0,0,0,0,
2003,Hosts tab: clicking on red badge should not toggle 'Alerts' filter, host tab click red badg toggl alert filter,Clicking on the red badge in the Hosts tab should not toggle the 'Alerts' filter on the Hosts page (clicking anywhere in Hosts tab should go to Hosts page with 'All' selected)., click red badg host tab toggl alert filter host page click anywher host tab go host page all select,1,0,0,0,0,0,
2008,Using mixed OS overwrites ambari.repo during install, use mix os overwrit ambari repo instal,Performed install on mixed OS environment with 8 hosts.Ambari Server = RHEL6Three Hosts = RHEL6Four Hosts = RHEL5Performed manual ambari-agent bootstrap of the Four RHEL5 hosts. I was able to successfully register all hosts. When install started  the four RHEL5 hosts failed on installing their first component. Looking at the servers  looks like the right HDP.repo and HDP-epel.repo files are put in place.But looks like the ambari.repo file had been overwritten at some point during the install process  and now is point to the RHEL6 repos  causing failures., perform instal mix os environ host ambari server rhel three host rhel four host rhel perform manual ambari agent bootstrap four rhel host i abl success regist host when instal start four rhel host fail instal first compon look server look like right hdp repo hdp epel repo file put place but look like ambari repo file overwritten point instal process point rhel repo caus failur,1,0,0,0,0,0,
2013,Cannot delete cluster with components in UNKNOWN state, cannot delet cluster compon unknown state,When components are marked in an UNKNOWN state  it is not possible to delete the cluster - this should be possible., when compon mark unknown state possibl delet cluster possibl,0,0,0,0,0,0,
2019,Cannot decommission data node (ensure recommission also works), cannot decommiss data node ensur recommiss also work,stderr: $configuration&#91;hdfs-site&#93; is not an hash or array when accessing it with dfs.hosts.exclude at /var/lib/ambari-agent/puppet/modules/hdp-hadoop/manifests/hdfs/decommission.pp:24 on node ip-10-82-213-66.ec2.internal stdout:None, stderr configur hdf site hash array access df host exclud var lib ambari agent puppet modul hdp hadoop manifest hdf decommiss pp node ip ec intern stdout none,0,0,0,0,1,0,
2024,Ambari Server becomes unresponsive after crashing on http reads on jersey., ambari server becom unrespons crash http read jersey,The api's are being handled by a queuedthreadpool. The queuedthread pool size is 25.Somehow the http connections are being torn down from the UI side but the server still is hanging onto that socket and reading (most likely UI will also need to close http connections if its not using them - which might be an issue as well but doesnt have to addressed as urgent). The server has a read timeout of 0 which means it will just hang on to that socket for read. This causes all the threads to block at one time or the other. Simple solution is add read timeouts to all the SelectChannelConnector and SslSelectChannelConnector we use.Exception trace: SEVERE: The exception contained within MappableContainerException could not be mapped to a response  re-throwing to the HTTP containerorg.eclipse.jetty.io.EofException: early EOF at org.eclipse.jetty.server.HttpInput.read(HttpInput.java:65) at org.codehaus.jackson.impl.ByteSourceBootstrapper.ensureLoaded(ByteSourceBootstrapper.java:507) at org.codehaus.jackson.impl.ByteSourceBootstrapper.detectEncoding(ByteSourceBootstrapper.java:129) at org.codehaus.jackson.impl.ByteSourceBootstrapper.constructParser(ByteSourceBootstrapper.java:224) at org.codehaus.jackson.JsonFactory._createJsonParser(JsonFactory.java:785) at org.codehaus.jackson.JsonFactory.createJsonParser(JsonFactory.java:561) at org.codehaus.jackson.jaxrs.JacksonJsonProvider.readFrom(JacksonJsonProvider.java:414) at com.sun.jersey.json.impl.provider.entity.JacksonProviderProxy.readFrom(JacksonProviderProxy.java:139) at com.sun.jersey.spi.container.ContainerRequest.getEntity(ContainerRequest.java:474)Notice the API's is being called all the time - meaning they probalby had a browser up and running for a long time.There might be a possibilility that the browser might have some issues after running for a long time. Something to keep in mind when this happens again. Easy way to check that is to call Ambari server API's and also bring up a new browser window (new instance) and try hitting the browser UI., the api handl queuedthreadpool the queuedthread pool size somehow http connect torn ui side server still hang onto socket read like ui also need close http connect use might issu well doesnt address urgent the server read timeout mean hang socket read thi caus thread block one time simpl solut add read timeout select channel connector ssl select channel connector use except trace sever the except contain within mappabl contain except could map respons throw http containerorg eclips jetti io eof except earli eof org eclips jetti server http input read http input java org codehau jackson impl byte sourc bootstrapp ensur load byte sourc bootstrapp java org codehau jackson impl byte sourc bootstrapp detect encod byte sourc bootstrapp java org codehau jackson impl byte sourc bootstrapp construct parser byte sourc bootstrapp java org codehau jackson json factori creat json parser json factori java org codehau jackson json factori creat json parser json factori java org codehau jackson jaxr jackson json provid read from jackson json provid java com sun jersey json impl provid entiti jackson provid proxi read from jackson provid proxi java com sun jersey spi contain contain request get entiti contain request java notic api call time mean probalbi browser run long time there might possibilil browser might issu run long time someth keep mind happen easi way check call ambari server api also bring new browser window new instanc tri hit browser ui,0,0,0,0,1,0,
2027,Add validation checks for Add Property on custom site configs, add valid check add properti custom site config,,,1,0,0,0,0,0,
2029,Error when loading /main/services directly, error load main servic directli,,,0,0,0,0,0,0,
2030,Make frontend changes to account for the host component status UNKNOWN, make frontend chang account host compon statu unknown,Service status: if any of the master components are in UNKNOWN state  show the unknown icon. The action buttons for the service are disabled. Host status: if the host status is UNKNOWN or the heartbeat has not been received in more than 180 seconds  show the unknown icon. Host component status: if the host component status is UNKNOWN  show the unknown icon. The action button for the host component is disabled., servic statu master compon unknown state show unknown icon the action button servic disabl host statu host statu unknown heartbeat receiv second show unknown icon host compon statu host compon statu unknown show unknown icon the action button host compon disabl,1,0,0,0,0,0,
2031,Add clover code coverage profile, add clover code coverag profil,mvn test -Pclover -Dclover.license=&lt;clover.coverage.license&gt; should run the unit tests and return html/xml code coverage reports, mvn test pclover dclover licens lt clover coverag licens gt run unit test return html xml code coverag report,0,0,0,0,0,0,
2034,Disable 'Add Component' button in the Host Details page if the host is in UNKNOWN state or !isHeartbeating, disabl add compon button host detail page host unknown state heartbeat,,,0,0,0,0,0,0,
2035,'Add local user' button is enabled but nothing happens upon clicking it under certain conditions, add local user button enabl noth happen upon click certain condit,Steps to reproduce1. Go to Admin tab2. Click on 'Add Local User' button3. Click on Admin tab again4. Clicking on 'Add Local User' button does nothing, step reproduc go admin tab click add local user button click admin tab click add local user button noth,0,0,0,0,0,0,
2038,Services links on Dashboard connected to incorrect pages, servic link dashboard connect incorrect page,Click on any of the service links shown on the Dashboard page.It transitions to the service page and the content displayed is correct for the service chosen  but the URL indicates that it is another service and the side-menu shows a different service highlighted., click servic link shown dashboard page it transit servic page content display correct servic chosen url indic anoth servic side menu show differ servic highlight,0,0,0,0,0,0,
2045,Add Unit test to verify  client re-install for install failed client, add unit test verifi client instal instal fail client,Add Unit test to verify  When INSTALL is schedules on client components it should also be scheduled on components that are in INSTALL_FAILED state, add unit test verifi when instal schedul client compon also schedul compon instal fail state,0,0,0,0,0,0,
2054,If 'Install from Local Repository' selected in install wizard  Add Host wizard not working, if instal local repositori select instal wizard add host wizard work,,,0,0,0,0,0,1,
2058,Host Detail page: if the host component is in INSTALL_FAILED state  we should let the user reinstall it, host detail page host compon instal fail state let user reinstal,,,0,0,0,0,0,0,
2061,HBase Heatmaps: clean up labels and units, h base heatmap clean label unit,,,0,0,0,0,0,0,
2065,Hadoop group customization does not take affect, hadoop group custom take affect,To customize the hadoop group  when it was changed from 'hadoop' to 'hadoopgroup'  it didn't look like it worked.root@ip-10-85-135-237 hdfsuser# id hdfsuid=495(hdfs) gid=494(hdfs) groups=494(hdfs) 495(hadoop)And looking at the /etc/group filepuppet:x:497:hadoopgroup:x:500:rrdcached:x:496:apache:x:48:hadoop:x:495:mapred hdfs, to custom hadoop group chang hadoop hadoopgroup look like work root ip hdfsuser id hdfsuid hdf gid hdf group hdf hadoop and look etc group filepuppet x hadoopgroup x rrdcach x apach x hadoop x mapr hdf,0,0,0,0,0,0,
2068,'Preparing to install ' message needs spacing, prepar instal messag need space,,,0,0,0,0,0,0,
2070,Changing service directories should popup a confirmation/warning dialog upon save, chang servic directori popup confirm warn dialog upon save,Post-install  if the user tries to reconfigure NN  SNN  MapReduce local/system directories  we should popup a confirmation/warning upon Save as this is potentially a dangerous operation., post instal user tri reconfigur nn snn map reduc local system directori popup confirm warn upon save potenti danger oper,0,0,0,0,0,0,
2075,Admin role can't be assigned to LDAP user, admin role assign ldap user,,,0,0,0,0,0,0,
2081,changeUid.sh failing during installation, chang uid sh fail instal,On SUSE  I received a puppet error on each agent that /tmp/changeUid.sh failed during installation. (Sorry I no longer have the error  I made puppet change and restarted to get by it). But in a nutshell  running the command manually gave:ip-10-82-233-26:/tmp # /tmp/changeUid.sh ambari-qa 1012 /tmp/ambari-qa /home/ambari-qa /var/spool/mail/ambari-qaChanging uid of ambari-qa from 1012 to 1012Changing directory permisions for /tmp/ambari-qa /home/ambari-qa /var/spool/mail/ambari-qausermod: UID 1012 is not unique.Note that the usermod is trying to change to an existing UID  so the command is failing everywhere, on suse i receiv puppet error agent tmp chang uid sh fail instal sorri i longer error i made puppet chang restart get but nutshel run command manual gave ip tmp tmp chang uid sh ambari qa tmp ambari qa home ambari qa var spool mail ambari qa chang uid ambari qa chang directori permis tmp ambari qa home ambari qa var spool mail ambari qausermod uid uniqu note usermod tri chang exist uid command fail everywher,0,0,0,0,0,0,
2087,Tasks are not filtered by parent request id, task filter parent request id,STEPS:1) Get tasks for first request  /api/v1/clusters/&lt;cluster&gt;/requests/&lt;firstRequest&gt;  i.e. task1 ... taskN12) Get tasks for second request by task from first requst  like /api/v1/clusters/&lt;cluster&gt;/requests/&lt;secondRequest&gt;/tasks/task1 (note task1 belongs to first request  not second)3) Notice that task from first request are present in second request., step get task first request api v cluster lt cluster gt request lt first request gt e task task n get task second request task first requst like api v cluster lt cluster gt request lt second request gt task task note task belong first request second notic task first request present second request,0,0,0,0,0,0,
2089,Post Ambari upgrade  Hive and Oozie fail to start after reconfigure, post ambari upgrad hive oozi fail start reconfigur,,,0,0,0,0,0,0,
2095,It's possible to get into a state where install retry is not possible if the agent stops heartbeating, it possibl get state instal retri possibl agent stop heartbeat,This affects both Install and Add Host Wizards.Steps to reproduce:While installing components  stop the agent on one of the hosts.Waiting for a while puts the components on the host into the UNKNOWN state.Click Retry from the UI. This causes a server-side error and the UI gets confused (the hosts are shown with 'Waiting' message and no 'Retry' button is available).The user is not able to get out of this state., thi affect instal add host wizard step reproduc while instal compon stop agent one host wait put compon host unknown state click retri ui thi caus server side error ui get confus host shown wait messag retri button avail the user abl get state,0,0,0,0,0,0,
2101,Hive service check (still) failing with file permissions, hive servic check still fail file permiss,Stack upgrade testing is still showing this to be an issue.warning: Unrecognised escape sequence '/;' in file /var/lib/ambari-agent/puppet/modules/hdp-hive/manifests/hive/service_check.pp at line 32warning: Dynamic lookup of $configuration is deprecated. Support will be removed in Puppet 2.8. Use a fully-qualified variable name (e.g.  $classname::variable) or parameterized classes.notice: /Stage[1]/Hdp::Snappy::Package/Hdp::Snappy::Package::Ln[32]/Hdp::Exec[hdp::snappy::package::ln 32]/Exec[hdp::snappy::package::ln 32]/returns: executed successfullynotice: /Stage[2]/Hdp-hcat::Hcat::Service_check/Exec[hcatSmoke.sh prepare]/returns: ls: cannot access /usr/share/java/*oracle*: No such file or directorynotice: /Stage[2]/Hdp-hcat::Hcat::Service_check/Exec[hcatSmoke.sh prepare]/returns: 13/05/09 15:01:52 WARN conf.HiveConf: DEPRECATED: Configuration property hive.metastore.local no longer has any effect. Make sure to provide a valid value for hive.metastore.uris if you are connecting to a remote metastore.notice: /Stage[2]/Hdp-hcat::Hcat::Service_check/Exec[hcatSmoke.sh prepare]/returns: log4j:ERROR setFile(null true) call failed.notice: /Stage[2]/Hdp-hcat::Hcat::Service_check/Exec[hcatSmoke.sh prepare]/returns: java.io.FileNotFoundException: /tmp/ambari_qa/hive.log (Permission denied)notice: /Stage[2]/Hdp-hcat::Hcat::Service_check/Exec[hcatSmoke.sh prepare]/returns: at java.io.FileOutputStream.openAppend(Native Method)notice: /Stage[2]/Hdp-hcat::Hcat::Service_check/Exec[hcatSmoke.sh prepare]/returns: at java.io.FileOutputStream.&lt;init&gt;(FileOutputStream.java:192)notice: /Stage[2]/Hdp-hcat::Hcat::Service_check/Exec[hcatSmoke.sh prepare]/returns: at java.io.FileOutputStream.&lt;init&gt;(FileOutputStream.java:116)notice: /Stage[2]/Hdp-hcat::Hcat::Service_check/Exec[hcatSmoke.sh prepare]/returns: at org.apache.log4j.FileAppender.setFile(FileAppender.java:290)notice: /Stage[2]/Hdp-hcat::Hcat::Service_check/Exec[hcatSmoke.sh prepare]/returns: at org.apache.log4j.FileAppender.activateOptions(FileAppender.java:164)notice: /Stage[2]/Hdp-hcat::Hcat::Service_check/Exec[hcatSmoke.sh prepare]/returns: at org.apache.log4j.DailyRollingFileAppender.activateOptions(DailyRollingFileAppender.java:216)notice: /Stage[2]/Hdp-hcat::Hcat::Service_check/Exec[hcatSmoke.sh prepare]/returns: at org.apache.log4j.config.PropertySetter.activate(PropertySetter.java:257)notice: /Stage[2]/Hdp-hcat::Hcat::Service_check/Exec[hcatSmoke.sh prepare]/returns: at org.apache.log4j.config.PropertySetter.setProperties(PropertySetter.java:133)notice: /Stage[2]/Hdp-hcat::Hcat::Service_check/Exec[hcatSmoke.sh prepare]/returns: at org.apache.log4j.config.PropertySetter.setProperties(PropertySetter.java:97)notice: /Stage[2]/Hdp-hcat::Hcat::Service_check/Exec[hcatSmoke.sh prepare]/returns: at org.apache.log4j.PropertyConfigurator.parseAppender(PropertyConfigurator.java:689)notice: /Stage[2]/Hdp-hcat::Hcat::Service_check/Exec[hcatSmoke.sh prepare]/returns: at org.apache.log4j.PropertyConfigurator.parseCategory(PropertyConfigurator.java:647)notice: /Stage[2]/Hdp-hcat::Hcat::Service_check/Exec[hcatSmoke.sh prepare]/returns: at org.apache.log4j.PropertyConfigurator.configureRootCategory(PropertyConfigurator.java:544)notice: /Stage[2]/Hdp-hcat::Hcat::Service_check/Exec[hcatSmoke.sh prepare]/returns: at org.apache.log4j.PropertyConfigurator.doConfigure(PropertyConfigurator.java:440)notice: /Stage[2]/Hdp-hcat::Hcat::Service_check/Exec[hcatSmoke.sh prepare]/returns: at org.apache.log4j.PropertyConfigurator.doConfigure(PropertyConfigurator.java:476)notice: /Stage[2]/Hdp-hcat::Hcat::Service_check/Exec[hcatSmoke.sh prepare]/returns: at org.apache.log4j.PropertyConfigurator.configure(PropertyConfigurator.java:354)notice: /Stage[2]/Hdp-hcat::Hcat::Service_check/Exec[hcatSmoke.sh prepare]/returns: at org.apache.hadoop.hive.common.LogUtils.initHiveLog4jDefault(LogUtils.java:124)notice: /Stage[2]/Hdp-hcat::Hcat::Service_check/Exec[hcatSmoke.sh prepare]/returns: at org.apache.hadoop.hive.common.LogUtils.initHiveLog4jCommon(LogUtils.java:77)notice: /Stage[2]/Hdp-hcat::Hcat::Service_check/Exec[hcatSmoke.sh prepare]/returns: at org.apache.hadoop.hive.common.LogUtils.initHiveLog4j(LogUtils.java:58)notice: /Stage[2]/Hdp-hcat::Hcat::Service_check/Exec[hcatSmoke.sh prepare]/returns: at org.apache.hcatalog.cli.HCatCli.main(HCatCli.java:61)notice: /Stage[2]/Hdp-hcat::Hcat::Service_check/Exec[hcatSmoke.sh prepare]/returns: at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)notice: /Stage[2]/Hdp-hcat::Hcat::Service_check/Exec[hcatSmoke.sh prepare]/returns: at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)notice: /Stage[2]/Hdp-hcat::Hcat::Service_check/Exec[hcatSmoke.sh prepare]/returns: at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)notice: /Stage[2]/Hdp-hcat::Hcat::Service_check/Exec[hcatSmoke.sh prepare]/returns: at java.lang.reflect.Method.invoke(Method.java:597)notice: /Stage[2]/Hdp-hcat::Hcat::Service_check/Exec[hcatSmoke.sh prepare]/returns: at org.apache.hadoop.util.RunJar.main(RunJar.java:160)notice: /Stage[2]/Hdp-hcat::Hcat::Service_check/Exec[hcatSmoke.sh prepare]/returns: log4j:ERROR Either File or DatePattern options are not set for appender [DRFA].notice: /Stage[2]/Hdp-hcat::Hcat::Service_check/Exec[hcatSmoke.sh prepare]/returns: Exception in thread 'main' java.lang.RuntimeException: java.io.IOException: Permission deniednotice: /Stage[2]/Hdp-hcat::Hcat::Service_check/Exec[hcatSmoke.sh prepare]/returns: at org.apache.hadoop.hive.ql.session.SessionState.start(SessionState.java:272)notice: /Stage[2]/Hdp-hcat::Hcat::Service_check/Exec[hcatSmoke.sh prepare]/returns: at org.apache.hcatalog.cli.HCatCli.main(HCatCli.java:79)notice: /Stage[2]/Hdp-hcat::Hcat::Service_check/Exec[hcatSmoke.sh prepare]/returns: at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method), stack upgrad test still show issu warn unrecognis escap sequenc file var lib ambari agent puppet modul hdp hive manifest hive servic check pp line warn dynam lookup configur deprec support remov puppet use fulli qualifi variabl name e g classnam variabl parameter class notic stage hdp snappi packag hdp snappi packag ln hdp exec hdp snappi packag ln exec hdp snappi packag ln return execut successfullynotic stage hdp hcat hcat servic check exec hcat smoke sh prepar return ls cannot access usr share java oracl no file directorynotic stage hdp hcat hcat servic check exec hcat smoke sh prepar return warn conf hive conf deprec configur properti hive metastor local longer effect make sure provid valid valu hive metastor uri connect remot metastor notic stage hdp hcat hcat servic check exec hcat smoke sh prepar return log j error set file null true call fail notic stage hdp hcat hcat servic check exec hcat smoke sh prepar return java io file not found except tmp ambari qa hive log permiss deni notic stage hdp hcat hcat servic check exec hcat smoke sh prepar return java io file output stream open append nativ method notic stage hdp hcat hcat servic check exec hcat smoke sh prepar return java io file output stream lt init gt file output stream java notic stage hdp hcat hcat servic check exec hcat smoke sh prepar return java io file output stream lt init gt file output stream java notic stage hdp hcat hcat servic check exec hcat smoke sh prepar return org apach log j file append set file file append java notic stage hdp hcat hcat servic check exec hcat smoke sh prepar return org apach log j file append activ option file append java notic stage hdp hcat hcat servic check exec hcat smoke sh prepar return org apach log j daili roll file append activ option daili roll file append java notic stage hdp hcat hcat servic check exec hcat smoke sh prepar return org apach log j config properti setter activ properti setter java notic stage hdp hcat hcat servic check exec hcat smoke sh prepar return org apach log j config properti setter set properti properti setter java notic stage hdp hcat hcat servic check exec hcat smoke sh prepar return org apach log j config properti setter set properti properti setter java notic stage hdp hcat hcat servic check exec hcat smoke sh prepar return org apach log j properti configur pars append properti configur java notic stage hdp hcat hcat servic check exec hcat smoke sh prepar return org apach log j properti configur pars categori properti configur java notic stage hdp hcat hcat servic check exec hcat smoke sh prepar return org apach log j properti configur configur root categori properti configur java notic stage hdp hcat hcat servic check exec hcat smoke sh prepar return org apach log j properti configur configur properti configur java notic stage hdp hcat hcat servic check exec hcat smoke sh prepar return org apach log j properti configur configur properti configur java notic stage hdp hcat hcat servic check exec hcat smoke sh prepar return org apach log j properti configur configur properti configur java notic stage hdp hcat hcat servic check exec hcat smoke sh prepar return org apach hadoop hive common log util init hive log j default log util java notic stage hdp hcat hcat servic check exec hcat smoke sh prepar return org apach hadoop hive common log util init hive log j common log util java notic stage hdp hcat hcat servic check exec hcat smoke sh prepar return org apach hadoop hive common log util init hive log j log util java notic stage hdp hcat hcat servic check exec hcat smoke sh prepar return org apach hcatalog cli h cat cli main h cat cli java notic stage hdp hcat hcat servic check exec hcat smoke sh prepar return sun reflect nativ method accessor impl invok nativ method notic stage hdp hcat hcat servic check exec hcat smoke sh prepar return sun reflect nativ method accessor impl invok nativ method accessor impl java notic stage hdp hcat hcat servic check exec hcat smoke sh prepar return sun reflect deleg method accessor impl invok deleg method accessor impl java notic stage hdp hcat hcat servic check exec hcat smoke sh prepar return java lang reflect method invok method java notic stage hdp hcat hcat servic check exec hcat smoke sh prepar return org apach hadoop util run jar main run jar java notic stage hdp hcat hcat servic check exec hcat smoke sh prepar return log j error either file date pattern option set append drfa notic stage hdp hcat hcat servic check exec hcat smoke sh prepar return except thread main java lang runtim except java io io except permiss deniednotic stage hdp hcat hcat servic check exec hcat smoke sh prepar return org apach hadoop hive ql session session state start session state java notic stage hdp hcat hcat servic check exec hcat smoke sh prepar return org apach hcatalog cli h cat cli main h cat cli java notic stage hdp hcat hcat servic check exec hcat smoke sh prepar return sun reflect nativ method accessor impl invok nativ method,0,0,0,1,0,0,
2103,Support for configuring and running Ambari Web Server https, support configur run ambari web server http,Need to be able to run Ambari Web (and access Ambari REST APIs) over HTTPS. Should document assuming the user comes with their own certificate. User should also be able to configure which port to expose HTTPS., need abl run ambari web access ambari rest ap is http should document assum user come certif user also abl configur port expos http,0,1,0,0,0,0,
2111,Enable customization of smoke test user, enabl custom smoke test user,,,0,0,0,0,0,0,
2118,ambari-web modifications to allow for Hadoop Compatible Filesystems (HCFS), ambari web modif allow hadoop compat filesystem hcf,Make modifications to ambari-web that will allow for the selection of a Hadoop Compatible Filesystem. These changes include allowing HDFS to be unselected (either HDFS or HCFS must be selected). If HCFS is chosen  push appropriate configuration (site-conf.xml) files during the install so that systems will work with HCFS as the underlying filesystem rather than HDFS., make modif ambari web allow select hadoop compat filesystem these chang includ allow hdf unselect either hdf hcf must select if hcf chosen push appropri configur site conf xml file instal system work hcf underli filesystem rather hdf,0,0,1,0,0,0,
2130,Use modified dependencies if a stack contains an HCFS service (Hadoop Compatible File System), use modifi depend stack contain hcf servic hadoop compat file system,Use stack metadata to determine if a stack contains an HCFS service and generate modified RoleCommandOrder dependencies if it does., use stack metadata determin stack contain hcf servic gener modifi role command order depend,0,0,1,0,0,0,
2134,Set default value of oozie property 'oozie.service.AuthorizationService.authorization.enabled' to true., set default valu oozi properti oozi servic author servic author enabl true,,,0,0,0,0,0,0,
2136,Home paths are not set correctly in /etc/sqoop/conf/sqoop-env.sh, home path set correctli etc sqoop conf sqoop env sh,Ambari sets the followings:#Set path to where bin/hadoop is availableexport HADOOP_HOME=${HADOOP_HOME:-/usr}#set the path to where bin/hbase is availableexport HBASE_HOME=${HBASE_HOME:-/usr}#Set the path to where bin/hive is availableexport HIVE_HOME=${HIVE_HOME:-/usr}# add libthrift in hive to sqoop class path first so hive imports workexport SQOOP_USER_CLASSPATH=''ls ${HIVE_HOME}/lib/libthrift-*.jar 2&gt; /dev/null':${SQOOP_USER_CLASSPATH}'#Set the path for where zookeper config dir isexport ZOOCFGDIR=${ZOOCFGDIR:-/etc/zookeeper/conf}It should be the followings (also screenshot is available):#Set path to where bin/hadoop is availableexport HADOOP_HOME=${HADOOP_HOME:-/usr/lib/hadoop}#set the path to where bin/hbase is availableexport HBASE_HOME=${HBASE_HOME:-/usr/lib/hbase}#Set the path to where bin/hive is availableexport HIVE_HOME=${HIVE_HOME:-/usr/lib/hive}#Set the path for where zookeper config dir isexport ZOOCFGDIR=${ZOOCFGDIR:-/etc/zookeeper/conf}# add libthrift in hive to sqoop class path first so hive imports workexport SQOOP_USER_CLASSPATH=''ls ${HIVE_HOME}/lib/libthrift-*.jar 2&gt; /dev/null':${SQOOP_USER_CLASSPATH}, ambari set follow set path bin hadoop availableexport hadoop home hadoop home usr set path bin hbase availableexport hbase home hbase home usr set path bin hive availableexport hive home hive home usr add libthrift hive sqoop class path first hive import workexport sqoop user classpath ls hive home lib libthrift jar gt dev null sqoop user classpath set path zookep config dir isexport zoocfgdir zoocfgdir etc zookeep conf it follow also screenshot avail set path bin hadoop availableexport hadoop home hadoop home usr lib hadoop set path bin hbase availableexport hbase home hbase home usr lib hbase set path bin hive availableexport hive home hive home usr lib hive set path zookep config dir isexport zoocfgdir zoocfgdir etc zookeep conf add libthrift hive sqoop class path first hive import workexport sqoop user classpath ls hive home lib libthrift jar gt dev null sqoop user classpath,0,0,0,0,0,0,
2143,HBASE fails to start on master, hbase fail start master,HBASE master fails to start on master. From log:2013-05-14 21:06:43 487 FATAL org.apache.hadoop.hbase.master.HMaster: Unhandled exception. Starting shutdown.org.apache.hadoop.security.AccessControlException: org.apache.hadoop.security.AccessControlException: Permission denied: user=hbase  access=EXECUTE  inode='/apps/hbase/data':hdfs:hdfs:drwx------ at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method) at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:39) at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:27) at java.lang.reflect.Constructor.newInstance(Constructor.java:513) at org.apache.hadoop.ipc.RemoteException.instantiateException(RemoteException.java:95) at org.apache.hadoop.ipc.RemoteException.unwrapRemoteException(RemoteException.java:57) at org.apache.hadoop.hdfs.DFSClient.getFileInfo(DFSClient.java:1134) at org.apache.hadoop.hdfs.DistributedFileSystem.getFileStatus(DistributedFileSystem.java:556) at org.apache.hadoop.fs.FileSystem.exists(FileSystem.java:779) at org.apache.hadoop.hbase.util.FSUtils.getVersion(FSUtils.java:287) at org.apache.hadoop.hbase.util.FSUtils.checkVersion(FSUtils.java:329) at org.apache.hadoop.hbase.master.MasterFileSystem.checkRootDir(MasterFileSystem.java:434) at org.apache.hadoop.hbase.master.MasterFileSystem.createInitialFileSystemLayout(MasterFileSystem.java:146) at org.apache.hadoop.hbase.master.MasterFileSystem.&lt;init&gt;(MasterFileSystem.java:131) at org.apache.hadoop.hbase.master.HMaster.finishInitialization(HMaster.java:532) at org.apache.hadoop.hbase.master.HMaster.run(HMaster.java:391) at java.lang.Thread.run(Thread.java:662), hbase master fail start master from log fatal org apach hadoop hbase master h master unhandl except start shutdown org apach hadoop secur access control except org apach hadoop secur access control except permiss deni user hbase access execut inod app hbase data hdf hdf drwx sun reflect nativ constructor accessor impl new instanc nativ method sun reflect nativ constructor accessor impl new instanc nativ constructor accessor impl java sun reflect deleg constructor accessor impl new instanc deleg constructor accessor impl java java lang reflect constructor new instanc constructor java org apach hadoop ipc remot except instanti except remot except java org apach hadoop ipc remot except unwrap remot except remot except java org apach hadoop hdf df client get file info df client java org apach hadoop hdf distribut file system get file statu distribut file system java org apach hadoop fs file system exist file system java org apach hadoop hbase util fs util get version fs util java org apach hadoop hbase util fs util check version fs util java org apach hadoop hbase master master file system check root dir master file system java org apach hadoop hbase master master file system creat initi file system layout master file system java org apach hadoop hbase master master file system lt init gt master file system java org apach hadoop hbase master h master finish initi h master java org apach hadoop hbase master h master run h master java java lang thread run thread java,0,0,0,0,0,0,
2144,Installation with existing Oracle DB fails, instal exist oracl db fail,On Step8 'Customize Services' for HIVE and OOZIE we can use option 'Existing Oracle DB'.But after Cluster install HIVE and OOZIE didn't start (with option 'Existing Oracle DB').In the logs I found such:/var/log/hive/hive.logCaused by: org.datanucleus.exceptions.NucleusException: Attempt to invoke the 'DBCP' plugin to create a ConnectionPool gave an error : The specified datastore driver ('oracle.jdbc.driver.OracleDriver') was not found in the CLASSPATH. Please check your CLASSPATH specification  and the name of the driver./var/log/oozie/oozie.log013-05-14 11:25:59 618 FATAL Services:533 - USER[-] GROUP[-] TOKEN[-] APP[-] JOB[-] ACTION[-] E0103: Could not load service classes  Cannot load JDBC driver class 'oracle.jdbc.driver.OracleDriver'org.apache.oozie.service.ServiceException: E0103: Could not load service classes  Cannot load JDBC driver class 'oracle.jdbc.driver.OracleDriver', on step custom servic hive oozi use option exist oracl db but cluster instal hive oozi start option exist oracl db in log i found var log hive hive log caus org datanucleu except nucleu except attempt invok dbcp plugin creat connect pool gave error the specifi datastor driver oracl jdbc driver oracl driver found classpath pleas check classpath specif name driver var log oozi oozi log fatal servic user group token app job action e could load servic class cannot load jdbc driver class oracl jdbc driver oracl driver org apach oozi servic servic except e could load servic class cannot load jdbc driver class oracl jdbc driver oracl driver,0,0,0,0,0,0,
2146,When hive and oozie users have been changed after upgrade hive metastore and oozie cannot start properly, when hive oozi user chang upgrad hive metastor oozi cannot start properli,Oozie start failure:^[[0;36mnotice: /Stage[2]/Hdp-oozie::Service/Hdp::Exec[exec su - ooziexx -c 'cd /var/tmp/oozie &amp;&amp; /usr/lib/oozie/bin/oozie-start.sh']/Exec[exec su - ooziexx -c 'cd /var/tmp/oozie &amp;&amp; /usr/lib/oozie/bin/oozie-start.sh']/returns: Setting OOZIE_BASE_URL: http://ip-10-212-166-111.ec2.internal:11000/oozie^[[0m^[[0;36mnotice: /Stage[2]/Hdp-oozie::Service/Hdp::Exec[exec su - ooziexx -c 'cd /var/tmp/oozie &amp;&amp; /usr/lib/oozie/bin/oozie-start.sh']/Exec[exec su - ooziexx -c 'cd /var/tmp/oozie &amp;&amp; /usr/lib/oozie/bin/oozie-start.sh']/returns: Using CATALINA_BASE: /var/lib/oozie/oozie-server^[[0m^[[0;36mnotice: /Stage[2]/Hdp-oozie::Service/Hdp::Exec[exec su - ooziexx -c 'cd /var/tmp/oozie &amp;&amp; /usr/lib/oozie/bin/oozie-start.sh']/Exec[exec su - ooziexx -c 'cd /var/tmp/oozie &amp;&amp; /usr/lib/oozie/bin/oozie-start.sh']/returns: Setting OOZIE_HTTPS_KEYSTORE_FILE: /home/ooziexx/.keystore^[[0m^[[0;36mnotice: /Stage[2]/Hdp-oozie::Service/Hdp::Exec[exec su - ooziexx -c 'cd /var/tmp/oozie &amp;&amp; /usr/lib/oozie/bin/oozie-start.sh']/Exec[exec su - ooziexx -c 'cd /var/tmp/oozie &amp;&amp; /usr/lib/oozie/bin/oozie-start.sh']/returns: Setting OOZIE_HTTPS_KEYSTORE_PASS: password^[[0m^[[0;36mnotice: /Stage[2]/Hdp-oozie::Service/Hdp::Exec[exec su - ooziexx -c 'cd /var/tmp/oozie &amp;&amp; /usr/lib/oozie/bin/oozie-start.sh']/Exec[exec su - ooziexx -c 'cd /var/tmp/oozie &amp;&amp; /usr/lib/oozie/bin/oozie-start.sh']/returns: Setting CATALINA_OUT: /var/log/oozie//catalina.out^[[0m^[[0;36mnotice: /Stage[2]/Hdp-oozie::Service/Hdp::Exec[exec su - ooziexx -c 'cd /var/tmp/oozie &amp;&amp; /usr/lib/oozie/bin/oozie-start.sh']/Exec[exec su - ooziexx -c 'cd /var/tmp/oozie &amp;&amp; /usr/lib/oozie/bin/oozie-start.sh']/returns: Using CATALINA_PID: /var/run/oozie/oozie.pid^[[0m^[[0;36mnotice: /Stage[2]/Hdp-oozie::Service/Hdp::Exec[exec su - ooziexx -c 'cd /var/tmp/oozie &amp;&amp; /usr/lib/oozie/bin/oozie-start.sh']/Exec[exec su - ooziexx -c 'cd /var/tmp/oozie &amp;&amp; /usr/lib/oozie/bin/oozie-start.sh']/returns: ^[[0m^[[0;36mnotice: /Stage[2]/Hdp-oozie::Service/Hdp::Exec[exec su - ooziexx -c 'cd /var/tmp/oozie &amp;&amp; /usr/lib/oozie/bin/oozie-start.sh']/Exec[exec su - ooziexx -c 'cd /var/tmp/oozie &amp;&amp; /usr/lib/oozie/bin/oozie-start.sh']/returns: Using CATALINA_OPTS: -Dderby.stream.error.file=/var/log/oozie//derby.log^[[0m^[[0;36mnotice: /Stage[2]/Hdp-oozie::Service/Hdp::Exec[exec su - ooziexx -c 'cd /var/tmp/oozie &amp;&amp; /usr/lib/oozie/bin/oozie-start.sh']/Exec[exec su - ooziexx -c 'cd /var/tmp/oozie &amp;&amp; /usr/lib/oozie/bin/oozie-start.sh']/returns: Adding to CATALINA_OPTS: -Doozie.home.dir=/usr/lib/oozie -Doozie.config.dir=/etc/oozie/conf -Doozie.log.dir=/var/log/oozie/ -Doozie.data.dir=/grid/0/hadoop/oozie/data/ -Doozie.config.file=oozie-site.xml -Doozie.log4j.file=oozie-log4j.properties -Doozie.log4j.reload=10 -Doozie.http.hostname=ip-10-212-166-111.ec2.internal -Doozie.admin.port=11001 -Doozie.http.port=11000 -Doozie.https.port=11443 -Doozie.base.url=http://ip-10-212-166-111.ec2.internal:11000/oozie -Doozie.https.keystore.file=/home/ooziexx/.keystore -Doozie.https.keystore.pass=password -Djava.library.path=/usr/lib/hadoop/lib/native/Linux-amd64-64^[[0m^[[0;36mnotice: /Stage[2]/Hdp-oozie::Service/Hdp::Exec[exec su - ooziexx -c 'cd /var/tmp/oozie &amp;&amp; /usr/lib/oozie/bin/oozie-start.sh']/Exec[exec su - ooziexx -c 'cd /var/tmp/oozie &amp;&amp; /usr/lib/oozie/bin/oozie-start.sh']/returns: ^[[0m^[[0;36mnotice: /Stage[2]/Hdp-oozie::Service/Hdp::Exec[exec su - ooziexx -c 'cd /var/tmp/oozie &amp;&amp; /usr/lib/oozie/bin/oozie-start.sh']/Exec[exec su - ooziexx -c 'cd /var/tmp/oozie &amp;&amp; /usr/lib/oozie/bin/oozie-start.sh']/returns: /usr/lib/oozie/oozie-server/bin/catalina.sh: line 386: /var/run/oozie/oozie.pid: Permission denied^[[0m^[[1;35merr: /Stage[2]/Hdp-oozie::Service/Hdp::Exec[exec su - ooziexx -c 'cd /var/tmp/oozie &amp;&amp; /usr/lib/oozie/bin/oozie-start.sh']/Exec[exec su - ooziexx -c 'cd /var/tmp/oozie &amp;&amp; /usr/lib/oozie/bin/oozie-start.sh']/returns: change from notrun to 0 failed: su - ooziexx -c 'cd /var/tmp/oozie &amp;&amp; /usr/lib/oozie/bin/oozie-start.sh' returned 1 instead of one of [0] at /var/lib/ambari-agent/puppet/modules/hdp/manifests/init.pp:340^[[0m^[[0;36mnotice: /Stage[2]/Hdp-oozie::Service/Hdp::Exec[exec su - ooziexx -c 'cd /var/tmp/oozie &amp;&amp; /usr/lib/oozie/bin/oozie-start.sh']/Anchor[hdp::exec::exec su - ooziexx -c 'cd /var/tmp/oozie &amp;&amp; /usr/lib/oozie/bin/oozie-start.sh'::end]: Dependency Exec[exec su - ooziexx -c 'cd /var/tmp/oozie &amp;&amp; /usr/lib/oozie/bin/oozie-start.sh'] has failures: true^[[0m^[[0;33mwarning: /Stage[2]/Hdp-oozie::Service/Hdp::Exec[exec su - ooziexx -c 'cd /var/tmp/oozie &amp;&amp; /usr/lib/oozie/bin/oozie-start.sh']/Anchor[hdp::exec::exec su - ooziexx -c 'cd /var/tmp/oozie &amp;&amp; /usr/lib/oozie/bin/oozie-start.sh'::end]: Skipping because of failed dependencies^[[0m^[[0;36mnotice: /Stage[2]/Hdp-oozie::Service/Hdp-oozie::Service::Directory[/var/log/oozie]/Hdp::Directory_recursive_create[/var/log/oozie]/Hdp::Directory[/var/log/oozie]/File[/var/log/oozie]/owner: owner changed 'oozie' to 'ooziexx'^[[0m^[[0;36mnotice: /Stage[2]/Hdp-oozie::Service/Hdp-oozie::Service::Directory[/var/log/oozie]/Hdp::Directory_recursive_create[/var/log/oozie]/Hdp::Directory[/var/log/oozie]/File[/var/log/oozie]/group: group changed 'oozie' to 'hadoopxx'^[[0m^[[0;36mnotice: /Stage[2]/Hdp-oozie::Service/Hdp-oozie::Service::Directory[/var/run/oozie]/Hdp::Directory_recursive_create[/var/run/oozie]/Hdp::Directory[/var/run/oozie]/File[/var/run/oozie]/owner: owner changed 'oozie' to 'ooziexx'^[[0m^[[0;36mnotice: /Stage[2]/Hdp-oozie::Service/Hdp-oozie::Service::Directory[/var/run/oozie]/Hdp::Directory_recursive_create[/var/run/oozie]/Hdp::Directory[/var/run/oozie]/File[/var/run/oozie]/group: group changed 'oozie' to 'hadoopxx'^[[0m^[[0;36mnotice: Finished catalog run in 9.36 seconds^[[0m, oozi start failur mnotic stage hdp oozi servic hdp exec exec su ooziexx c cd var tmp oozi amp amp usr lib oozi bin oozi start sh exec exec su ooziexx c cd var tmp oozi amp amp usr lib oozi bin oozi start sh return set oozi base url http ip ec intern oozi mnotic stage hdp oozi servic hdp exec exec su ooziexx c cd var tmp oozi amp amp usr lib oozi bin oozi start sh exec exec su ooziexx c cd var tmp oozi amp amp usr lib oozi bin oozi start sh return use catalina base var lib oozi oozi server mnotic stage hdp oozi servic hdp exec exec su ooziexx c cd var tmp oozi amp amp usr lib oozi bin oozi start sh exec exec su ooziexx c cd var tmp oozi amp amp usr lib oozi bin oozi start sh return set oozi http keystor file home ooziexx keystor mnotic stage hdp oozi servic hdp exec exec su ooziexx c cd var tmp oozi amp amp usr lib oozi bin oozi start sh exec exec su ooziexx c cd var tmp oozi amp amp usr lib oozi bin oozi start sh return set oozi http keystor pass password mnotic stage hdp oozi servic hdp exec exec su ooziexx c cd var tmp oozi amp amp usr lib oozi bin oozi start sh exec exec su ooziexx c cd var tmp oozi amp amp usr lib oozi bin oozi start sh return set catalina out var log oozi catalina mnotic stage hdp oozi servic hdp exec exec su ooziexx c cd var tmp oozi amp amp usr lib oozi bin oozi start sh exec exec su ooziexx c cd var tmp oozi amp amp usr lib oozi bin oozi start sh return use catalina pid var run oozi oozi pid mnotic stage hdp oozi servic hdp exec exec su ooziexx c cd var tmp oozi amp amp usr lib oozi bin oozi start sh exec exec su ooziexx c cd var tmp oozi amp amp usr lib oozi bin oozi start sh return mnotic stage hdp oozi servic hdp exec exec su ooziexx c cd var tmp oozi amp amp usr lib oozi bin oozi start sh exec exec su ooziexx c cd var tmp oozi amp amp usr lib oozi bin oozi start sh return use catalina opt dderbi stream error file var log oozi derbi log mnotic stage hdp oozi servic hdp exec exec su ooziexx c cd var tmp oozi amp amp usr lib oozi bin oozi start sh exec exec su ooziexx c cd var tmp oozi amp amp usr lib oozi bin oozi start sh return ad catalina opt doozi home dir usr lib oozi doozi config dir etc oozi conf doozi log dir var log oozi doozi data dir grid hadoop oozi data doozi config file oozi site xml doozi log j file oozi log j properti doozi log j reload doozi http hostnam ip ec intern doozi admin port doozi http port doozi http port doozi base url http ip ec intern oozi doozi http keystor file home ooziexx keystor doozi http keystor pass password djava librari path usr lib hadoop lib nativ linux amd mnotic stage hdp oozi servic hdp exec exec su ooziexx c cd var tmp oozi amp amp usr lib oozi bin oozi start sh exec exec su ooziexx c cd var tmp oozi amp amp usr lib oozi bin oozi start sh return mnotic stage hdp oozi servic hdp exec exec su ooziexx c cd var tmp oozi amp amp usr lib oozi bin oozi start sh exec exec su ooziexx c cd var tmp oozi amp amp usr lib oozi bin oozi start sh return usr lib oozi oozi server bin catalina sh line var run oozi oozi pid permiss deni merr stage hdp oozi servic hdp exec exec su ooziexx c cd var tmp oozi amp amp usr lib oozi bin oozi start sh exec exec su ooziexx c cd var tmp oozi amp amp usr lib oozi bin oozi start sh return chang notrun fail su ooziexx c cd var tmp oozi amp amp usr lib oozi bin oozi start sh return instead one var lib ambari agent puppet modul hdp manifest init pp mnotic stage hdp oozi servic hdp exec exec su ooziexx c cd var tmp oozi amp amp usr lib oozi bin oozi start sh anchor hdp exec exec su ooziexx c cd var tmp oozi amp amp usr lib oozi bin oozi start sh end depend exec exec su ooziexx c cd var tmp oozi amp amp usr lib oozi bin oozi start sh failur true mwarn stage hdp oozi servic hdp exec exec su ooziexx c cd var tmp oozi amp amp usr lib oozi bin oozi start sh anchor hdp exec exec su ooziexx c cd var tmp oozi amp amp usr lib oozi bin oozi start sh end skip fail depend mnotic stage hdp oozi servic hdp oozi servic directori var log oozi hdp directori recurs creat var log oozi hdp directori var log oozi file var log oozi owner owner chang oozi ooziexx mnotic stage hdp oozi servic hdp oozi servic directori var log oozi hdp directori recurs creat var log oozi hdp directori var log oozi file var log oozi group group chang oozi hadoopxx mnotic stage hdp oozi servic hdp oozi servic directori var run oozi hdp directori recurs creat var run oozi hdp directori var run oozi file var run oozi owner owner chang oozi ooziexx mnotic stage hdp oozi servic hdp oozi servic directori var run oozi hdp directori recurs creat var run oozi hdp directori var run oozi file var run oozi group group chang oozi hadoopxx mnotic finish catalog run second,0,0,0,0,0,0,
2147,Capture user for auditing config changes, captur user audit config chang,Add the ability to capture username and save in the table for config mappings. This applies to cluster and host level, add abil captur usernam save tabl config map thi appli cluster host level,0,0,0,0,0,0,
2149,Ambari needs to set right path for GC log directory of Hbase process., ambari need set right path gc log directori hbase process,Ambari needs to set right path for GC log directory of Hbase process., ambari need set right path gc log directori hbase process,0,0,0,0,0,0,
2152,Sometimes stale host / host component indicators are shown, sometim stale host host compon indic shown,,,0,0,0,0,0,0,
2159,After upgrading ambari from 1.2.2.5 to 1.2.3.6 the server throws 500 error when starting/stopping any service, after upgrad ambari server throw error start stop servic,After upgrading ambari from 1.2.2.5 to 1.2.3.6 the server throws 500 error when starting/stopping any service, after upgrad ambari server throw error start stop servic,0,0,0,0,0,0,
2161,Datanode Start fails in secure cluster., datanod start fail secur cluster,,,0,0,0,1,0,0,
2171,Host status filter not restored on Hosts page when navigating back, host statu filter restor host page navig back,,,0,0,0,0,0,0,
2172,Fix unit tests for Ambari Web, fix unit test ambari web,Fix currently failing unit tests., fix current fail unit test,0,0,0,0,0,0,
2173,TEST BROKEN : FAIL: test_upgradeCommand_executeCommand (TestActionQueue.TestActionQueue), test broken fail test upgrad command execut command test action queue test action queue,TEST BROKEN : FAIL: test_upgradeCommand_executeCommand (TestActionQueue.TestActionQueue), test broken fail test upgrad command execut command test action queue test action queue,0,0,0,0,0,0,
2180,Remove '0.1' stack definition since its never been used and is redundant., remov stack definit sinc never use redund,Remove '0.1' stack definition since its never been used and is redundant., remov stack definit sinc never use redund,0,0,0,0,0,0,
2187,Hadoop2 Monitoring: Jobs page should be hidden when HDP 2.0.x stack is installed, hadoop monitor job page hidden hdp x stack instal,When a HDP 2.0.x stack is installed  the Jobs page should be hidden., when hdp x stack instal job page hidden,0,0,0,0,0,0,
2188,Update mock json data for Test mode, updat mock json data test mode,,,0,0,0,0,0,0,
2192,Agent heartbeat lost during install, agent heartbeat lost instal,Agent heartbeat can become lost during install. The underlying issue is that during install  various yum commands are executed for component installation. However  the heartbeat ALSO performs a 'yum -C repolist'. If that command is taking a long time  the yum process can become deadlocked. The results of the repolist'ing are not used at this time  so remove it until needed., agent heartbeat becom lost instal the underli issu instal variou yum command execut compon instal howev heartbeat also perform yum c repolist if command take long time yum process becom deadlock the result repolist ing use time remov need,0,0,0,0,0,0,
2195,Ambari has a deadlock when re-installing after reboot of cluster nodes, ambari deadlock instal reboot cluster node,Java stack information for the threads listed above:==================================================='Thread-2': at org.apache.ambari.server.state.ServiceImpl.getDesiredConfigs(ServiceImpl.java:240) waiting to lock &lt;0x000000077b356dd0&gt; (a org.apache.ambari.server.state.ServiceImpl$$EnhancerByGuice$$9e2acafa) at org.apache.ambari.server.state.ServiceComponentImpl.getDesiredConfigs(ServiceComponentImpl.java:292) locked &lt;0x000000077b39bce8&gt; (a org.apache.ambari.server.state.ServiceComponentImpl$$EnhancerByGuice$$af7a745c) at org.apache.ambari.server.state.svccomphost.ServiceComponentHostImpl.getDesiredConfigs(ServiceComponentHostImpl.java:1057) at org.apache.ambari.server.agent.HeartbeatMonitor.generateStatusCommands(HeartbeatMonitor.java:166) at org.apache.ambari.server.agent.HeartbeatMonitor.doWork(HeartbeatMonitor.java:137) at org.apache.ambari.server.agent.HeartbeatMonitor.run(HeartbeatMonitor.java:85) at java.lang.Thread.run(Thread.java:662)'main': at org.apache.ambari.server.state.ServiceComponentImpl.debugDump(ServiceComponentImpl.java:376) waiting to lock &lt;0x000000077b39bce8&gt; (a org.apache.ambari.server.state.ServiceComponentImpl$$EnhancerByGuice$$af7a745c) at org.apache.ambari.server.state.ServiceImpl.debugDump(ServiceImpl.java:354) locked &lt;0x000000077b356dd0&gt; (a org.apache.ambari.server.state.ServiceImpl$$EnhancerByGuice$$9e2acafa) at org.apache.ambari.server.state.cluster.ClusterImpl.debugDump(ClusterImpl.java:693) at org.apache.ambari.server.state.cluster.ClustersImpl.debugDump(ClustersImpl.java:517) at org.apache.ambari.server.controller.AmbariServer.run(AmbariServer.java:320) at org.apache.ambari.server.controller.AmbariServer.main(AmbariServer.java:432)Found 1 deadlock., java stack inform thread list thread org apach ambari server state servic impl get desir config servic impl java wait lock lt x b dd gt org apach ambari server state servic impl enhanc by guic e acafa org apach ambari server state servic compon impl get desir config servic compon impl java lock lt x b bce gt org apach ambari server state servic compon impl enhanc by guic af c org apach ambari server state svccomphost servic compon host impl get desir config servic compon host impl java org apach ambari server agent heartbeat monitor gener statu command heartbeat monitor java org apach ambari server agent heartbeat monitor work heartbeat monitor java org apach ambari server agent heartbeat monitor run heartbeat monitor java java lang thread run thread java main org apach ambari server state servic compon impl debug dump servic compon impl java wait lock lt x b bce gt org apach ambari server state servic compon impl enhanc by guic af c org apach ambari server state servic impl debug dump servic impl java lock lt x b dd gt org apach ambari server state servic impl enhanc by guic e acafa org apach ambari server state cluster cluster impl debug dump cluster impl java org apach ambari server state cluster cluster impl debug dump cluster impl java org apach ambari server control ambari server run ambari server java org apach ambari server control ambari server main ambari server java found deadlock,0,0,0,0,1,0,
2200,ambari-server start script (ambari-server.py) will never use SERVER_START_CMD_DEBUG, ambari server start script ambari server py never use server start cmd debug,The ambari-server.py start script has a command defined for starting the ambari-server in debug mode (SERVER_START_CMD_DEBUG  which turns on remote debugging)  but there is currently no option supported that will force the script to use the debug start commaand. I propose adding a --debug option so that you can run 'ambari-server start --debug' to activate remote debugging., the ambari server py start script command defin start ambari server debug mode server start cmd debug turn remot debug current option support forc script use debug start commaand i propos ad debug option run ambari server start debug activ remot debug,0,0,0,0,0,0,
2203,Background operations popup does not automatically refresh the task log, background oper popup automat refresh task log,,,0,0,0,0,0,0,
2207,Add unit tests for Utils, add unit test util,,,0,0,0,0,0,0,
2208,Reassign Master Wizard: refreshing page on step 2  3 or 4 breaks wizard, reassign master wizard refresh page step break wizard,,,0,0,0,0,0,0,
2212,Change config loading mechanism to allow for different stack versions, chang config load mechan allow differ stack version,,,0,0,0,0,0,0,
2217,Increase ambari-agent test coverage, increas ambari agent test coverag,ActionQueue.py missing 'Unrecognized command' testcase (L. 173) /src/test/python/TestActionQueue.py:42 unused test_RetryAction stub (retry is implemented in another way) /src/main/python/ambari_agent/ActionQueue.py:221 not covered case if commandresult&#91;&#39;exitcode&#39;&#93; != 0: /src/main/python/ambari_agent/ActionQueue.py:247 not covered case if command.has_key('roleCommand') and command&#91;&#39;roleCommand&#39;&#93; == 'START':PuppetExecutor.py configureEnviron/generate_repo_manifests/run_manifest/runCommand are not covered.PythonExecutor.py isSuccessfull is not testedRepoInstaller.py prepareReposInfo/generateFiles are not coveredshell.py is not covered, action queue py miss unrecogn command testcas l src test python test action queue py unus test retri action stub retri implement anoth way src main python ambari agent action queue py cover case commandresult exitcod src main python ambari agent action queue py cover case command key role command command role command start puppet executor py configur environ gener repo manifest run manifest run command cover python executor py successful test repo instal py prepar repo info gener file coveredshel py cover,0,0,0,0,0,0,
2223,Using an external MySQL / Oracle database for Oozie does not work, use extern my sql oracl databas oozi work,When setting up Oozie with an external database  the following commands are run:cd /var/tmp/oozie &amp;&amp; /usr/lib/oozie/bin/oozie-setup.sh -hadoop 0.20.200 /usr/lib/hadoop/ -extjs /usr/share/HDP-oozie/ext.zip -jars /usr/lib/hadoop/lib/hadoop-lzo-0.5.0.jar:/usr/share/java/mysql-connector-java.jarThe above command succeeds.However  the next command fails:cd /var/tmp/oozie &amp;&amp; /usr/lib/oozie/bin/ooziedb.sh create -sqlfile oozie.sql -run setting OOZIE_CONFIG=${OOZIE_CONFIG:-/etc/oozie/conf} setting OOZIE_DATA=${OOZIE_DATA:-/var/lib/oozie} setting OOZIE_LOG=${OOZIE_LOG:-/var/log/oozie} setting CATALINA_BASE=${CATALINA_BASE:-/var/lib/oozie/oozie-server} setting CATALINA_TMPDIR=${CATALINA_TMPDIR:-/var/tmp/oozie} setting CATALINA_PID=${CATALINA_PID:-/var/run/oozie/oozie.pid} setting JAVA_HOME=/usr/jdk/jdk1.6.0_31 setting OOZIE_LOG=/var/log/oozie/ setting CATALINA_PID=/var/run/oozie/oozie.pid setting OOZIE_DATA=/grid/0/hadoop/oozie/data/ setting JAVA_LIBRARY_PATH=/usr/lib/hadoop/lib/native/Linux-amd64-64Validate DB ConnectionError: Could not connect to the database: java.lang.ClassNotFoundException: com.mysql.jdbc.DriverStack trace for the error was (for debug purposes):--------------------------------------java.lang.Exception: Could not connect to the database: java.lang.ClassNotFoundException: com.mysql.jdbc.Driver at org.apache.oozie.tools.OozieDBCLI.validateConnection(OozieDBCLI.java:358) at org.apache.oozie.tools.OozieDBCLI.createDB(OozieDBCLI.java:168) at org.apache.oozie.tools.OozieDBCLI.run(OozieDBCLI.java:112) at org.apache.oozie.tools.OozieDBCLI.main(OozieDBCLI.java:63)Caused by: java.lang.ClassNotFoundException: com.mysql.jdbc.Driver at java.net.URLClassLoader$1.run(URLClassLoader.java:202) at java.security.AccessController.doPrivileged(Native Method) at java.net.URLClassLoader.findClass(URLClassLoader.java:190) at java.lang.ClassLoader.loadClass(ClassLoader.java:306) at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:301) at java.lang.ClassLoader.loadClass(ClassLoader.java:247) at java.lang.Class.forName0(Native Method) at java.lang.Class.forName(Class.java:169) at org.apache.oozie.tools.OozieDBCLI.createConnection(OozieDBCLI.java:347) at org.apache.oozie.tools.OozieDBCLI.validateConnection(OozieDBCLI.java:354) ... 3 more--------------------------------------, when set oozi extern databas follow command run cd var tmp oozi amp amp usr lib oozi bin oozi setup sh hadoop usr lib hadoop extj usr share hdp oozi ext zip jar usr lib hadoop lib hadoop lzo jar usr share java mysql connector java jar the command succe howev next command fail cd var tmp oozi amp amp usr lib oozi bin ooziedb sh creat sqlfile oozi sql run set oozi config oozi config etc oozi conf set oozi data oozi data var lib oozi set oozi log oozi log var log oozi set catalina base catalina base var lib oozi oozi server set catalina tmpdir catalina tmpdir var tmp oozi set catalina pid catalina pid var run oozi oozi pid set java home usr jdk jdk set oozi log var log oozi set catalina pid var run oozi oozi pid set oozi data grid hadoop oozi data set java librari path usr lib hadoop lib nativ linux amd valid db connect error could connect databas java lang class not found except com mysql jdbc driver stack trace error debug purpos java lang except could connect databas java lang class not found except com mysql jdbc driver org apach oozi tool oozi dbcli valid connect oozi dbcli java org apach oozi tool oozi dbcli creat db oozi dbcli java org apach oozi tool oozi dbcli run oozi dbcli java org apach oozi tool oozi dbcli main oozi dbcli java caus java lang class not found except com mysql jdbc driver java net url class loader run url class loader java java secur access control privileg nativ method java net url class loader find class url class loader java java lang class loader load class class loader java sun misc launcher app class loader load class launcher java java lang class loader load class class loader java java lang class name nativ method java lang class name class java org apach oozi tool oozi dbcli creat connect oozi dbcli java org apach oozi tool oozi dbcli valid connect oozi dbcli java,0,0,0,0,1,0,
2228,Fix MySQL and Oracle DDL scripts according to last DB changes, fix my sql oracl ddl script accord last db chang,user_name column was added to clusterconfigmapping and hostconfigmapping tables. This changes should be made to DDL scripts for Oracle and MySQL also., user name column ad clusterconfigmap hostconfigmap tabl thi chang made ddl script oracl my sql also,0,0,0,0,0,0,
2233,Ensure version values are used appropriately throughout Ambari, ensur version valu use appropri throughout ambari,The current version of Ambari build is being used in several scenarios: Ensure Ambari Server installs the correct version of Ambari Agent Ensure that Ambari Server only accepts registration from correct version of Ambari Agent Ensure that DB version is compatible with Ambari Server  The DB version itself will be used to control DB upgrades  Towards this end the following open issues remain: Get the build version be automatically embedded in the version file Use the above for deploying agent as well as allowing agents to register Regarding DB version there are two possible paths:  Separate out DB version and have it be modified manually as needed Have the build version be used as DB version - this may make writing upgrade scripts little complicated as build version may change due to some proj mgmt decision, the current version ambari build use sever scenario ensur ambari server instal correct version ambari agent ensur ambari server accept registr correct version ambari agent ensur db version compat ambari server the db version use control db upgrad toward end follow open issu remain get build version automat embed version file use deploy agent well allow agent regist regard db version two possibl path separ db version modifi manual need have build version use db version may make write upgrad script littl complic build version may chang due proj mgmt decis,1,0,0,0,0,0,
2240,Allow Security related configs to be modified via custom settings, allow secur relat config modifi via custom set,,,0,0,0,0,0,0,
2259,Start/Stop button may stay enabled for 30-40 seconds after it has been clicked, start stop button may stay enabl second click,Start/Stop button stays enabled for atleast 30-40 seconds after its been clicked already., start stop button stay enabl atleast second click alreadi,1,0,0,0,0,0,
2260,Bad hosts query example in API docs, bad host queri exampl api doc,The example ...'hosts' : [ { 'href' : 'http://your.ambari.server/api/v1/clusters/c1/hosts/host1'  'Hosts' : { 'cluster_name' : 'c1'  'host_name' : 'some.cluster.host' } }  { 'href' : 'http://your.ambari.server/api/v1/clusters/c1/hosts/host2'  'Hosts' : { 'cluster_name' : 'c1'  'host_name' : 'another.cluster.host' } ]... should read ... 'hosts' : [ { 'href' : 'http://your.ambari.server/api/v1/clusters/c1/hosts/some.host'  'Hosts' : { 'cluster_name' : 'c1'  'host_name' : 'some.host' } }  { 'href' : 'http://your.ambari.server/api/v1/clusters/c1/hosts/another.host'  'Hosts' : { 'cluster_name' : 'c1'  'host_name' : 'another.host' } } ], the exampl host href http ambari server api v cluster c host host host cluster name c host name cluster host href http ambari server api v cluster c host host host cluster name c host name anoth cluster host read host href http ambari server api v cluster c host host host cluster name c host name host href http ambari server api v cluster c host anoth host host cluster name c host name anoth host,0,0,0,0,0,0,
2262,On 'install Options' page  when selecting 'Perform manual registration on hosts and do not use SSH' is setting 'Path to 64-bit JDK' disabled, on instal option page select perform manual registr host use ssh set path bit jdk disabl,On 'install Options' page  when selecting 'Perform manual registration on hosts and do not use SSH' is setting 'Path to 64-bit JDK' disabled(Screen Shot 2013-06-03 at 10.54.49 AM.png).Also path to 64-bit JDK JAVA_HOME' input field is enabled with unchecked check box(unchecked.png).Steps:1. Go to 'Install Options' page.Result:'Path to 64-bit JDK JAVA_HOME' input field is available for editing when check box is unchecked., on instal option page select perform manual registr host use ssh set path bit jdk disabl screen shot am png also path bit jdk java home input field enabl uncheck check box uncheck png step go instal option page result path bit jdk java home input field avail edit check box uncheck,0,0,0,1,0,0,
2270,Provide way to optionally enable two-way SSL for Server-Agent communication, provid way option enabl two way ssl server agent commun,The two-way SSL mechanism used during server-agent registration exists to protect communication. This is useful in production environments but in typical 'first use' or POC scenarios  having this level of security is not necessary. As well  certificate generation can be problematic causing failures.We need to provide a way to make this mechanism optional:1) By default  ship with Server-Agent Two-Way SSL off.2) At any time post install  a user should be able to turn on Two-Way SSL and turn it back off  etc., the two way ssl mechan use server agent registr exist protect commun thi use product environ typic first use poc scenario level secur necessari as well certif gener problemat caus failur we need provid way make mechan option by default ship server agent two way ssl at time post instal user abl turn two way ssl turn back etc,0,0,0,1,0,0,
2279,Configuration mapping metadata on ambari-web should be computed as per the stack selection., configur map metadata ambari web comput per stack select,,,0,0,0,0,0,0,
2290,Ambari Upgrade prcoess should preserve the old configs and add the new config options to the old config files., ambari upgrad prcoess preserv old config add new config option old config file,The Ambari Upgrade process should preserve the old configs and add the new config options to the old config files.Currently we have it the other way around  that we copy the needed 3 properties from the old config files - this is wrong. We need to use the older config file and add the new options to the old config file. This is because the older config file can have all kinds of config option that the user might have used. We have to really really keep in mind usability of the product when fixing issues., the ambari upgrad process preserv old config add new config option old config file current way around copi need properti old config file wrong we need use older config file add new option old config file thi older config file kind config option user might use we realli realli keep mind usabl product fix issu,1,0,0,0,0,0,
2300,500 Exception creating service component during install, except creat servic compon instal,I was installing a new cluster in my VM when the progress blocked on step 12. Looking on browser log  the PUTs for clusters desired_configs succeeded  but the very next call to create service component failed.&#91;POST&#93; http://dev.hortonworks.com:8080/api/v1/clusters/vmc/services?ServiceInfo/service_name=HDFSStatus Code:??500 Invalid arguments  clustername and componentname should be non-null and non-empty when trying to create a componentData uploaded:{'components':[{'ServiceComponentInfo':{'component_name':'NAMENODE'}} {'ServiceComponentInfo':{'component_name':'SECONDARY_NAMENODE'}} {'ServiceComponentInfo':{'component_name':'DATANODE'}} {'ServiceComponentInfo':{'component_name':'HDFS_CLIENT'}}]}:Exception on server console:Mar 21  2013 11:25:09 AM com.sun.jersey.spi.container.ContainerResponse mapMappableContainerExceptionSEVERE: The RuntimeException could not be mapped to a response  re-throwing to the HTTP containerjava.lang.IllegalArgumentException: Invalid arguments  clustername and componentname should be non-null and non-empty when trying to create a component at org.apache.ambari.server.controller.AmbariManagementControllerImpl.createComponents(AmbariManagementControllerImpl.java:387) at org.apache.ambari.server.controller.internal.ComponentResourceProvider$1.invoke(ComponentResourceProvider.java:88) at org.apache.ambari.server.controller.internal.ComponentResourceProvider$1.invoke(ComponentResourceProvider.java:85) at org.apache.ambari.server.controller.internal.AbstractResourceProvider.createResources(AbstractResourceProvider.java:229) at org.apache.ambari.server.controller.internal.ComponentResourceProvider.createResources(ComponentResourceProvider.java:85) at org.apache.ambari.server.controller.internal.ClusterControllerImpl.createResources(ClusterControllerImpl.java:131) at org.apache.ambari.server.api.services.persistence.PersistenceManagerImpl.create(PersistenceManagerImpl.java:75) at org.apache.ambari.server.api.handlers.QueryCreateHandler.persist(QueryCreateHandler.java:163) at org.apache.ambari.server.api.handlers.QueryCreateHandler.handleRequest(QueryCreateHandler.java:68) at org.apache.ambari.server.api.services.BaseRequest.process(BaseRequest.java:98) at org.apache.ambari.server.api.services.BaseService.handleRequest(BaseService.java:73) at org.apache.ambari.server.api.services.ServiceService.createServices(ServiceService.java:114) at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39) at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25) at java.lang.reflect.Method.invoke(Method.java:597), i instal new cluster vm progress block step look browser log pu ts cluster desir config succeed next call creat servic compon fail post http dev hortonwork com api v cluster vmc servic servic info servic name hdf statu code invalid argument clusternam componentnam non null non empti tri creat compon data upload compon servic compon info compon name namenod servic compon info compon name secondari namenod servic compon info compon name datanod servic compon info compon name hdf client except server consol mar am com sun jersey spi contain contain respons map mappabl contain except sever the runtim except could map respons throw http containerjava lang illeg argument except invalid argument clusternam componentnam non null non empti tri creat compon org apach ambari server control ambari manag control impl creat compon ambari manag control impl java org apach ambari server control intern compon resourc provid invok compon resourc provid java org apach ambari server control intern compon resourc provid invok compon resourc provid java org apach ambari server control intern abstract resourc provid creat resourc abstract resourc provid java org apach ambari server control intern compon resourc provid creat resourc compon resourc provid java org apach ambari server control intern cluster control impl creat resourc cluster control impl java org apach ambari server api servic persist persist manag impl creat persist manag impl java org apach ambari server api handler queri creat handler persist queri creat handler java org apach ambari server api handler queri creat handler handl request queri creat handler java org apach ambari server api servic base request process base request java org apach ambari server api servic base servic handl request base servic java org apach ambari server api servic servic servic creat servic servic servic java sun reflect nativ method accessor impl invok nativ method sun reflect nativ method accessor impl invok nativ method accessor impl java sun reflect deleg method accessor impl invok deleg method accessor impl java java lang reflect method invok method java,1,0,0,0,0,0,
2313,UI allows adding already existing properties to custom core-site.xml /hdfs-site.xml settings and creates confusion, ui allow ad alreadi exist properti custom core site xml hdf site xml set creat confus,Steps:1. Go to 'Services' page.2. Select 'HDFS' service.3. Select 'Configs' tab.4. Open 'Custom core-site.xml' panel.5. Add property with name 'ipc.client.idlethreshold' to this panel.Result:Property with name 'ipc.client.idlethreshold' was added to 'Custom core-site.xml' panel. But there is already presented property with same name in 'core-site.xml' file. After saving added property was disappeared from UI  and after service starting value of the old was changed to new (on UI and in 'core-site.xml' file).Expected result:UI should not allow to add property with existing name in specified file., step go servic page select hdf servic select config tab open custom core site xml panel add properti name ipc client idlethreshold panel result properti name ipc client idlethreshold ad custom core site xml panel but alreadi present properti name core site xml file after save ad properti disappear ui servic start valu old chang new ui core site xml file expect result ui allow add properti exist name specifi file,0,0,0,0,0,0,
2337,Security Wizard: navigation not locked down  causes artifacts  and other unwanted side effects, secur wizard navig lock caus artifact unwant side effect,,,1,0,0,1,0,0,
2346,API call to get 'metrics/cpu' does not work for NameNode and JobTracker host components, api call get metric cpu work name node job tracker host compon,1. http://ambari:8080/api/v1/clusters/cluster/services?fields=components/host_components/metricsincludes all metrics for NameNode and JobTracker  including cpu metrics.2. http://ambari:8080/api/v1/clusters/cluster/services?fields=components/host_components/metrics/cpudoes not return cpu metrics for NameNode and JobTracker, http ambari api v cluster cluster servic field compon host compon metricsinclud metric name node job tracker includ cpu metric http ambari api v cluster cluster servic field compon host compon metric cpudo return cpu metric name node job tracker,1,0,0,0,0,0,
2349,Enhance processing of ojdbc.jar before starting ambari server, enhanc process ojdbc jar start ambari server,Enhancements: Read RESOURCE_DIR from the ambari.properties Ask user twice to place drivers to /usr/share/java, enhanc read resourc dir ambari properti ask user twice place driver usr share java,1,0,0,0,0,0,
2362,Unit Tests: Added tests to install wizard for step 3  5  10, unit test ad test instal wizard step,,,1,0,0,0,0,0,
2363,Intermittent test failure with HBase port Scanner test., intermitt test failur h base port scanner test,This test fails sometimes and is not very reliable., thi test fail sometim reliabl,1,0,0,0,0,0,
2371,Security Wizard: webhcat Server start fails on enabling security, secur wizard webhcat server start fail enabl secur,This happens when templeton.kerberos.principal property is set to HTTP/_HOST@&lt;realm name&gt; instead of HTTP/&lt;internal host name&gt;@&lt;realm name&gt;, thi happen templeton kerbero princip properti set http host lt realm name gt instead http lt intern host name gt lt realm name gt,1,0,0,1,0,0,
2372,Show installed stack and its services, show instal stack servic,List the installed stack and its services in Admin &gt; Cluster page., list instal stack servic admin gt cluster page,1,0,0,0,0,0,
2375,Unit Tests: Added tests to models, unit test ad test model,Added unit tests to models: Rack  Host  HostComponent., ad unit test model rack host host compon,1,0,0,0,0,0,
2377,Add Host Wizard: show info about manual steps required on a secure cluster, add host wizard show info manual step requir secur cluster,When a cluster is in secure mode  there are additional manual steps that need to be performed when adding new hosts to the cluster.In the Review page  add a prominent box with a red background and display the following text:You are running your cluster in secure mode. You must set up the keytabs for all the hosts you are adding before you proceed.'Upon clicking 'Deploy'  show a confirmation popup with the text:Before you proceed  please make sure that the keytabs have been set up on the hosts you are adding per the instructions on the Review page. Otherwise  the assigned components will not be able to start properly on the hosts being added. OK CancelNote that the extra message and confirmation popup should show only when security is enabled on the cluster., when cluster secur mode addit manual step need perform ad new host cluster in review page add promin box red background display follow text you run cluster secur mode you must set keytab host ad proceed upon click deploy show confirm popup text befor proceed pleas make sure keytab set host ad per instruct review page otherwis assign compon abl start properli host ad ok cancel note extra messag confirm popup show secur enabl cluster,1,0,0,0,0,0,
2383,Add unit tests for ambari-server python changes, add unit test ambari server python chang,AMBARI-2174 - Add missing unit tests for the code changes, ambari add miss unit test code chang,1,0,0,0,0,0,
2391,Bootstrap is broken for ambari web with RHEL-5.8, bootstrap broken ambari web rhel,Steps:1. Install ambari-server.2. Go to 'Install Options' page.3. Set the hosts list and ssh-key  click 'Next' button.Result:Ambari Web is blocked in 'Confirm Hosts'. Confirming was not ended after not less than 40 minutes., step instal ambari server go instal option page set host list ssh key click next button result ambari web block confirm host confirm end less minut,1,0,0,0,0,0,
2394,ambari-server setup borken in trunk, ambari server setup borken trunk,install_jce_manually() in the download_jdk() result in fatal exception.Checking JDK...INFO: Loading properties from /etc/ambari-server/conf/ambari.propertiesERROR: Error getting ambari propertiesERROR: Exiting with exit code -1. Reason: Downloading or installing JDK failed: 'Fatal exception: Error getting ambari properties  exit code -1'. Exiting., instal jce manual download jdk result fatal except check jdk info load properti etc ambari server conf ambari properti error error get ambari properti error exit exit code reason download instal jdk fail fatal except error get ambari properti exit code exit,1,0,0,0,0,0,
2402,Add support for 'classic' dashboard, add support classic dashboard,,,1,0,0,0,0,0,
2403,ambari-server setup should allow user to change database password, ambari server setup allow user chang databas password,If we run setup second time. Amabri should allow user to change the DB password for the ambari-user.Currently  the executed script should allow for this.command: &#91;&#39;su&#39;  &#39;-&#39;  &#39;postgres&#39;  &#39;--command=psql -f /var/lib/ambari-server/resources/Ambari-DDL-Postgres-CREATE.sql -v username=/&#39;&quot;ambari-server&quot;/&#39; -v password=&quot;/&#39;/&#39;&quot;&#39;&#93;, if run setup second time amabri allow user chang db password ambari user current execut script allow command su postgr command psql f var lib ambari server resourc ambari ddl postgr creat sql v usernam quot ambari server quot v password quot quot,0,0,0,1,0,0,
2408,Kerberos globals are shown in HDFS config page during install, kerbero global shown hdf config page instal,,,1,0,0,0,0,0,
2413,Installer Wizard step-6: NameNode and SNameNode should not be co-hosted by default on multinode cluster., instal wizard step name node s name node co host default multinod cluster,,,0,0,0,0,0,0,
2414,HDFS Config page is broken in testMode on trunk, hdf config page broken test mode trunk,HDFS Config page (post-install) does not load when App.testMode = true, hdf config page post instal load app test mode true,0,0,0,0,0,0,
2426,Set default widgets to show for the Dashboard, set default widget show dashboard,,,1,0,0,0,0,0,
2433,Bootstrap failed on rhel 5.6, bootstrap fail rhel,STDOUTTraceback (most recent call last):File '/tmp/setupAgent.py'  line 192  in ?main(sys.argv)File '/tmp/setupAgent.py'  line 188  in mainsys.exit(runAgent(passPhrase  expected_hostname))File '/tmp/setupAgent.py'  line 80  in runAgentagent_retcode = subprocess.call('/usr/sbin/ambari-agent start --expected-hostname={0}'.format(expected_hostname)  shell=True)AttributeError: 'str' object has no attribute 'format'Error seems to be caused by python 2.4 version., stdout traceback recent call last file tmp setup agent py line main sy argv file tmp setup agent py line mainsi exit run agent pass phrase expect hostnam file tmp setup agent py line run agentag retcod subprocess call usr sbin ambari agent start expect hostnam format expect hostnam shell true attribut error str object attribut format error seem caus python version,1,0,0,0,0,0,
2441,Ambari server start fails with reconfigured user, ambari server start fail reconfigur user,STR:1) Run ambari-server setup.2) Choose custom user  user1.3) Delete ambari.user from ambari.properties.4) Run ambari-server setup.5) Choose custom user  different from choosen in step 2  user2.6) Run ambari-server start.Got error:ambari-server startUsing python /usr/bin/python2.6Starting ambari-serverHave root privileges.Checking iptables...iptables is disabled nowRunning server: &#91;&#39;/bin/su&#39;  &#39;user2&#39;  &#39;-s&#39;  &#39;/bin/sh&#39;  &#39;-c&#39;  &#39;/usr/jdk64/jdk1.6.0_31/bin/java -server -XX:NewRatio=3 -XX:+UseConcMarkSweepGC -XX:-UseGCOverheadLimit -XX:CMSInitiatingOccupancyFraction=60 -Xms512m -Xmx2048m -cp /etc/ambari-server/conf:/usr/lib/ambari-server/*:/usr/lib64/qt-3.3/bin:/usr/local/sbin:/usr/local/bin:/sbin:/bin:/usr/sbin:/usr/bin:/root/bin:/usr/lib/ambari-server/* org.apache.ambari.server.controller.AmbariServer &gt;/var/log/ambari-server/ambari-server.out 2&gt;&amp;1 &amp; echo $! &gt; /var/run/ambari-server/ambari-server.pid&#39;&#93;done.sh: /var/run/ambari-server/ambari-server.pid: Permission deniedsh: /var/log/ambari-server/ambari-server.out: Permission denied, str run ambari server setup choos custom user user delet ambari user ambari properti run ambari server setup choos custom user differ choosen step user run ambari server start got error ambari server start use python usr bin python start ambari server have root privileg check iptabl iptabl disabl run server bin su user bin sh c usr jdk jdk bin java server xx new ratio xx use conc mark sweep gc xx use gc overhead limit xx cm initi occup fraction xm xmx cp etc ambari server conf usr lib ambari server usr lib qt bin usr local sbin usr local bin sbin bin usr sbin usr bin root bin usr lib ambari server org apach ambari server control ambari server gt var log ambari server ambari server gt amp amp echo gt var run ambari server ambari server pid done sh var run ambari server ambari server pid permiss deniedsh var log ambari server ambari server permiss deni,1,0,0,0,0,0,
2443,Security wizard: smoke test for services fails with customized service user names., secur wizard smoke test servic fail custom servic user name,,,1,0,0,1,0,0,
2447,Upgrade from 1.2.2/1.2.3.7 to 1.2.5 fails because the ddl script does not work - metainfo version change is broken., upgrad fail ddl script work metainfo version chang broken,Upgrade from 1.2.2/1.2.3.7 to 1.2.5 fails because the ddl script does not work - metainfo version change is broken.Again we need thorugh testing around this. Please make sure we have tested 1.2.2 and 1.2.3  1.2.4 upgrade to 1.2.5., upgrad fail ddl script work metainfo version chang broken again need thorugh test around pleas make sure test upgrad,0,0,0,0,0,0,
2461,Add unit tests for bootstrap and setupAgent python scripts for the server., add unit test bootstrap setup agent python script server,Add unit tests for bootstrap and setupAgent python scripts for the server.We need to find gaps on the bootstrap and setupagent script and add unit tests for them., add unit test bootstrap setup agent python script server we need find gap bootstrap setupag script add unit test,1,0,0,0,0,0,
2465,Create command line script to manipulate Ambari configurations (get  set  add  delete), creat command line script manipul ambari configur get set add delet,We need a script which can manually set the configs when UI is not able to  or does not support., we need script manual set config ui abl support,1,0,0,0,0,0,
2466,Hive/Oozie database settings should accept custom JDBC URLs, hive oozi databas set accept custom jdbc ur ls,Ran into issues setting up Hive and Oozie with an Oracle database.1. We are hard-coding port 1521 for the JDBC URL. 2. There are two types of JDBC URLs for Oracle: jdbc:oracle:thin:@&#91;HOST&#93;&#91;:PORT&#93;:SID jdbc:oracle:thin:@//&#91;HOST&#93;&#91;:PORT&#93;/SERVICEWe are making the assumption that it is the latter  but this may not work depending on how Oracle is set up.3. We prompt for the 'Database Name'. In Oracle context  this could be the SID or SERVICE NAME  but it's not clear what this is.As a solution to all of the above  we will construct the JDBC URL based on the database type  host  and name for Hive/Oozie and present it to the user as an editable text field during install.Post-install  the JDBC URL remains editable  but does not change automatically as changes other database-related parameters., ran issu set hive oozi oracl databas we hard code port jdbc url there two type jdbc ur ls oracl jdbc oracl thin host port sid jdbc oracl thin host port servic we make assumpt latter may work depend oracl set we prompt databas name in oracl context could sid servic name clear as solut construct jdbc url base databas type host name hive oozi present user edit text field instal post instal jdbc url remain edit chang automat chang databas relat paramet,1,0,0,0,0,0,
2471,Remove unnecessary check for hostnames/ service name which is wrong., remov unnecessari check hostnam servic name wrong,Remove unnecessary check for hostnames/ service name which is wrong., remov unnecessari check hostnam servic name wrong,1,0,0,0,0,0,
2475,Ambari bootstrap actions report success even if a failure happened, ambari bootstrap action report success even failur happen,Sometimes  bootstrap actions reports success even if it has failed host tasks:&lt;bootStrapRequest&gt; &lt;status&gt;SUCCESS&lt;/status&gt; &lt;hostsStatus&gt; &lt;hostName&gt;andromeda54.hi.inet&lt;/hostName&gt; &lt;status&gt;FAILED&lt;/status&gt; &lt;statusCode&gt;1&lt;/statusCode&gt; &lt;log&gt; [...] &lt;/log&gt; &lt;/hostsStatus&gt; &lt;hostsStatus&gt; &lt;hostName&gt;andromeda55.hi.inet&lt;/hostName&gt; &lt;status&gt;FAILED&lt;/status&gt; &lt;statusCode&gt;1&lt;/statusCode&gt; &lt;log&gt; [...] &lt;/log&gt; &lt;/hostsStatus&gt; &lt;log&gt; [...] &lt;/log&gt;&lt;/bootStrapRequest&gt;, sometim bootstrap action report success even fail host task lt boot strap request gt lt statu gt success lt statu gt lt host statu gt lt host name gt andromeda hi inet lt host name gt lt statu gt fail lt statu gt lt statu code gt lt statu code gt lt log gt lt log gt lt host statu gt lt host statu gt lt host name gt andromeda hi inet lt host name gt lt statu gt fail lt statu gt lt statu code gt lt statu code gt lt log gt lt log gt lt host statu gt lt log gt lt log gt lt boot strap request gt,1,0,0,0,0,0,
2480,Dashboard page has a lot of footer padding, dashboard page lot footer pad,This ticket solved:1. Dashboard page has padding space under footer.2. When a widget got deleted  the page scroll to top.(page should stay in place), thi ticket solv dashboard page pad space footer when widget got delet page scroll top page stay place,1,0,0,0,0,0,
2486,shell.killprocessgrp is not working in a reliable way, shell killprocessgrp work reliabl way,We have noticed an issue where shell.killprocessgrp is not working correctly. We have seen a scenario where namenode start takes a long time when on a secure cluster the jce policy is unavailable. After 10 minutes when the agent tries to kill the puppet process it invariably fails.We need to run some experiment (perhaps using long running puppet processes) to ensure that shell.killprocessgrp works as expected.Also  we need to verify that the behavior is as expected on both RHEL and Suse., we notic issu shell killprocessgrp work correctli we seen scenario namenod start take long time secur cluster jce polici unavail after minut agent tri kill puppet process invari fail we need run experi perhap use long run puppet process ensur shell killprocessgrp work expect also need verifi behavior expect rhel suse,1,0,0,0,0,0,
2490,Issues with setup ldap, issu setup ldap,I got this blow-up (when bind anon was either false or I just pressed returnBind anonymously true/false (false):====================Review Settings====================authentication.ldap.primaryUrl: my.ldap:389authentication.ldap.secondaryUrl: asdauthentication.ldap.useSSL: falseauthentication.ldap.usernameAttribute: uidauthentication.ldap.baseDn: basednauthorization.userRoleName: userauthorization.adminRoleName: adminauthentication.ldap.bindAnonymously: falseTraceback (most recent call last):File '/usr/sbin/ambari-server.py'  line 3047  in &lt;module&gt;main()File '/usr/sbin/ambari-server.py'  line 2891  in mainsetup_ldap()File '/usr/sbin/ambari-server.py'  line 2353  in setup_ldapprint('%s: %s' % (property  ldap_property_value_mapproperty))KeyError: 'authentication.ldap.managerDn', i got blow bind anon either fals i press return bind anonym true fals fals review set authent ldap primari url ldap authent ldap secondari url asdauthent ldap use ssl falseauthent ldap usernam attribut uidauthent ldap base dn basednauthor user role name userauthor admin role name adminauthent ldap bind anonym fals traceback recent call last file usr sbin ambari server py line lt modul gt main file usr sbin ambari server py line mainsetup ldap file usr sbin ambari server py line setup ldapprint properti ldap properti valu mapproperti key error authent ldap manag dn,1,0,0,0,0,0,
2491,Security Wizard: show which principals and keytabs need to be created on which hosts, secur wizard show princip keytab need creat host,Currently it is very difficult to know what principals and keytabs need to be created on which hosts.We should present this information to the end user in a format that is easy to consume.The user running the wizard may not be the one who will be creating keytabs and principals. We can expose the capability to download a csv file and send it to the appropriate person who may parse the data to create a script to generate principals/keytabs (or do so manually).Display the attached as a popup after Configure Services step is done.Let's show it as a popup so that we don't affect any existing navigation/flow.For generating the content:Keytab paths are based on the user inputPrincipal names are based on the user inputNameNode host: show the nn and HTTP principals and keytab pathsJobTracker host: show the jt principal and keytab pathOozie Server host: show the oozie and HTTP principals and keytab pathsNagios Server host: show the nagios principal and keytab pathHBase Master host: show the hbase principal and keytab pathHive Server host: show the hive principal and keytab pathWebHCat Server host: show the HTTP principal and keytab pathZooKeeper Server host: show the zookeeper principal and keytab pathDataNode host: show the dn principal and keytab pathTaskTracker host: show the tt principal and keytab pathRegionServer host: show the hbase principal and keytab pathIf there are duplicated principals on the same host  display it only once.Clickng on 'Download CSV' downloads the CSV file ('host-principal-keytab-list.csv'). The same content  except each row is a comma-delimited list with a /n at the end., current difficult know princip keytab need creat host we present inform end user format easi consum the user run wizard may one creat keytab princip we expos capabl download csv file send appropri person may pars data creat script gener princip keytab manual display attach popup configur servic step done let show popup affect exist navig flow for gener content keytab path base user input princip name base user input name node host show nn http princip keytab path job tracker host show jt princip keytab path oozi server host show oozi http princip keytab path nagio server host show nagio princip keytab path h base master host show hbase princip keytab path hive server host show hive princip keytab path web h cat server host show http princip keytab path zoo keeper server host show zookeep princip keytab path data node host show dn princip keytab path task tracker host show tt princip keytab path region server host show hbase princip keytab path if duplic princip host display clickng download csv download csv file host princip keytab list csv the content except row comma delimit list n end,1,0,0,0,0,0,
2497,Remove dependence on dfs_datanode_http_address global for Nagios checks, remov depend df datanod http address global nagio check,ambari-agent/src/main/puppet/modules/hdp-nagios/templates/hadoop-services.cfg.erb shows the following:# HDFS::DATANODE Checksdefine service { hostgroup_name slaves use hadoop-service service_description DATANODE::DataNode process down servicegroups HDFS check_command check_tcp!&lt;%=scope.function_hdp_template_var('dfs_datanode_http_address')%&gt;!-w 1 -c 1 normal_check_interval 1 retry_check_interval 0.5 max_check_attempts 3}define service { hostgroup_name slaves use hadoop-service service_description DATANODE::DataNode storage full servicegroups HDFS check_command check_datanode_storage!&lt;%=scope.function_hdp_template_var('dfs_datanode_http_address')%&gt;!90%!90% normal_check_interval 5 retry_check_interval 1 max_check_attempts 2}We need to remove dependence on dfs_datanode_http_address and use the actual config property like:hdp_get_port_from_url($hdfs-site['dfs.datanode.http.address']), ambari agent src main puppet modul hdp nagio templat hadoop servic cfg erb show follow hdf datanod checksdefin servic hostgroup name slave use hadoop servic servic descript datanod data node process servicegroup hdf check command check tcp lt scope function hdp templat var df datanod http address gt w c normal check interv retri check interv max check attempt defin servic hostgroup name slave use hadoop servic servic descript datanod data node storag full servicegroup hdf check command check datanod storag lt scope function hdp templat var df datanod http address gt normal check interv retri check interv max check attempt we need remov depend df datanod http address use actual config properti like hdp get port url hdf site df datanod http address,0,0,0,0,0,0,
2498,Cleanup setup https flow, cleanup setup http flow,Expected flow:[root@localhost ~]# ambari-server setup-httpsUsing python /usr/bin/python2.6Setting up HTTPS properties...Do you want to configure HTTPS [y/n] (y)?SSL port (8443) ? Please enter path to Certificate: /some/path/on/my/host/server.crtPlease enter path to Private Key: /some/path/on/my/host/server.keyPlease enter password for Private Key:Importing and saving certificate...done.NOTE: Reset Ambari Server to apply changes ('ambari-server restart|stop|start')Ambari Server 'HTTPS setup' completed successfully. Exiting., expect flow root localhost ambari server setup http use python usr bin python set http properti do want configur http n ssl port pleas enter path certif path host server crt pleas enter path privat key path host server key pleas enter password privat key import save certif done note reset ambari server appli chang ambari server restart stop start ambari server http setup complet success exit,1,0,0,0,0,0,
2515,Cannot add property mapred.task.tracker.task-controller, cannot add properti mapr task tracker task control,,,0,0,0,1,0,0,
2517,Decommission data node not working in secure mode, decommiss data node work secur mode,Decommission datanode does not do kinit before refresh., decommiss datanod kinit refresh,1,0,0,0,0,0,
2519,Add download CSV action for security wizard, add download csv action secur wizard,We need CSV content which shows which principals  keytabs etc. end up on the various hosts. This will be useful in scripts or other tools which can create appropriate environment., we need csv content show princip keytab etc end variou host thi use script tool creat appropri environ,1,0,0,0,0,0,
2522,Zookeeper smoke test failing in secure cluster, zookeep smoke test fail secur cluster,Zookeeper smoke test failing in secure cluster, zookeep smoke test fail secur cluster,1,0,0,0,0,0,
2525,Add helpful message when not able to download jdk with setup options for the user to be able to specify the jdk., add help messag abl download jdk setup option user abl specifi jdk,Add helpful message when not able to download jdk with setup options for the user to be able to specify the jdk., add help messag abl download jdk setup option user abl specifi jdk,0,0,0,0,0,0,
2532,Incorrect permission on taskcontroller.cfg, incorrect permiss taskcontrol cfg,/etc/conf/hadoop/taskcontroller.cfgIn secure mode permissions are set to '400'., etc conf hadoop taskcontrol cfg in secur mode permiss set,0,0,0,1,0,0,
2534,Some memory configs are set to -1 in Ambari, some memori config set ambari,Some memory configs are set to -1 in ambari-mapred.cluster.reduce.memory.mb-mapred.jobtracker.maxtasks.per.job-mapred.cluster.max.reduce.memory.mb-mapred.cluster.map.memory.mb-mapred.job.map.memory.mb-mapred.job.reduce.memory.mb-mapred.cluster.max.map.memory.mbModify the stack definition to put default values as appropriate., some memori config set ambari mapr cluster reduc memori mb mapr jobtrack maxtask per job mapr cluster max reduc memori mb mapr cluster map memori mb mapr job map memori mb mapr job reduc memori mb mapr cluster max map memori mb modifi stack definit put default valu appropri,0,0,0,0,0,0,
2542,Custom Repo URL cannot be set when non-root, custom repo url cannot set non root,The first iteration for creating custom repo URL persisted the new URL to disk. This poses a problem when running Ambari as non-root because the file system is owned by root. Change the implementation to save the override in the metainfo table., the first iter creat custom repo url persist new url disk thi pose problem run ambari non root file system own root chang implement save overrid metainfo tabl,0,0,0,0,0,0,
2545,Regression: Agent external hostname is not verified during bootstrap with no warnings, regress agent extern hostnam verifi bootstrap warn,When confirming hosts using external addresses bootstrapping should be failed immediately and a warning should be logged. Right now this functionality is broken  neither warning in log nor failing immediately present, when confirm host use extern address bootstrap fail immedi warn log right function broken neither warn log fail immedi present,0,0,0,0,0,0,
2546,Simplify Local Repo setup in installer UI, simplifi local repo setup instal ui,Enhance the UI to use stacks API to give user a choice of stacks and be able to customize repository locations., enhanc ui use stack api give user choic stack abl custom repositori locat,0,0,0,0,0,0,
2555,Security Wizard: Create separate page for principal/keytab, secur wizard creat separ page princip keytab,Add step 'Create Principals and Keytabs' to Security wizard., add step creat princip keytab secur wizard,0,0,0,0,0,0,
2556,Ctrl+C during ambari-server setup prints out a python stack trace, ctrl c ambari server setup print python stack trace,We should be able to catch KeyBoardInterrupt in ambari-server main and print a useful message like:'Aborting ... Keyboard Interrupt.' and avoid the stack trace for the user., we abl catch key board interrupt ambari server main print use messag like abort keyboard interrupt avoid stack trace user,0,0,0,0,0,0,
2568,Setup LDAP does not validate true/false response, setup ldap valid true fals respons,Garbage responses to true/false questions just pass thru. Notice below  just put in garbage for Use SSL and that's what it would have written.Need validation to confirm they enter either 1) return to accept default or 2) the word true or 3) the word false. Else  inform the user'Property must be 'true' or 'false'.' and ask again.Secondary URL :Use SS &#91;true/false&#93; (false): asdUser name attribute* (uid):Base DN* :Property cannot be blank.Base DN* : asdBind anonymously* true/false (false):Manager DN* :asdEnter Manager Password*:Re-enter password:Passwords do not matchEnter Manager Password*:Re-enter password:====================Review Settings====================authentication.ldap.primaryUrl: my.url:849authentication.ldap.useSSL: asdauthentication.ldap.usernameAttribute: uid, garbag respons true fals question pass thru notic put garbag use ssl would written need valid confirm enter either return accept default word true word fals els inform user properti must true fals ask secondari url use ss true fals fals asd user name attribut uid base dn properti cannot blank base dn asd bind anonym true fals fals manag dn asd enter manag password re enter password password match enter manag password re enter password review set authent ldap primari url url authent ldap use ssl asdauthent ldap usernam attribut uid,0,0,0,1,0,0,
2578,Using another user for ambari server user create a local group for the ambari server user with same name., use anoth user ambari server user creat local group ambari server user name,Using an ambari-qa user (that is a ldap user and he has hadoop set up as a primary ldap group) [root@va21 ldap]# id ambari-qauid=524(ambari-qa) gid=522(hadoop) groups=522(hadoop)causes ambari-server setup to create an ambari-qa local group (at /etc/group). ambari-qa:x:601:ambari-qa[root@va21 ldap]# id ambari-qauid=524(ambari-qa) gid=522(hadoop) groups=522(hadoop) 601(ambari-qa)Ldap users and groups are transparent for ambari-server  it starts well.The problem is that additional group is created., use ambari qa user ldap user hadoop set primari ldap group root va ldap id ambari qauid ambari qa gid hadoop group hadoop caus ambari server setup creat ambari qa local group etc group ambari qa x ambari qa root va ldap id ambari qauid ambari qa gid hadoop group hadoop ambari qa ldap user group transpar ambari server start well the problem addit group creat,0,0,0,0,0,0,
2585,Host Check report show hosts without issues, host check report show host without issu,Report contains hosts  which don't have issues  but should show 'A space delimited list of hosts which have issues'., report contain host issu show a space delimit list host issu,0,0,0,0,0,0,
2590,JS Error when deleting a widget after sorting it on remove/edit sign, js error delet widget sort remov edit sign,,,0,0,0,0,0,0,
2594,HDP installation fails due to puppet syntax error, hdp instal fail due puppet syntax error,namenode_host is not an hash or array when accessing it with 0 at /var/lib/ambari-agent/puppet/modules/hdp/manifests/params.pp:70 on node host1., namenod host hash array access var lib ambari agent puppet modul hdp manifest param pp node host,0,0,0,0,0,0,
2595,Properties of the same name cannot be added to different custom site.xml's, properti name cannot ad differ custom site xml,Steps (Installer Wizard):Go to 'Customize Services' page.Select 'HDFS' tab.Add custom property 'xxx' to 'Custom core-site.xml' panel.Try add custom property 'xxx' to 'Custom hdfs-site.xml' panel.Result:Custom property can not be added to 'Custom hdfs-site.xml' panel (see attachment).Steps (Ambari monitoring UI):Go to 'Customize Services' page.Select 'HDFS' tab.Add custom property to 'Custom core-site.xml' panel (for example  'install-test-core-site').Continue and end hadoop installation.Go to 'Services' page.Select 'MapReduce' tab.Try add custom property 'install-test-core-site' to 'Custom mapred-site.xml' panel.Result:Custom property can not be added to 'Custom mapred-site.xml' panel (see attachment)., step instal wizard go custom servic page select hdf tab add custom properti xxx custom core site xml panel tri add custom properti xxx custom hdf site xml panel result custom properti ad custom hdf site xml panel see attach step ambari monitor ui go custom servic page select hdf tab add custom properti custom core site xml panel exampl instal test core site continu end hadoop instal go servic page select map reduc tab tri add custom properti instal test core site custom mapr site xml panel result custom properti ad custom mapr site xml panel see attach,0,0,0,0,0,0,
2600,Add Quick Links (Web UI) for Oozie  Hue  Nagios  Ganglia, add quick link web ui oozi hue nagio ganglia,There are no quick links for Oozie. Similarly  some other services also are missing the quick links., there quick link oozi similarli servic also miss quick link,0,0,0,0,0,0,
2605,'kdestroy' not required for zookeeper smoke test, kdestroy requir zookeep smoke test,zookeeper smoke test passes without having to 'kdestroy' on the user running the smoke test., zookeep smoke test pass without kdestroy user run smoke test,0,0,0,0,0,0,
2608,WebHCat and Oozie services does not start on RHEL5 with enabled security because of 'CRITICAL: Error doing kinit for nagios', web h cat oozi servic start rhel enabl secur critic error kinit nagio,FE only has support to provide single path for kinit. As Ambari supports mixed OS deployment it cannot be guaranteed that kinit exists at the same path on all nodes. FE should allow providing a set of look-up paths for kinit as well as the BE should support a set of default lookup paths., fe support provid singl path kinit as ambari support mix os deploy cannot guarante kinit exist path node fe allow provid set look path kinit well be support set default lookup path,1,0,0,0,0,0,
2612,Rename agent.fqdn property in ambari.props to server.fqdn, renam agent fqdn properti ambari prop server fqdn,lets just rename agent -&gt; server, let renam agent gt server,0,0,0,0,0,0,
2613,Host Checks: truncation on checked processes makes it difficult to know the actual processes in conflict, host check truncat check process make difficult know actual process conflict,Processes are truncated too short and can't really tell what's in conflict. Since there is a lot of space on the right (in fact  the hostname column is too far to the left compared to other sections)  we should display more characters (with hover tooltip showing full text)., process truncat short realli tell conflict sinc lot space right fact hostnam column far left compar section display charact hover tooltip show full text,0,0,0,0,0,0,
2614,Popover with config name goes beyond the container, popov config name goe beyond contain,See attachecd screenshot., see attachecd screenshot,0,0,0,0,0,0,
2619,Wrong info on Services > Summary tab for DataNodes Live  TaskTrackers Live  RegionServers live, wrong info servic summari tab data node live task tracker live region server live,1. Install cluster2. On the host with SNameNode  we also have a region server3. Stop snamenode component on that host4. Services &gt; hbase &gt; summary shows region server is not liveAlso after stopping DataNode or TaskTracker  component status changes are not reflected on Services &gt; summary tab, instal cluster on host s name node also region server stop snamenod compon host servic gt hbase gt summari show region server live also stop data node task tracker compon statu chang reflect servic gt summari tab,1,0,0,0,0,0,
2631,Host cleanup left two packages(ambari-log4j  libconfuse), host cleanup left two packag ambari log j libconfus,,,1,0,0,0,0,0,
2632,Dashboard Widgets: 'hover to show details' experience is jarring, dashboard widget hover show detail experi jar,,,1,0,0,0,0,0,
2633,Reset the latest stack version for 1.2.5, reset latest stack version,Reset the latest stack version for 1.2.5, reset latest stack version,1,0,0,0,0,0,
2635,Perf: Service summary view inefficiently binds to host components, perf servic summari view ineffici bind host compon,In ambari-web/app/views/main/service/info/summary.js#hostComponentsUpd()  is called per each hostComponent's host and master property change. On a 150 node cluster  we get like 300 calls just for this method.Due to this  service_mapper  which usually maps in 600ms  takes now 5.8s., in ambari web app view main servic info summari js host compon upd call per host compon host master properti chang on node cluster get like call method due servic mapper usual map ms take,1,0,0,0,1,0,
2636,Dashboard Metrics legend size increased unexpectedly on mouseover from line space, dashboard metric legend size increas unexpectedli mouseov line space,This happened in a very specific situation.1. Put mouse on the line space around a metric widget.2. hover on the widget with mouse down.Result:The legend show up as a strange bigger size., thi happen specif situat put mous line space around metric widget hover widget mous result the legend show strang bigger size,0,0,0,0,0,0,
2637,Security CSV cleanup, secur csv cleanup,Should say 'Hive Metastore and HiveServer2'  not just HiveServer2. Even though they are co-located master components  let's make it clear this principal is for both include keytab file column. In addition to the keytab full path column (/etc/security/keytabs/jt.service.keytab)  include a column with just the filename (jt.service.keytab). Easier to copy/paste/parse if you want to use the CSV file., should say hive metastor hive server hive server even though co locat master compon let make clear princip includ keytab file column in addit keytab full path column etc secur keytab jt servic keytab includ column filenam jt servic keytab easier copi past pars want use csv file,1,0,0,0,0,0,
2640,Going back to Customize Services page from the Install page resets certain directory values, go back custom servic page instal page reset certain directori valu,Steps to reproduce: Install using non-default directories Upon install failure  go back to Customize Services page from the left nav. Certain directories (NN dirs  SNN dir  DN dirs  Oozie Data Dir  ZK Dir  etc) are reverted back to the default. Other parameters are not reverted back., step reproduc instal use non default directori upon instal failur go back custom servic page left nav certain directori nn dir snn dir dn dir oozi data dir zk dir etc revert back default other paramet revert back,1,0,0,0,0,0,
2642,Update Ember-I18n, updat ember i n,,,1,0,0,0,0,0,
2643,Read timeout issues in Oracle JDBC connections where read has a long timeout, read timeout issu oracl jdbc connect read long timeout,Read timeout issues in Oracle JDBC connections where read has a long timeout. This happens when the read timeout is too long. In order to set appropriate timeout  add specially prefixed values in ambari.properties, read timeout issu oracl jdbc connect read long timeout thi happen read timeout long in order set appropri timeout add special prefix valu ambari properti,1,0,0,0,1,0,
2644,Ambari-server can not find password for remote database with password encryption enabled, ambari server find password remot databas password encrypt enabl,Performed cluster setup as proposed at E2E test scenario. ambari-server setupambari-server setup-ldapambari-server encrypt-passwordsambari-server setup-httpsambari-server startServer does not start. It complains about missing password file / db password alias19:03:36 249 INFO Configuration:300 - Generation of file with password19:03:37 320 INFO CredentialProvider:146 - action =&gt; PUT  alias =&gt; ambari.db.password19:03:37 885 INFO Configuration:313 - Reading password from existing file19:03:38 838 INFO CredentialProvider:146 - action =&gt; PUT  alias =&gt; ambari.ldap.manager.password19:12:02 925 INFO Configuration:313 - Reading password from existing file19:12:02 946 INFO Configuration:324 - API SSL Authentication is turned on.19:12:02 946 INFO Configuration:329 - Reading password from existing file19:12:02 948 INFO Configuration:481 - Hosts Mapping File null19:12:02 951 INFO HostsMap:60 - Using hostsmap file null19:12:04 467 INFO MasterKeyServiceImpl:209 - Loading from persistent master: #1.0# Fri  Jul 12 2013 19:03:34.71719:12:06 016 INFO AmbariServer:446 - Getting the controller19:12:11 146 INFO CertificateManager:68 - Initialization of root certificate19:12:11 147 INFO CertificateManager:70 - Certificate exists:false19:12:11 147 INFO CertificateManager:137 - Generation of server certificate19:12:16 383 INFO ShellCommandUtil:43 - Command openssl genrsa -des3 -passout pass:n15KV1q6aWRZIP86XAjpTdbTaKo0HHWIsTuaOPZQdxycChECKG -out /var/lib/ambari-server/keys/ca.key 4096 was finished with exit code: 0 - the operation was completely successfully.19:12:16 431 INFO ShellCommandUtil:43 - Command openssl req -passin pass:n15KV1q6aWRZIP86XAjpTdbTaKo0HHWIsTuaOPZQdxycChECKG -new -key /var/lib/ambari-server/keys/ca.key -out /var/lib/ambari-server/keys/ca.crt -batch was finished with exit code: 0 - the operation was completely successfully.19:12:16 483 INFO ShellCommandUtil:43 - Command openssl x509 -passin pass:n15KV1q6aWRZIP86XAjpTdbTaKo0HHWIsTuaOPZQdxycChECKG -req -days 365 -in /var/lib/ambari-server/keys/ca.crt -signkey /var/lib/ambari-server/keys/ca.key -out /var/lib/ambari-server/keys/ca.crt was finished with exit code: 0 - the operation was completely successfully.19:12:16 496 INFO ShellCommandUtil:43 - Command openssl pkcs12 -export -in /var/lib/ambari-server/keys/ca.crt -inkey /var/lib/ambari-server/keys/ca.key -certfile /var/lib/ambari-server/keys/ca.crt -out /var/lib/ambari-server/keys/keystore.p12 -password pass:n15KV1q6aWRZIP86XAjpTdbTaKo0HHWIsTuaOPZQdxycChECKG -passin pass:n15KV1q6aWRZIP86XAjpTdbTaKo0HHWIsTuaOPZQdxycChECKG was finished with exit code: 0 - the operation was completely successfully.19:12:16 883 INFO AmbariServer:123 - ********* Meta Info initialized **********19:12:16 896 INFO ClustersImpl:88 - Initializing the ClustersImpl19:12:17 115 ERROR Configuration:610 - Error reading from credential store.19:12:17 116 ERROR Configuration:616 - Cannot read password for alias = /etc/ambari-server/conf/password.dat19:12:17 117 ERROR AmbariServer:455 - Failed to run the Ambari Serverjava.lang.RuntimeException: Unable to read database password at org.apache.ambari.server.configuration.Configuration.readPasswordFromFile(Configuration.java:596) at org.apache.ambari.server.configuration.Configuration.getRcaDatabasePassword(Configuration.java:583) at org.apache.ambari.eventdb.webservice.WorkflowJsonService.setDBProperties(WorkflowJsonService.java:95) at org.apache.ambari.server.controller.AmbariServer.performStaticInjection(AmbariServer.java:437) at org.apache.ambari.server.controller.AmbariServer.run(AmbariServer.java:125) at org.apache.ambari.server.controller.AmbariServer.main(AmbariServer.java:452)Caused by: java.io.FileNotFoundException: File '/etc/ambari-server/conf/password.dat' does not exist at org.apache.commons.io.FileUtils.openInputStream(FileUtils.java:265) at org.apache.commons.io.FileUtils.readFileToString(FileUtils.java:1457) at org.apache.commons.io.FileUtils.readFileToString(FileUtils.java:1475) at org.apache.ambari.server.configuration.Configuration.readPasswordFromFile(Configuration.java:594) ... 5 more19:12:17 118 ERROR AmbariServer:420 - Error stopping the serverjava.lang.NullPointerException at org.apache.ambari.server.controller.AmbariServer.stop(AmbariServer.java:418) at org.apache.ambari.server.controller.AmbariServer.main(AmbariServer.java:457)Content of ambari.properties:server.jdbc.rca.driver=oracle.jdbc.driver.OracleDriverauthentication.ldap.managerDn=uid=hdfs ou=people ou=dev dc=apache dc=orgauthentication.ldap.primaryUrl=localhost:389server.jdbc.rca.url=jdbc:oracle:thin:@ip-10-34-79-165.ec2.internal:1521/XEserver.connection.max.idle.millis=900000server.jdbc.port=1521server.version.file=/var/lib/ambari-server/resources/versionserver.jdbc.rca.user.passwd=/etc/ambari-server/conf/password.datapi.authenticate=truejce_policy.url=http://public-repo-1.hortonworks.com/ARTIFACTS/jce_policy-6.zipserver.persistence.type=remoteclient.api.ssl.key_name=https.keyauthentication.ldap.useSSL=falseambari-server.user=ambar-serverclient.api.ssl.port=8443authentication.ldap.usernameAttribute=uidserver.jdbc.user.name=ambariserver.jdbc.schema=XEjava.home=/usr/jdk64/jdk1.6.0_31server.os_type=redhat6api.ssl=truebootstrap.script=/usr/lib/python2.6/site-packages/ambari_server/bootstrap.pyclient.api.ssl.cert_name=https.crtauthentication.ldap.bindAnonymously=falseclient.security=ldapserver.jdbc.hostname=ip-10-34-79-165.ec2.internalresources.dir=/var/lib/ambari-server/resourcessecurity.passwords.encryption.enabled=truebootstrap.setup_agent.script=/usr/lib/python2.6/site-packages/ambari_server/setupAgent.pyserver.jdbc.driver=oracle.jdbc.driver.OracleDriverjdk.url=http://public-repo-1.hortonworks.com/ARTIFACTS/jdk-6u31-linux-x64.binsecurity.server.keys_dir=/var/lib/ambari-server/keysserver.jdbc.rca.user.name=ambariwebapp.dir=/usr/lib/ambari-server/webmetadata.path=/var/lib/ambari-server/resources/stacksserver.jdbc.url=jdbc:oracle:thin:@ip-10-34-79-165.ec2.internal:1521/XEserver.fqdn.service.url=http://169.254.169.254/latest/meta-data/public-hostnamebootstrap.dir=/var/run/ambari-server/bootstrapauthentication.ldap.baseDn=dc=apache dc=orgserver.jdbc.user.passwd=${alias=ambari.db.password}authentication.ldap.managerPassword=${alias=ambari.ldap.manager.password}server.jdbc.database=oraclesecurity.server.two_way_ssl=trueFile /etc/ambari-server/conf/password.dat is missingSetup flow:[root@ip-10-116-65-200 kerb]# ambari-server setupUsing python /usr/bin/python2.6Initializing...Setup ambari-serverChecking SELinux...SELinux status is 'enabled'SELinux mode is 'enforcing'Temporarily disabling SELinuxWARNING: SELinux is set to 'permissive' mode and temporarily disabled.OK to continue [y/n] (y)? yCustomize user account for ambari-server daemon [y/n] (n)? yEnter user account for ambari-server daemon (root):ambar-serverAdjusting ambari-server permissions and ownership...Checking iptables...iptables is disabled now. please reenable later.Checking JDK...Downloading JDK from http://public-repo-1.hortonworks.com/ARTIFACTS/jdk-6u31-linux-x64.bin to /var/lib/ambari-server/resources/jdk-6u31-linux-x64.binJDK distribution size is 85581913 bytesjdk-6u31-linux-x64.bin... 100% (81.6 MB of 81.6 MB)Successfully downloaded JDK distribution to /var/lib/ambari-server/resources/jdk-6u31-linux-x64.binTo install the Oracle JDK you must accept the license terms found at http://www.oracle.com/technetwork/java/javase/downloads/jdk-6u21-license-159167.txt. Not accepting will cancel the Ambari Server setup.Do you accept the Oracle Binary Code License Agreement [y/n] (y)? Installing JDK to /usr/jdk64Successfully installed JDK to /usr/jdk64/jdk1.6.0_31Downloading JCE Policy archive from http://public-repo-1.hortonworks.com/ARTIFACTS/jce_policy-6.zip to /var/lib/ambari-server/resources/jce_policy-6.zipSuccessfully downloaded JCE Policy archive to /var/lib/ambari-server/resources/jce_policy-6.zipCompleting setup...Configuring database...Enter advanced database configuration [y/n] (n)? ySelect database:1 - PostgreSQL (Embedded)2 - Oracle[1]:2Hostname [localhost]:ip-10-34-79-165.ec2.internalPort [1521]:Select Oracle identifier type:1 - Service Name2 - SID[1]:XEInvalid number.Select Oracle identifier type:1 - Service Name2 - SID[1]:1Service Name [ambari]:XEUsername [ambari]: Enter Database Password [bigdata]: WARNING: Before starting Ambari Server  you must copy the Oracle JDBC driver JAR file to /usr/share/java.Press &lt;enter&gt; to continue.Copying JDBC drivers to server resources...Configuring remote database connection properties...WARNING: Cannot find oracle sqlplus client in the path to load the Ambari Server schema. Before starting Ambari Server  you must run the following DDL against the database to create the schema sqlplus ambari/bigdata &lt; /var/lib/ambari-server/resources/Ambari-DDL-Oracle-CREATE.sql Press &lt;enter&gt; to continue.WARNING: The cli was not foundAmbari Server 'setup' completed with warnings.[root@ip-10-116-65-200 kerb]# less /etc/passwd, perform cluster setup propos e e test scenario ambari server setupambari server setup ldapambari server encrypt passwordsambari server setup httpsambari server start server start it complain miss password file db password alia info configur gener file password info credenti provid action gt put alia gt ambari db password info configur read password exist file info credenti provid action gt put alia gt ambari ldap manag password info configur read password exist file info configur api ssl authent turn info configur read password exist file info configur host map file null info host map use hostsmap file null info master key servic impl load persist master fri jul info ambari server get control info certif manag initi root certif info certif manag certif exist fals info certif manag gener server certif info shell command util command openssl genrsa de passout pass n kv q wrzip x ajp tdb ta ko hhw is tua opz qdxyc ch eckg var lib ambari server key ca key finish exit code oper complet success info shell command util command openssl req passin pass n kv q wrzip x ajp tdb ta ko hhw is tua opz qdxyc ch eckg new key var lib ambari server key ca key var lib ambari server key ca crt batch finish exit code oper complet success info shell command util command openssl x passin pass n kv q wrzip x ajp tdb ta ko hhw is tua opz qdxyc ch eckg req day var lib ambari server key ca crt signkey var lib ambari server key ca key var lib ambari server key ca crt finish exit code oper complet success info shell command util command openssl pkc export var lib ambari server key ca crt inkey var lib ambari server key ca key certfil var lib ambari server key ca crt var lib ambari server key keystor p password pass n kv q wrzip x ajp tdb ta ko hhw is tua opz qdxyc ch eckg passin pass n kv q wrzip x ajp tdb ta ko hhw is tua opz qdxyc ch eckg finish exit code oper complet success info ambari server meta info initi info cluster impl initi cluster impl error configur error read credenti store error configur cannot read password alia etc ambari server conf password dat error ambari server fail run ambari serverjava lang runtim except unabl read databas password org apach ambari server configur configur read password from file configur java org apach ambari server configur configur get rca databas password configur java org apach ambari eventdb webservic workflow json servic set db properti workflow json servic java org apach ambari server control ambari server perform static inject ambari server java org apach ambari server control ambari server run ambari server java org apach ambari server control ambari server main ambari server java caus java io file not found except file etc ambari server conf password dat exist org apach common io file util open input stream file util java org apach common io file util read file to string file util java org apach common io file util read file to string file util java org apach ambari server configur configur read password from file configur java error ambari server error stop serverjava lang null pointer except org apach ambari server control ambari server stop ambari server java org apach ambari server control ambari server main ambari server java content ambari properti server jdbc rca driver oracl jdbc driver oracl driverauthent ldap manag dn uid hdf ou peopl ou dev dc apach dc orgauthent ldap primari url localhost server jdbc rca url jdbc oracl thin ip ec intern x eserv connect max idl milli server jdbc port server version file var lib ambari server resourc versionserv jdbc rca user passwd etc ambari server conf password datapi authent truejc polici url http public repo hortonwork com artifact jce polici zipserv persist type remotecli api ssl key name http keyauthent ldap use ssl falseambari server user ambar servercli api ssl port authent ldap usernam attribut uidserv jdbc user name ambariserv jdbc schema x ejava home usr jdk jdk server os type redhat api ssl truebootstrap script usr lib python site packag ambari server bootstrap pyclient api ssl cert name http crtauthent ldap bind anonym falsecli secur ldapserv jdbc hostnam ip ec internalresourc dir var lib ambari server resourcessecur password encrypt enabl truebootstrap setup agent script usr lib python site packag ambari server setup agent pyserv jdbc driver oracl jdbc driver oracl driverjdk url http public repo hortonwork com artifact jdk u linux x binsecur server key dir var lib ambari server keysserv jdbc rca user name ambariwebapp dir usr lib ambari server webmetadata path var lib ambari server resourc stacksserv jdbc url jdbc oracl thin ip ec intern x eserv fqdn servic url http latest meta data public hostnamebootstrap dir var run ambari server bootstrapauthent ldap base dn dc apach dc orgserv jdbc user passwd alia ambari db password authent ldap manag password alia ambari ldap manag password server jdbc databas oraclesecur server two way ssl true file etc ambari server conf password dat miss setup flow root ip kerb ambari server setup use python usr bin python initi setup ambari server check se linux se linux statu enabl se linux mode enforc temporarili disabl se linux warn se linux set permiss mode temporarili disabl ok continu n custom user account ambari server daemon n n enter user account ambari server daemon root ambar server adjust ambari server permiss ownership check iptabl iptabl disabl pleas reenabl later check jdk download jdk http public repo hortonwork com artifact jdk u linux x bin var lib ambari server resourc jdk u linux x bin jdk distribut size bytesjdk u linux x bin mb mb success download jdk distribut var lib ambari server resourc jdk u linux x bin to instal oracl jdk must accept licens term found http www oracl com technetwork java javas download jdk u licens txt not accept cancel ambari server setup do accept oracl binari code licens agreement n instal jdk usr jdk success instal jdk usr jdk jdk download jce polici archiv http public repo hortonwork com artifact jce polici zip var lib ambari server resourc jce polici zip success download jce polici archiv var lib ambari server resourc jce polici zip complet setup configur databas enter advanc databas configur n n select databas postgr sql embed oracl hostnam localhost ip ec intern port select oracl identifi type servic name sid xe invalid number select oracl identifi type servic name sid servic name ambari xe usernam ambari enter databas password bigdata warn befor start ambari server must copi oracl jdbc driver jar file usr share java press lt enter gt continu copi jdbc driver server resourc configur remot databas connect properti warn cannot find oracl sqlplu client path load ambari server schema befor start ambari server must run follow ddl databas creat schema sqlplu ambari bigdata lt var lib ambari server resourc ambari ddl oracl creat sql press lt enter gt continu warn the cli found ambari server setup complet warn root ip kerb less etc passwd,0,0,0,0,0,0,
2646,Improve styles for HostCleanup code area, improv style host cleanup code area,,,0,0,0,0,0,0,
2653,Add umask checks for host checks - we should alert if umask is not 022., add umask check host check alert umask,Add umask checks for host checks - we should alert if umask is not 022., add umask check host check alert umask,1,0,0,0,0,0,
2657,Add a re-type new password field when changing passwords for ambari users, add type new password field chang password ambari user,We should add a third text box to re-type the new password while changing the password for an ambari user., we add third text box type new password chang password ambari user,1,0,0,0,0,0,
2660,Host checks say pass when all hosts failed to register, host check say pass host fail regist,When all hosts fail to register  there are no warnings  and hence we show OK for host checks., when host fail regist warn henc show ok host check,1,0,0,0,0,0,
2661,Security wizard: Relogin while on step3 without quitting the wizard throws JS error., secur wizard relogin step without quit wizard throw js error,Steps to reproduce: Go to step-3 (Generate principals and keytabs) of Enable security wizard. Restart Amabri server. Refresh on step-3. ui will take you to login page. Entering correct credentials  user will be navigated again to step-3 of security wizard. At this point JS error is encountered., step reproduc go step gener princip keytab enabl secur wizard restart amabri server refresh step ui take login page enter correct credenti user navig step secur wizard at point js error encount,1,0,0,0,0,0,
2670,Start button not available for various components within 10 sec after stop operation finishes, start button avail variou compon within sec stop oper finish,Steps to reproduce: 1. Stop a component. 2. Wait for the corresponding BG operation to finish. Result: 'Start' button isn't available for the component within 10 seconds since stop operation finished in UI. It appears later.This happens cuz of update interval  it is set to 15 seconds (App.contentUpdateInterval)  that's why in some moments it can take up to 15 sec to update components status  after request is done.Solution: Create a seperate update interval specialy for updating host components. In config.js we even have App.componentsUpdateInterval = 6000; But this value was not used anywhere in code till now., step reproduc stop compon wait correspond bg oper finish result start button avail compon within second sinc stop oper finish ui it appear later thi happen cuz updat interv set second app content updat interv moment take sec updat compon statu request done solut creat seper updat interv speciali updat host compon in config js even app compon updat interv but valu use anywher code till,1,0,0,0,1,0,
2681,setup ldap does not validate secondary url, setup ldap valid secondari url,setup ldap does not validate secondary url. It should validate the input (when entered) the same way as the primary., setup ldap valid secondari url it valid input enter way primari,0,0,0,0,0,0,
2688,Error messages printed to log, error messag print log,Notice in the ambari-server snippet below a lot of these messages:ERROR Configuration:616 - Cannot read password for alias = nullSteps to reproduce:Setup serverSetup encrypt passwords  don't persist the keySetup httpsStart the server  provided master keyDo cluster install, notic ambari server snippet lot messag error configur cannot read password alia null step reproduc setup server setup encrypt password persist key setup http start server provid master key do cluster instal,0,0,0,0,0,0,
2689,Enable Security Wizard stops on step '2. Save Configurations' and doesn't let the user leave the wizard, enabl secur wizard stop step save configur let user leav wizard,,,0,0,0,0,0,0,
2690,Datanode Live widget displays 0 dead when no datanode is live on a cluster., datanod live widget display dead datanod live cluster,,,1,0,0,0,0,0,
2692,Disable Show report for 0 issues, disabl show report issu,Report on Host Checks for 0 isses looks like:####################################### Host Checks Report## Generated: Tue Jul 09 2013 13:11:14 GMT+0300 (FLE Daylight Time)############################################################################# Hosts## A space delimited list of hosts which have issues.# Provided so that administrators can easily copy hostnames into scripts  email etc.######################################HOSTSShow Report button should be disabled for 0 issues or at least do not show HOSTS section in report., report host check iss look like host check report gener tue jul gmt fle daylight time host a space delimit list host issu provid administr easili copi hostnam script email etc host show report button disabl issu least show host section report,0,0,0,0,0,0,
2697,Disable security not working in web-ui testMode., disabl secur work web ui test mode,,,0,0,0,1,0,0,
2698,Host specific progress bar for a task has some inconsistencies, host specif progress bar task inconsist,See the attached images. The third image added is a summary of what was going on., see attach imag the third imag ad summari go,1,0,0,0,0,0,
2701,Implement a cleanup thread that removes files in ambari-agent data directory that are older than a configurable amount of time, implement cleanup thread remov file ambari agent data directori older configur amount time,Implement a cleanup thread that removes files in ambari-agent data directory that are older than a month or so(must be configurable).It's required  because the directory will grow unbounded if it's not cleaned up., implement cleanup thread remov file ambari agent data directori older month must configur it requir directori grow unbound clean,1,0,0,0,0,0,
2707,Fix JS Unit tests after merge 1.4.0 to trunk, fix js unit test merg trunk,,,1,0,0,0,0,0,
2708,Make ambari web testMode work for installer wizard with HDP stack-2 selection., make ambari web test mode work instal wizard hdp stack select,Installer wizard with HDP stack-2 selection in test mode, instal wizard hdp stack select test mode,0,0,0,0,0,0,
2716,Disable autocomplete on form tag for Ambari UI., disabl autocomplet form tag ambari ui,,,1,0,0,0,0,0,
2723,hbase super user cannot submit jobs since Ambari creates hbase super user with uid<1000, hbase super user cannot submit job sinc ambari creat hbase super user uid,Copytable jobs need to be submitted as the hbase super user however the uid for hbase super user created by Ambari has uid &lt; 1000, copyt job need submit hbase super user howev uid hbase super user creat ambari uid lt,1,0,0,0,0,0,
2727,Disallow actions upon host components on hosts that stopped heartbeating, disallow action upon host compon host stop heartbeat,On a host that stopped heartbeating  the UI shows actions to perform on host components. However  upon executing an action  the backend does not create any tasks and returns 200. The UI doesn't do anything in this case. Instead  the UI should disable action buttons in this case., on host stop heartbeat ui show action perform host compon howev upon execut action backend creat task return the ui anyth case instead ui disabl action button case,1,0,0,0,0,0,
2729,While a host component is being installed (INSTALLING state)  it does not show up in the Host Detail page, while host compon instal instal state show host detail page,Say Add Hosts wizard fails to add a host and the host components are in INSTALL_FAILED state. In this case  the UI displays the host component with a red gear and shows the action menu with the current state 'Install Failed' and the action 'Re-Install'. Once you invoke 'Re-Install'  the host component disappears from the UI. Once the host component finishes installing  it magically appears again., say add host wizard fail add host host compon instal fail state in case ui display host compon red gear show action menu current state instal fail action re instal onc invok re instal host compon disappear ui onc host compon finish instal magic appear,1,0,0,0,0,0,
2733,Hosts and Host Details page UI tweaks, host host detail page ui tweak,1. Hosts &gt; Design around full hostname being displayed. Consider being able to show 40 characters and then truncate &gt; 40 chars 2. Hosts &gt; Show # control should persist when navigating around app  and after logout/login3. Hosts &gt; Make Components list an expand/collapse control instead of a list with abbreviations4. Host Details &gt; Move Components area above Summary area since the Component Controls are used very often (much more often than viewing the Summary area info)., host gt design around full hostnam display consid abl show charact truncat gt char host gt show control persist navig around app logout login host gt make compon list expand collaps control instead list abbrevi host detail gt move compon area summari area sinc compon control use often much often view summari area info,1,0,0,0,0,0,
2748,Misc logging changes, misc log chang,Additional logs:  When a task is timed-out/failed API requests to update component and component hosts   Remove logs  Do not log ganglia population time when its less than 5 second, addit log when task time fail api request updat compon compon host remov log do log ganglia popul time less second,1,0,0,0,0,0,
2753,Security Wizard step 4: no hosts shown when clicking on the 'Start Services'/'Stop Services' link, secur wizard step host shown click start servic stop servic link,Proceed to step 4 of security wizard  click on 'Start Services' or 'Stop Services' link.Result: popup window is shown with empty contentThis bug was happening due to js error 'Uncaught TypeError: Cannot call method 'filterProperty' of null' in setBackgroundOperationHeader: functionAfter background operation (host popup) popup was optimized for better peformace on large cluster this error showed up. New function on setting popup header  did not account that this popup (HostPopup) is used not only in BG operations  but also in security wizard., proce step secur wizard click start servic stop servic link result popup window shown empti content thi bug happen due js error uncaught type error cannot call method filter properti null set background oper header function after background oper host popup popup optim better peformac larg cluster error show new function set popup header account popup host popup use bg oper also secur wizard,1,0,0,0,0,0,
2758,Jobs page: table is not striped and pagination is not disabled, job page tabl stripe pagin disabl,To reproduce go to the Jobs page from top menu (do not open .../main/apps directly). Table is not striped and pagination buttons are enabled even if there is only one page., to reproduc go job page top menu open main app directli tabl stripe pagin button enabl even one page,1,0,0,0,0,0,
2760,Stack 2.0.3  Hive Check execute fail, stack hive check execut fail,When installing Hadoop 2 stack  Hive service check fails to run., when instal hadoop stack hive servic check fail run,1,0,0,0,0,0,
2761,Customize Services page - Misc tab: incorrect behavior of popup window for changing user names, custom servic page misc tab incorrect behavior popup window chang user name,In firefoxSteps:Go to 'Customize Services' page.Select 'Misc' tab.Change username for HDFS  HBase or Group (do not move focus to other elements).Click 'Next' button.Result:Browser was switched to 'Review' page.Was opened popup window for changing properties depended with user names. User must to refresh the page for popup menu disappearing., in firefox step go custom servic page select misc tab chang usernam hdf h base group move focu element click next button result browser switch review page wa open popup window chang properti depend user name user must refresh page popup menu disappear,1,0,0,0,0,0,
2763,Ozzie does not work with local FS user, ozzi work local fs user,Running jobs as a local FS user does not work work 2.0.* stack because of permissions on /tmp/hadoop-yarn/staging  which is the default staging dir., run job local fs user work work stack permiss tmp hadoop yarn stage default stage dir,1,0,0,0,1,0,
2768,Host Checks > Show Report is showing bogus information for FILES AND FOLDERS, host check show report show bogu inform file and folder,Host Checks popup is showing that /usr/lib/hadoop already exists.When I clicked on 'Show Reports'  it is showing '/usr/lib/hadoop/ /folder'. This doesn't make much sense. FILES AND FOLDERS/usr/lib/hadoop/ /folderThe API is showing: 'stackFoldersAndFiles' : [ { 'name' : '/usr/lib/hadoop'  'type' : 'directory' }, host check popup show usr lib hadoop alreadi exist when i click show report show usr lib hadoop folder thi make much sens file and folder usr lib hadoop folder the api show stack folder and file name usr lib hadoop type directori,0,0,0,0,0,0,
2769,FE should show the error-details when it encounters error while persisting web client state, fe show error detail encount error persist web client state,Error which occurred while requesting cluster status is not informative.Change url in request for cluster state  go to admin page and try to enable security., error occur request cluster statu inform chang url request cluster state go admin page tri enabl secur,1,0,0,1,0,0,
2777,Cannot save HDFS configs with SNN in MAINTENANCE mode, cannot save hdf config snn mainten mode,,,1,0,0,0,0,0,
2782,Hadoop2 stack install should merge YARN MR2 options, hadoop stack instal merg yarn mr option,YARN in current Hadoop 2 stack has only MR2 as application. Since it does not make sense to install YARN without a default application  and MR2 cannot be installed by itself  we should combine both into a single install option (see screenshot)., yarn current hadoop stack mr applic sinc make sens instal yarn without default applic mr cannot instal combin singl instal option see screenshot,1,0,0,0,0,0,
2786,YARN time series data needed for NodeManager statuses, yarn time seri data need node manag status,We need API call for a graph which will show NodeManager status counts. NodeManagers can be in the following states: active  lost  unhealthy  rebooted  and decommissioned., we need api call graph show node manag statu count node manag follow state activ lost unhealthi reboot decommiss,1,0,0,0,0,0,
2799,YARN service summary additional information, yarn servic summari addit inform,We need to show below 2 additional information Across cluster memory - used/reserved/total Queues information if available, we need show addit inform across cluster memori use reserv total queue inform avail,1,0,0,0,0,0,
2808,Can't add from UI some queues in capacity-scheduler.xml, can add ui queue capac schedul xml,Properties of capacity-scheduler.xml are truncated by ' '. It make impossible to create multiple queues  please see http://hadoop.apache.org/docs/current/hadoop-yarn/hadoop-yarn-site/CapacityScheduler.html, properti capac schedul xml truncat it make imposs creat multipl queue pleas see http hadoop apach org doc current hadoop yarn hadoop yarn site capac schedul html,1,0,0,0,0,0,
2816,Customize Services: directories are shown in comma-delimited format when revisiting, custom servic directori shown comma delimit format revisit,In the Install Wizard  Customize Services page shows multiple directories delimited by newlines.However  when revisiting the page (go back to Customize Services from the Review page  for example)  the directories are shown in comma-delimited format. We should always show the directories in newline-delimited format. Note that when we actually store the value  the directories are comma-delimited (which is correct)., in instal wizard custom servic page show multipl directori delimit newlin howev revisit page go back custom servic review page exampl directori shown comma delimit format we alway show directori newlin delimit format note actual store valu directori comma delimit correct,1,0,0,0,0,0,
2829,Dashboard refactor and Unit tests, dashboard refactor unit test,,,1,0,0,0,0,0,
2834,Utility script to generate keytabs is broken, util script gener keytab broken,keytab Tar for each host is packaged including hostname. Untaring it on a host creates path starting with &lt;hostanme&gt;/&lt;actual path&gt;. Fix is to package the content inside the hostname directory excluding the hostname directory itself., keytab tar host packag includ hostnam untar host creat path start lt hostanm gt lt actual path gt fix packag content insid hostnam directori exclud hostnam directori,0,0,0,0,0,0,
2836,HBase 0.95.2 - Logger doesn't work, h base logger work,Installing 0.95.2 from internal repo 2.0.5. hbase-daemon.sh defines a default logger of INFO RFA{{  when log4j.properties uses {{INFO DRFA. They should match  as startup generates an error and does not continue.We should stop over writing the log4j properties for hbase. This will allow for hbase log4j properties to be in sync with those that come with the rpms., instal intern repo hbase daemon sh defin default logger info rfa log j properti use info drfa they match startup gener error continu we stop write log j properti hbase thi allow hbase log j properti sync come rpm,1,0,0,0,0,0,
2838,Running Requests are not visibile on the UI since the API is not returning the running requests., run request visibil ui sinc api return run request,The issue is the following:getRequestsByTaskStatus behaves correctly and returns the latest N requests. Note that the N requests have M (M &gt;&gt; N) tasks.Then the call to findByRequestIds gets oldest N tasks where the requestId for tasks belong to the list returned by the first call. So instead of getting M tasks we only get N tasks that too N oldest tasks which are returned. As a result the call never returns that latest request/tasks. The fix is to drop the filter done by the calls findByRequestIds and findByRequestAndTaskIds. Filter should only be applied on the number of requests to be returned., the issu follow get request by task statu behav correctli return latest n request note n request m m gt gt n task then call find by request id get oldest n task request id task belong list return first call so instead get m task get n task n oldest task return as result call never return latest request task the fix drop filter done call find by request id find by request and task id filter appli number request return,0,0,0,0,0,0,
2840,YARN and ZK data directory names have ' ' at end, yarn zk data directori name end,Installed the Hadoop2 stack and upon finishing the zk_data_dir in global  and yarn.nodemanager.local-dirs in yarn-site have the folder names suffixed with ' ' in API. Consequently  the folder names on system end up with a ' ' at end. Ex: /hadoop/yarn  and /hadoop/zookeeper ., instal hadoop stack upon finish zk data dir global yarn nodemanag local dir yarn site folder name suffix api consequ folder name system end end ex hadoop yarn hadoop zookeep,1,0,0,0,0,0,
2847,Restart service component fails if pid is reallocated, restart servic compon fail pid realloc,1. Stop secondary namenode.2. Edit the pid file  default location = /var/run/hadoop/hdfs/hadoop-hdfs-secondarynamenode.pid3. Change the pid to any other process pid that is currently running.4. Start secondary namenode.Outcome:Secondary namenode start command succeeds but secondary namenode does not start. (indicated by live status of the component)., stop secondari namenod edit pid file default locat var run hadoop hdf hadoop hdf secondarynamenod pid chang pid process pid current run start secondari namenod outcom secondari namenod start command succe secondari namenod start indic live statu compon,1,0,0,0,0,0,
2858,YARN time series data needed for AllocatedContainers, yarn time seri data need alloc contain,API call for a graph which will show time series for YARN Allocated containers., api call graph show time seri yarn alloc contain,1,0,0,0,0,0,
2864,Host registration fails, host registr fail,Host registration fails with:INFO 2013-08-10 01:50:49 923 Controller.py:99 - Unable to connect to: https://c6401.ambari.apache.org:8441/agent/v1/register/c6403.ambari.apache.orgTraceback (most recent call last): File '/usr/lib/python2.6/site-packages/ambari_agent/Controller.py'  line 80  in registerWithServer ret = json.loads(response) File '/usr/lib64/python2.6/json/__init__.py'  line 307  in loads return _default_decoder.decode(s) File '/usr/lib64/python2.6/json/decoder.py'  line 319  in decode obj  end = self.raw_decode(s  idx=_w(s  0).end()) File '/usr/lib64/python2.6/json/decoder.py'  line 338  in raw_decode raise ValueError('No JSON object could be decoded')ValueError: No JSON object could be decodedambari-server.log is showing:01:55:22 732 WARN [qtp967966535-50] ServletHandler:514 - /agent/v1/register/c6401.ambari.apache.orgcom.google.gson.JsonSyntaxException: java.lang.IllegalStateException: Expected a string but was BEGIN_OBJECT at line 1 column 3680 at com.google.gson.internal.bind.ReflectiveTypeAdapterFactory$Adapter.read(ReflectiveTypeAdapterFactory.java:176) at com.google.gson.internal.bind.TypeAdapterRuntimeTypeWrapper.read(TypeAdapterRuntimeTypeWrapper.java:40) at com.google.gson.internal.bind.ArrayTypeAdapter.read(ArrayTypeAdapter.java:72) at com.google.gson.internal.bind.ReflectiveTypeAdapterFactory$1.read(ReflectiveTypeAdapterFactory.java:93) at com.google.gson.internal.bind.ReflectiveTypeAdapterFactory$Adapter.read(ReflectiveTypeAdapterFactory.java:172) at com.google.gson.internal.bind.ReflectiveTypeAdapterFactory$1.read(ReflectiveTypeAdapterFactory.java:93) at com.google.gson.internal.bind.ReflectiveTypeAdapterFactory$Adapter.read(ReflectiveTypeAdapterFactory.java:172) at com.google.gson.internal.bind.ReflectiveTypeAdapterFactory$1.read(ReflectiveTypeAdapterFactory.java:93) at com.google.gson.internal.bind.ReflectiveTypeAdapterFactory$Adapter.read(ReflectiveTypeAdapterFactory.java:172) at com.google.gson.Gson.fromJson(Gson.java:795) at com.google.gson.Gson.fromJson(Gson.java:761) at org.apache.ambari.server.api.GsonJsonProvider.readFrom(GsonJsonProvider.java:60) at com.sun.jersey.spi.container.ContainerRequest.getEntity(ContainerRequest.java:474) at com.sun.jersey.server.impl.model.method.dispatch.EntityParamDispatchProvider$EntityInjectable.getValue(EntityParamDispatchProvider.java:123) at com.sun.jersey.server.impl.inject.InjectableValuesProvider.getInjectableValues(InjectableValuesProvider.java:46) at com.sun.jersey.server.impl.model.method.dispatch.AbstractResourceMethodDispatchProvider$EntityParamInInvoker.getParams(AbstractResourceMethodDispatchProvider.java:153) at com.sun.jersey.server.impl.model.method.dispatch.AbstractResourceMethodDispatchProvider$TypeOutInvoker._dispatch(AbstractResourceMethodDispatchProvider.java:183) at com.sun.jersey.server.impl.model.method.dispatch.ResourceJavaMethodDispatcher.dispatch(ResourceJavaMethodDispatcher.java:75) at com.sun.jersey.server.impl.uri.rules.HttpMethodRule.accept(HttpMethodRule.java:288) at com.sun.jersey.server.impl.uri.rules.RightHandPathRule.accept(RightHandPathRule.java:147) at com.sun.jersey.server.impl.uri.rules.ResourceClassRule.accept(ResourceClassRule.java:108) at com.sun.jersey.server.impl.uri.rules.RightHandPathRule.accept(RightHandPathRule.java:147) at com.sun.jersey.server.impl.uri.rules.RootResourceClassesRule.accept(RootResourceClassesRule.java:84) at com.sun.jersey.server.impl.application.WebApplicationImpl._handleRequest(WebApplicationImpl.java:1469), host registr fail info control py unabl connect http c ambari apach org agent v regist c ambari apach org traceback recent call last file usr lib python site packag ambari agent control py line regist with server ret json load respons file usr lib python json init py line load return default decod decod file usr lib python json decod py line decod obj end self raw decod idx w end file usr lib python json decod py line raw decod rais valu error no json object could decod valu error no json object could decodedambari server log show warn qtp servlet handler agent v regist c ambari apach orgcom googl gson json syntax except java lang illeg state except expect string begin object line column com googl gson intern bind reflect type adapt factori adapt read reflect type adapt factori java com googl gson intern bind type adapt runtim type wrapper read type adapt runtim type wrapper java com googl gson intern bind array type adapt read array type adapt java com googl gson intern bind reflect type adapt factori read reflect type adapt factori java com googl gson intern bind reflect type adapt factori adapt read reflect type adapt factori java com googl gson intern bind reflect type adapt factori read reflect type adapt factori java com googl gson intern bind reflect type adapt factori adapt read reflect type adapt factori java com googl gson intern bind reflect type adapt factori read reflect type adapt factori java com googl gson intern bind reflect type adapt factori adapt read reflect type adapt factori java com googl gson gson json gson java com googl gson gson json gson java org apach ambari server api gson json provid read from gson json provid java com sun jersey spi contain contain request get entiti contain request java com sun jersey server impl model method dispatch entiti param dispatch provid entiti inject get valu entiti param dispatch provid java com sun jersey server impl inject inject valu provid get inject valu inject valu provid java com sun jersey server impl model method dispatch abstract resourc method dispatch provid entiti param in invok get param abstract resourc method dispatch provid java com sun jersey server impl model method dispatch abstract resourc method dispatch provid type out invok dispatch abstract resourc method dispatch provid java com sun jersey server impl model method dispatch resourc java method dispatch dispatch resourc java method dispatch java com sun jersey server impl uri rule http method rule accept http method rule java com sun jersey server impl uri rule right hand path rule accept right hand path rule java com sun jersey server impl uri rule resourc class rule accept resourc class rule java com sun jersey server impl uri rule right hand path rule accept right hand path rule java com sun jersey server impl uri rule root resourc class rule accept root resourc class rule java com sun jersey server impl applic web applic impl handl request web applic impl java,0,0,0,0,0,0,
2865,Nagios server fails to start with invalid configuration error, nagio server fail start invalid configur error,Nagios fails to start with invalid configuration error. (This is likely intermittent).Error: Configuration validation failed - when Nagios is started./usr/sbin/nagios -v /etc/nagios/nagios.cfgCopyright (c) 1999-2009 Ethan GalstadLast Modified: 03-15-2013License: GPLWebsite: http://www.nagios.orgReading configuration data... Read main config file okay...Processing object config file '/etc/nagios/objects/commands.cfg'...Processing object config file '/etc/nagios/objects/contacts.cfg'...Processing object config file '/etc/nagios/objects/timeperiods.cfg'...Processing object config file '/etc/nagios/objects/templates.cfg'...Processing object config file '/etc/nagios/objects/hadoop-hosts.cfg'...Processing object config file '/etc/nagios/objects/hadoop-hostgroups.cfg'...Processing object config file '/etc/nagios/objects/hadoop-servicegroups.cfg'...Processing object config file '/etc/nagios/objects/hadoop-services.cfg'...Processing object config file '/etc/nagios/objects/hadoop-commands.cfg'...Error: Could not find any hostgroup matching 'resourcemanager' (config file '/etc/nagios/objects/hadoop-services.cfg'  starting on line 292) Error processing object config files!***&gt; One or more problems was encountered while processing the config files... Check your configuration file(s) to ensure that they contain valid directives and data defintions. If you are upgrading from a previous version of Nagios  you should be aware that some variables/definitions may have been removed or modified in this version. Make sure to read the HTML documentation regarding the config files  as well as the 'Whats New' section to find out what has changed., nagio fail start invalid configur error thi like intermitt error configur valid fail nagio start usr sbin nagio v etc nagio nagio cfg copyright c ethan galstad last modifi licens gpl websit http www nagio org read configur data read main config file okay process object config file etc nagio object command cfg process object config file etc nagio object contact cfg process object config file etc nagio object timeperiod cfg process object config file etc nagio object templat cfg process object config file etc nagio object hadoop host cfg process object config file etc nagio object hadoop hostgroup cfg process object config file etc nagio object hadoop servicegroup cfg process object config file etc nagio object hadoop servic cfg process object config file etc nagio object hadoop command cfg error could find hostgroup match resourcemanag config file etc nagio object hadoop servic cfg start line error process object config file gt one problem encount process config file check configur file ensur contain valid direct data defint if upgrad previou version nagio awar variabl definit may remov modifi version make sure read html document regard config file well what new section find chang,0,0,0,0,0,0,
2866,API JMX mapping needs to be updated due to property name changes, api jmx map need updat due properti name chang,In Ambari UI  we show properties like NameNode RPC Time.We used to get this metric by querying the NameNode component for 'RpcQueueTime_avg_time'.However  in Hadoop 2  it looks like this property name changed to 'RpcQueueTimeAvgTime'  so the Ambari API no longer contains these metrics. Other properties related to NameNode RPC may have changed. We need to update the mapping accordingly., in ambari ui show properti like name node rpc time we use get metric queri name node compon rpc queue time avg time howev hadoop look like properti name chang rpc queue time avg time ambari api longer contain metric other properti relat name node rpc may chang we need updat map accordingli,0,0,0,0,0,0,
2871,Nagios start fails due to invalid configs, nagio start fail due invalid config,Steps to reproduce:1) Deploy HDP-2.0.5 cluster with Nagios.2) Start of Nagios failed  puppet log:notice: /Stage[1]/Hdp::Snappy::Package/Hdp::Snappy::Package::Ln[32]/Hdp::Exec[hdp::snappy::package::ln 32]/Exec[hdp::snappy::package::ln 32]/returns: executed successfullynotice: /Stage[2]/Hdp-nagios::Server::Enable_snmp/Exec[enable_snmp]/returns: executed successfullynotice: /Stage[2]/Hdp-nagios::Server::Config/Hdp-nagios::Server::Configfile[hadoop-hostgroups.cfg]/Hdp::Configfile[/etc/nagios/objects/hadoop-hostgroups.cfg]/File[/etc/nagios/objects/hadoop-hostgroups.cfg]/content: content changed '{md5}873d2be7b9f78137e0740223944d93af' to '{md5}780117e3c2407e9d02b37eab93159149'notice: /Stage[2]/Hdp-nagios::Server::Config/Hdp-nagios::Server::Configfile[nagios]/Hdp::Configfile[/etc/init.d//nagios]/File[/etc/init.d//nagios]/content: content changed '{md5}3990694abc37617c79e2ea5276d71089' to '{md5}c4c4454911c0c6c1ba29d9d0dc2aa28c'notice: /Stage[2]/Hdp-nagios::Server::Config/Hdp-nagios::Server::Configfile[hadoop-hosts.cfg]/Hdp::Configfile[/etc/nagios/objects/hadoop-hosts.cfg]/File[/etc/nagios/objects/hadoop-hosts.cfg]/content: content changed '{md5}7979396ff0b495e40901acdd2ecc457c' to '{md5}fa5ec3a93a4827cc6a691e0b8e22b4f6'notice: /Stage[2]/Hdp-nagios::Server::Config/Hdp-nagios::Server::Configfile[hadoop-services.cfg]/Hdp::Configfile[/etc/nagios/objects/hadoop-services.cfg]/File[/etc/nagios/objects/hadoop-services.cfg]/content: content changed '{md5}06c9d0bb0aa3b1e7b30b19fb1bb30b5a' to '{md5}934241156b5489483dab8018f9cecd22'notice: /Stage[2]/Hdp-nagios::Server::Web_permisssions/Hdp::Exec[htpasswd -c -b /etc/nagios/htpasswd.users nagiosadmin p]/Exec[htpasswd -c -b /etc/nagios/htpasswd.users nagiosadmin p]/returns: executed successfullynotice: /Stage[2]/Hdp-nagios::Server::Web_permisssions/Hdp::Exec[apache_permissions_htpasswd.users]/Exec[apache_permissions_htpasswd.users]/returns: executed successfullynotice: /Stage[2]/Hdp-nagios::Server::Services/Exec[nagios]/returns: nagios is stoppednotice: /Stage[2]/Hdp-nagios::Server::Services/Exec[nagios]/returns: Configuration validation failed[FAILED]err: /Stage[2]/Hdp-nagios::Server::Services/Exec[nagios]/returns: change from notrun to 0 failed: service nagios start returned 1 instead of one of [0] at /var/lib/ambari-agent/puppet/modules/hdp-nagios/manifests/server.pp:284notice: /Stage[2]/Hdp-nagios::Server::Services/Exec[nagios]/returns: nagios is stoppednotice: /Stage[2]/Hdp-nagios::Server::Services/Exec[nagios]/returns: Configuration validation failed[FAILED]err: /Stage[2]/Hdp-nagios::Server::Services/Exec[nagios]: Failed to call refresh: service nagios start returned 1 instead of one of [0] at /var/lib/ambari-agent/puppet/modules/hdp-nagios/manifests/server.pp:284notice: /Stage[2]/Hdp-nagios::Server::Services/Anchor[hdp-nagios::server::services::end]: Dependency Exec[nagios] has failures: truewarning: /Stage[2]/Hdp-nagios::Server::Services/Anchor[hdp-nagios::server::services::end]: Skipping because of failed dependenciesnotice: /Stage[2]/Hdp-monitor-webserver/Hdp::Package[httpd]/Hdp::Package::Process_pkg[httpd]/Anchor[hdp::package::httpd::begin]: Dependency Exec[nagios] has failures: truewarning: /Stage[2]/Hdp-monitor-webserver/Hdp::Package[httpd]/Hdp::Package::Process_pkg[httpd]/Anchor[hdp::package::httpd::begin]: Skipping because of failed dependenciesnotice: /Stage[2]/Hdp-monitor-webserver/Hdp::Package[httpd]/Hdp::Package::Process_pkg[httpd]/Package[httpd]: Dependency Exec[nagios] has failures: truewarning: /Stage[2]/Hdp-monitor-webserver/Hdp::Package[httpd]/Hdp::Package::Process_pkg[httpd]/Package[httpd]: Skipping because of failed dependenciesnotice: /Stage[2]/Hdp-monitor-webserver/Hdp::Package[httpd]/Hdp::Package::Process_pkg[httpd]/Hdp::Java::Package[httpd]/Anchor[hdp::java::package::httpd::begin]: Dependency Exec[nagios] has failures: truewarning: /Stage[2]/Hdp-monitor-webserver/Hdp::Package[httpd]/Hdp::Package::Process_pkg[httpd]/Hdp::Java::Package[httpd]/Anchor[hdp::java::package::httpd::begin]: Skipping because of failed dependenciesnotice: /Stage[2]/Hdp-monitor-webserver/Hdp::Package[httpd]/Hdp::Package::Process_pkg[httpd]/Hdp::Java::Package[httpd]/Exec[mkdir -p /tmp/HDP-artifacts/ ; curl -kf --retry 10 http://dev01.hortonworks.com:8080/resources//jdk-6u31-linux-x64.bin -o /tmp/HDP-artifacts//jdk-6u31-linux-x64.bin httpd]: Dependency Exec[nagios] has failures: truewarning: /Stage[2]/Hdp-monitor-webserver/Hdp::Package[httpd]/Hdp::Package::Process_pkg[httpd]/Hdp::Java::Package[httpd]/Exec[mkdir -p /tmp/HDP-artifacts/ ; curl -kf --retry 10 http://dev01.hortonworks.com:8080/resources//jdk-6u31-linux-x64.bin -o /tmp/HDP-artifacts//jdk-6u31-linux-x64.bin httpd]: Skipping because of failed dependenciesnotice: /Stage[2]/Hdp-monitor-webserver/Hdp::Package[httpd]/Hdp::Package::Process_pkg[httpd]/Hdp::Java::Package[httpd]/Exec[mkdir -p /usr/jdk ; chmod +x /tmp/HDP-artifacts//jdk-6u31-linux-x64.bin; cd /usr/jdk ; echo A | /tmp/HDP-artifacts//jdk-6u31-linux-x64.bin -noregister &gt; /dev/null 2&gt;&amp;1 httpd]: Dependency Exec[nagios] has failures: truewarning: /Stage[2]/Hdp-monitor-webserver/Hdp::Package[httpd]/Hdp::Package::Process_pkg[httpd]/Hdp::Java::Package[httpd]/Exec[mkdir -p /usr/jdk ; chmod +x /tmp/HDP-artifacts//jdk-6u31-linux-x64.bin; cd /usr/jdk ; echo A | /tmp/HDP-artifacts//jdk-6u31-linux-x64.bin -noregister &gt; /dev/null 2&gt;&amp;1 httpd]: Skipping because of failed dependenciesnotice: /Stage[2]/Hdp-monitor-webserver/Hdp::Package[httpd]/Hdp::Package::Process_pkg[httpd]/Hdp::Java::Package[httpd]/File[/usr/jdk/jdk1.6.0_31/bin/java httpd]: Dependency Exec[nagios] has failures: truewarning: /Stage[2]/Hdp-monitor-webserver/Hdp::Package[httpd]/Hdp::Package::Process_pkg[httpd]/Hdp::Java::Package[httpd]/File[/usr/jdk/jdk1.6.0_31/bin/java httpd]: Skipping because of failed dependenciesnotice: /Stage[2]/Hdp-monitor-webserver/Hdp::Package[httpd]/Hdp::Package::Process_pkg[httpd]/Hdp::Java::Package[httpd]/Anchor[hdp::java::package::httpd::end]: Dependency Exec[nagios] has failures: truewarning: /Stage[2]/Hdp-monitor-webserver/Hdp::Package[httpd]/Hdp::Package::Process_pkg[httpd]/Hdp::Java::Package[httpd]/Anchor[hdp::java::package::httpd::end]: Skipping because of failed dependenciesnotice: /Stage[2]/Hdp-monitor-webserver/Hdp::Package[httpd]/Hdp::Package::Process_pkg[httpd]/Anchor[hdp::package::httpd::end]: Dependency Exec[nagios] has failures: truewarning: /Stage[2]/Hdp-monitor-webserver/Hdp::Package[httpd]/Hdp::Package::Process_pkg[httpd]/Anchor[hdp::package::httpd::end]: Skipping because of failed dependenciesnotice: /Stage[2]/Hdp-monitor-webserver/Hdp::Exec[monitor webserver restart]/Anchor[hdp::exec::monitor webserver restart::begin]: Dependency Exec[nagios] has failures: truewarning: /Stage[2]/Hdp-monitor-webserver/Hdp::Exec[monitor webserver restart]/Anchor[hdp::exec::monitor webserver restart::begin]: Skipping because of failed dependenciesnotice: /Stage[2]/Hdp-monitor-webserver/Hdp::Exec[monitor webserver restart]/Exec[monitor webserver restart]: Dependency Exec[nagios] has failures: truewarning: /Stage[2]/Hdp-monitor-webserver/Hdp::Exec[monitor webserver restart]/Exec[monitor webserver restart]: Skipping because of failed dependenciesnotice: /Stage[2]/Hdp-monitor-webserver/Hdp::Exec[monitor webserver restart]/Anchor[hdp::exec::monitor webserver restart::end]: Dependency Exec[nagios] has failures: truewarning: /Stage[2]/Hdp-monitor-webserver/Hdp::Exec[monitor webserver restart]/Anchor[hdp::exec::monitor webserver restart::end]: Skipping because of failed dependencies3) Run checkconfig:[root@dev02 ~]# /usr/sbin/nagios -v /etc/nagios/nagios.cfg Nagios Core 3.5.0Copyright (c) 2009-2011 Nagios Core Development Team and Community ContributorsCopyright (c) 1999-2009 Ethan GalstadLast Modified: 03-15-2013License: GPLWebsite: http://www.nagios.orgReading configuration data... Read main config file okay...Processing object config file '/etc/nagios/objects/commands.cfg'...Processing object config file '/etc/nagios/objects/contacts.cfg'...Processing object config file '/etc/nagios/objects/timeperiods.cfg'...Processing object config file '/etc/nagios/objects/templates.cfg'...Processing object config file '/etc/nagios/objects/hadoop-hosts.cfg'...Processing object config file '/etc/nagios/objects/hadoop-hostgroups.cfg'...Processing object config file '/etc/nagios/objects/hadoop-servicegroups.cfg'...Processing object config file '/etc/nagios/objects/hadoop-services.cfg'...Processing object config file '/etc/nagios/objects/hadoop-commands.cfg'...Error: Could not find any servicegroup matching 'MAPREDUCE' (config file '/etc/nagios/objects/hadoop-services.cfg'  starting on line 67) Error processing object config files!***&gt; One or more problems was encountered while processing the config files... Check your configuration file(s) to ensure that they contain valid directives and data defintions. If you are upgrading from a previous version of Nagios  you should be aware that some variables/definitions may have been removed or modified in this version. Make sure to read the HTML documentation regarding the config files  as well as the 'Whats New' section to find out what has changed.[root@dev02 ~]# It seems we have invalid condition for generation of MAPREDUCE Nagios checks., step reproduc deploy hdp cluster nagio start nagio fail puppet log notic stage hdp snappi packag hdp snappi packag ln hdp exec hdp snappi packag ln exec hdp snappi packag ln return execut successfullynotic stage hdp nagio server enabl snmp exec enabl snmp return execut successfullynotic stage hdp nagio server config hdp nagio server configfil hadoop hostgroup cfg hdp configfil etc nagio object hadoop hostgroup cfg file etc nagio object hadoop hostgroup cfg content content chang md b f e af md e c e b eab notic stage hdp nagio server config hdp nagio server configfil nagio hdp configfil etc init nagio file etc init nagio content content chang md abc c e ea md c c c c c ba dc aa c notic stage hdp nagio server config hdp nagio server configfil hadoop host cfg hdp configfil etc nagio object hadoop host cfg file etc nagio object hadoop host cfg content content chang md ff b e acdd ecc c md fa ec cc e b e b f notic stage hdp nagio server config hdp nagio server configfil hadoop servic cfg hdp configfil etc nagio object hadoop servic cfg file etc nagio object hadoop servic cfg content content chang md c bb aa b e b b fb bb b md b dab f cecd notic stage hdp nagio server web permisss hdp exec htpasswd c b etc nagio htpasswd user nagiosadmin p exec htpasswd c b etc nagio htpasswd user nagiosadmin p return execut successfullynotic stage hdp nagio server web permisss hdp exec apach permiss htpasswd user exec apach permiss htpasswd user return execut successfullynotic stage hdp nagio server servic exec nagio return nagio stoppednotic stage hdp nagio server servic exec nagio return configur valid fail fail err stage hdp nagio server servic exec nagio return chang notrun fail servic nagio start return instead one var lib ambari agent puppet modul hdp nagio manifest server pp notic stage hdp nagio server servic exec nagio return nagio stoppednotic stage hdp nagio server servic exec nagio return configur valid fail fail err stage hdp nagio server servic exec nagio fail call refresh servic nagio start return instead one var lib ambari agent puppet modul hdp nagio manifest server pp notic stage hdp nagio server servic anchor hdp nagio server servic end depend exec nagio failur truewarn stage hdp nagio server servic anchor hdp nagio server servic end skip fail dependenciesnotic stage hdp monitor webserv hdp packag httpd hdp packag process pkg httpd anchor hdp packag httpd begin depend exec nagio failur truewarn stage hdp monitor webserv hdp packag httpd hdp packag process pkg httpd anchor hdp packag httpd begin skip fail dependenciesnotic stage hdp monitor webserv hdp packag httpd hdp packag process pkg httpd packag httpd depend exec nagio failur truewarn stage hdp monitor webserv hdp packag httpd hdp packag process pkg httpd packag httpd skip fail dependenciesnotic stage hdp monitor webserv hdp packag httpd hdp packag process pkg httpd hdp java packag httpd anchor hdp java packag httpd begin depend exec nagio failur truewarn stage hdp monitor webserv hdp packag httpd hdp packag process pkg httpd hdp java packag httpd anchor hdp java packag httpd begin skip fail dependenciesnotic stage hdp monitor webserv hdp packag httpd hdp packag process pkg httpd hdp java packag httpd exec mkdir p tmp hdp artifact curl kf retri http dev hortonwork com resourc jdk u linux x bin tmp hdp artifact jdk u linux x bin httpd depend exec nagio failur truewarn stage hdp monitor webserv hdp packag httpd hdp packag process pkg httpd hdp java packag httpd exec mkdir p tmp hdp artifact curl kf retri http dev hortonwork com resourc jdk u linux x bin tmp hdp artifact jdk u linux x bin httpd skip fail dependenciesnotic stage hdp monitor webserv hdp packag httpd hdp packag process pkg httpd hdp java packag httpd exec mkdir p usr jdk chmod x tmp hdp artifact jdk u linux x bin cd usr jdk echo a tmp hdp artifact jdk u linux x bin noregist gt dev null gt amp httpd depend exec nagio failur truewarn stage hdp monitor webserv hdp packag httpd hdp packag process pkg httpd hdp java packag httpd exec mkdir p usr jdk chmod x tmp hdp artifact jdk u linux x bin cd usr jdk echo a tmp hdp artifact jdk u linux x bin noregist gt dev null gt amp httpd skip fail dependenciesnotic stage hdp monitor webserv hdp packag httpd hdp packag process pkg httpd hdp java packag httpd file usr jdk jdk bin java httpd depend exec nagio failur truewarn stage hdp monitor webserv hdp packag httpd hdp packag process pkg httpd hdp java packag httpd file usr jdk jdk bin java httpd skip fail dependenciesnotic stage hdp monitor webserv hdp packag httpd hdp packag process pkg httpd hdp java packag httpd anchor hdp java packag httpd end depend exec nagio failur truewarn stage hdp monitor webserv hdp packag httpd hdp packag process pkg httpd hdp java packag httpd anchor hdp java packag httpd end skip fail dependenciesnotic stage hdp monitor webserv hdp packag httpd hdp packag process pkg httpd anchor hdp packag httpd end depend exec nagio failur truewarn stage hdp monitor webserv hdp packag httpd hdp packag process pkg httpd anchor hdp packag httpd end skip fail dependenciesnotic stage hdp monitor webserv hdp exec monitor webserv restart anchor hdp exec monitor webserv restart begin depend exec nagio failur truewarn stage hdp monitor webserv hdp exec monitor webserv restart anchor hdp exec monitor webserv restart begin skip fail dependenciesnotic stage hdp monitor webserv hdp exec monitor webserv restart exec monitor webserv restart depend exec nagio failur truewarn stage hdp monitor webserv hdp exec monitor webserv restart exec monitor webserv restart skip fail dependenciesnotic stage hdp monitor webserv hdp exec monitor webserv restart anchor hdp exec monitor webserv restart end depend exec nagio failur truewarn stage hdp monitor webserv hdp exec monitor webserv restart anchor hdp exec monitor webserv restart end skip fail depend run checkconfig root dev usr sbin nagio v etc nagio nagio cfg nagio core copyright c nagio core develop team commun contributor copyright c ethan galstad last modifi licens gpl websit http www nagio org read configur data read main config file okay process object config file etc nagio object command cfg process object config file etc nagio object contact cfg process object config file etc nagio object timeperiod cfg process object config file etc nagio object templat cfg process object config file etc nagio object hadoop host cfg process object config file etc nagio object hadoop hostgroup cfg process object config file etc nagio object hadoop servicegroup cfg process object config file etc nagio object hadoop servic cfg process object config file etc nagio object hadoop command cfg error could find servicegroup match mapreduc config file etc nagio object hadoop servic cfg start line error process object config file gt one problem encount process config file check configur file ensur contain valid direct data defint if upgrad previou version nagio awar variabl definit may remov modifi version make sure read html document regard config file well what new section find chang root dev it seem invalid condit gener mapreduc nagio check,0,0,0,0,0,0,
2876,Puppet script syntax issues result in hive deployment with custom DB and oozie service check failures, puppet script syntax issu result hive deploy custom db oozi servic check failur,Few puppet variables are missing in jdbc-connector and oozie service-check puppet scripts., few puppet variabl miss jdbc connector oozi servic check puppet script,0,0,0,0,0,0,
2879,Oozie failed at smoke test in secured cluster, oozi fail smoke test secur cluster,stderr:None stdout:notice: /Stage[1]/Hdp::Snappy::Package/Hdp::Snappy::Package::Ln[32]/Hdp::Exec[hdp::snappy::package::ln 32]/Exec[hdp::snappy::package::ln 32]/returns: executed successfullynotice: /Stage[1]/Hdp::Snmp/Hdp::Package[snmp]/Hdp::Package::Process_pkg[snmp]/Hdp::Java::Package[snmp]/Hdp::Java::Jce::Package[snmp]/Exec[jce-install snmp]/returns: executed successfullynotice: /Stage[2]/Hdp-oozie::Oozie::Service_check/Hdp-oozie::Smoke_shell_file[oozieSmoke.sh]/Exec[/tmp/oozieSmoke.sh]/returns: Moved to trash: hdfs://domU-12-31-39-07-D5-91.compute-1.internal:8020/user/ambari-qa/examplesnotice: /Stage[2]/Hdp-oozie::Oozie::Service_check/Hdp-oozie::Smoke_shell_file[oozieSmoke.sh]/Exec[/tmp/oozieSmoke.sh]/returns: Moved to trash: hdfs://domU-12-31-39-07-D5-91.compute-1.internal:8020/user/ambari-qa/input-datanotice: /Stage[2]/Hdp-oozie::Oozie::Service_check/Hdp-oozie::Smoke_shell_file[oozieSmoke.sh]/Exec[/tmp/oozieSmoke.sh]/returns: Error: AUTHENTICATION : Could not authenticate  GSSException: No valid credentials provided (Mechanism level: Server not found in Kerberos database (7) - UNKNOWN_SERVER)notice: /Stage[2]/Hdp-oozie::Oozie::Service_check/Hdp-oozie::Smoke_shell_file[oozieSmoke.sh]/Exec[/tmp/oozieSmoke.sh]/returns: Invalid sub-command: Missing argument for option: infonotice: /Stage[2]/Hdp-oozie::Oozie::Service_check/Hdp-oozie::Smoke_shell_file[oozieSmoke.sh]/Exec[/tmp/oozieSmoke.sh]/returns:notice: /Stage[2]/Hdp-oozie::Oozie::Service_check/Hdp-oozie::Smoke_shell_file[oozieSmoke.sh]/Exec[/tmp/oozieSmoke.sh]/returns: use 'help [sub-command]' for help detailsnotice: /Stage[2]/Hdp-oozie::Oozie::Service_check/Hdp-oozie::Smoke_shell_file[oozieSmoke.sh]/Exec[/tmp/oozieSmoke.sh]/returns: Invalid sub-command: Missing argument for option: infonotice: /Stage[2]/Hdp-oozie::Oozie::Service_check/Hdp-oozie::Smoke_shell_file[oozieSmoke.sh]/Exec[/tmp/oozieSmoke.sh]/returns:notice: /Stage[2]/Hdp-oozie::Oozie::Service_check/Hdp-oozie::Smoke_shell_file[oozieSmoke.sh]/Exec[/tmp/oozieSmoke.sh]/returns: use 'help [sub-command]' for help detailsnotice: /Stage[2]/Hdp-oozie::Oozie::Service_check/Hdp-oozie::Smoke_shell_file[oozieSmoke.sh]/Exec[/tmp/oozieSmoke.sh]/returns:notice: /Stage[2]/Hdp-oozie::Oozie::Service_check/Hdp-oozie::Smoke_shell_file[oozieSmoke.sh]/Exec[/tmp/oozieSmoke.sh]/returns: workflow_status=err: /Stage[2]/Hdp-oozie::Oozie::Service_check/Hdp-oozie::Smoke_shell_file[oozieSmoke.sh]/Exec[/tmp/oozieSmoke.sh]/returns: change from notrun to 0 failed: sh /tmp/oozieSmoke.sh /etc/oozie/conf /etc/hadoop/conf ambari-qa true /etc/security/keytabs/smokeuser.headless.keytab EXAMPLE.COM jt/domu-12-31-39-07-d5-91.compute-1.internal@EXAMPLE.COM nn/domu-12-31-39-07-d5-91.compute-1.internal@EXAMPLE.COM /usr/bin/kinit returned 1 instead of one of [0] at /var/lib/ambari-agent/puppet/modules/hdp-oozie/manifests/oozie/service_check.pp:63notice: Finished catalog run in 51.05 seconds, stderr none stdout notic stage hdp snappi packag hdp snappi packag ln hdp exec hdp snappi packag ln exec hdp snappi packag ln return execut successfullynotic stage hdp snmp hdp packag snmp hdp packag process pkg snmp hdp java packag snmp hdp java jce packag snmp exec jce instal snmp return execut successfullynotic stage hdp oozi oozi servic check hdp oozi smoke shell file oozi smoke sh exec tmp oozi smoke sh return move trash hdf dom u d comput intern user ambari qa examplesnotic stage hdp oozi oozi servic check hdp oozi smoke shell file oozi smoke sh exec tmp oozi smoke sh return move trash hdf dom u d comput intern user ambari qa input datanotic stage hdp oozi oozi servic check hdp oozi smoke shell file oozi smoke sh exec tmp oozi smoke sh return error authent could authent gss except no valid credenti provid mechan level server found kerbero databas unknown server notic stage hdp oozi oozi servic check hdp oozi smoke shell file oozi smoke sh exec tmp oozi smoke sh return invalid sub command miss argument option infonotic stage hdp oozi oozi servic check hdp oozi smoke shell file oozi smoke sh exec tmp oozi smoke sh return notic stage hdp oozi oozi servic check hdp oozi smoke shell file oozi smoke sh exec tmp oozi smoke sh return use help sub command help detailsnotic stage hdp oozi oozi servic check hdp oozi smoke shell file oozi smoke sh exec tmp oozi smoke sh return invalid sub command miss argument option infonotic stage hdp oozi oozi servic check hdp oozi smoke shell file oozi smoke sh exec tmp oozi smoke sh return notic stage hdp oozi oozi servic check hdp oozi smoke shell file oozi smoke sh exec tmp oozi smoke sh return use help sub command help detailsnotic stage hdp oozi oozi servic check hdp oozi smoke shell file oozi smoke sh exec tmp oozi smoke sh return notic stage hdp oozi oozi servic check hdp oozi smoke shell file oozi smoke sh exec tmp oozi smoke sh return workflow statu err stage hdp oozi oozi servic check hdp oozi smoke shell file oozi smoke sh exec tmp oozi smoke sh return chang notrun fail sh tmp oozi smoke sh etc oozi conf etc hadoop conf ambari qa true etc secur keytab smokeus headless keytab exampl com jt domu comput intern exampl com nn domu comput intern exampl com usr bin kinit return instead one var lib ambari agent puppet modul hdp oozi manifest oozi servic check pp notic finish catalog run second,0,0,0,0,0,0,
2884,Oozie start fails - likely due to 'failed install', oozi start fail like due fail instal,Oozie start failed with following in the log: hadoop dfs -chmod -R 755 /user/oozie/share' returned 1 instead of one of 0On the node: chmod: '/user/oozie/share': No such file or directoryoozie@c6402 ~$ hadoop dfs -ls /user/ DEPRECATED: Use of this script to execute hdfs command is deprecated.Instead use the hdfs command for it.Found 3 itemsdrwxrwx--- - ambari-qa hdfs 0 2013-08-02 02:40 /user/ambari-qadrwx------ - hive hdfs 0 2013-08-02 02:41 /user/hivedrwxrwxr-x - hdfs hdfs 0 2013-08-02 02:42 /user/oozieoozie@c6402 ~$ hadoop dfs -ls /user/oozieDEPRECATED: Use of this script to execute hdfs command is deprecated.Instead use the hdfs command for it.oozie@c6402 ~$The reason  the oozie smoke is failing to run an oozie job with more than one node in the cluster  due to bad settings in /etc/hadoop/core-site.xml:&lt;property&gt; &lt;name&gt;hadoop.proxyuser.oozie.hosts&lt;/name&gt; &lt;value&gt;host1&lt;/value&gt;&lt;/property&gt;host1  in my case is host on which oozie is not installed. After changing this to host2 (where oozie server is installed)  oozie smoke succeeded., oozi start fail follow log hadoop df chmod r user oozi share return instead one on node chmod user oozi share no file directoryoozi c hadoop df ls user deprec use script execut hdf command deprec instead use hdf command found itemsdrwxrwx ambari qa hdf user ambari qadrwx hive hdf user hivedrwxrwxr x hdf hdf user oozieoozi c hadoop df ls user oozi deprec use script execut hdf command deprec instead use hdf command oozi c the reason oozi smoke fail run oozi job one node cluster due bad set etc hadoop core site xml lt properti gt lt name gt hadoop proxyus oozi host lt name gt lt valu gt host lt valu gt lt properti gt host case host oozi instal after chang host oozi server instal oozi smoke succeed,0,0,0,0,0,0,
2886,HDFS Service Check failed (as designed) but took 10 mins to fail, hdf servic check fail design took min fail,After I had a cluster up and running successfully  but manually turned HDFS Safe Mode ON by running:hdfs dfsadmin -safemode enter.HDFS Execute Check failed due to Puppet timeout after 10 minutes.It should have failed quicker since task timeout is 10 minutes on server., after i cluster run success manual turn hdf safe mode on run hdf dfsadmin safemod enter hdf execut check fail due puppet timeout minut it fail quicker sinc task timeout minut server,0,0,0,0,1,0,
2891,hadoop-env.sh and core-site are missing on hosts that have only yarn components deployed, hadoop env sh core site miss host yarn compon deploy,Yarn component need hadoop-env.sh and core-site.xml. These files should be deployed on hosts on which only yarn components are deployed., yarn compon need hadoop env sh core site xml these file deploy host yarn compon deploy,0,0,0,0,0,0,
2903,Add HBase 96 metrics changes to jmx in a backwards compatible way., add h base metric chang jmx backward compat way,Add HBase 96 metrics changes to jmx in a backwards compatible way., add h base metric chang jmx backward compat way,0,0,0,0,0,0,
2905,SNAMENODE should not start after transition to Maintenance mode, snamenod start transit mainten mode,Currently  in HA NN cluster  starting HDFS service at services tab  tries to start SNAMENODE as well (through it is in Maintainance state because of executing curl -u admin:admin -i -X PUT -d '{'RequestInfo':{'context':'SNN maintenance'} 'Body':{'HostRoles':{'state':'MAINTENANCE'}}}' http://$SERVER:8080/api/v1/clusters/$CLUSTER/hosts/$SNN_HOST/host_components/SECONDARY_NAMENODE command ), current ha nn cluster start hdf servic servic tab tri start snamenod well maintain state execut curl u admin admin x put request info context snn mainten bodi host role state mainten http server api v cluster cluster host snn host host compon secondari namenod command,0,0,0,0,0,0,
2920,Rename alert titles and descriptions, renam alert titl descript,Currently Nagios alerts are shown in Ambari UI like so: (green check) NameNode process down &lt;- means NameNode process is up (red X) NameNode process down &lt;- means NameNode process is down (green check) Percent DataNode down &lt;- means % of DataNodes that are up is above the threshold (red X) Percent DataNode down &lt;- means % of DataNodes that are up is below the threshold (green check) Nagios status log staleness &lt;- means Nagios status log is fresh (red X) Nagios status log staleness &lt;- means Nagios status log is staleWhen a user sees the word 'down' with a positive indication (green check) for it  it's confusing. It's like saying 'this is red' in green... is it green or red?The proposal here is to rename these alerts  like so: (green check) NameNode process &lt;- means NameNode process is up/healthy (red X) NameNode process &lt;- means NameNode process is down/unhealthy (green check) Percent DataNodes live &lt;- means % of DataNodes that are up/healthy is above the threshold (red X) Percent DataNodes live &lt;- means % of DataNodes that are up/healthy is below the threshold (green check) Nagios status log freshness &lt;- means Nagios status log is fresh (red X) Nagios status log freshness &lt;- means Nagios status log is staleAlso there are inconsistencies in the way we show component names in alert titles and descriptions (like 'templeton server status' to mean 'WebHCat Server status'  etc). These need to be fixed., current nagio alert shown ambari ui like green check name node process lt mean name node process red x name node process lt mean name node process green check percent data node lt mean data node threshold red x percent data node lt mean data node threshold green check nagio statu log stale lt mean nagio statu log fresh red x nagio statu log stale lt mean nagio statu log stale when user see word posit indic green check confus it like say red green green red the propos renam alert like green check name node process lt mean name node process healthi red x name node process lt mean name node process unhealthi green check percent data node live lt mean data node healthi threshold red x percent data node live lt mean data node healthi threshold green check nagio statu log fresh lt mean nagio statu log fresh red x nagio statu log fresh lt mean nagio statu log stale also inconsist way show compon name alert titl descript like templeton server statu mean web h cat server statu etc these need fix,0,0,0,0,0,0,
2927,ResourceManager's RPC and NodeManager counts missing from time-series, resourc manag rpc node manag count miss time seri,ResourceManager's rpc.rpc.RpcQueueTimeAvgTime and yarn.ClusterMetrics.NumActiveNMs are not being provided accurately due to changes introduced in AMBARI-2910 to YARN configuration to collect more metrics.We need to narrow down the scope of metric collection to keep getting these previous metrics., resourc manag rpc rpc rpc queue time avg time yarn cluster metric num activ n ms provid accur due chang introduc ambari yarn configur collect metric we need narrow scope metric collect keep get previou metric,0,0,0,0,0,0,
2931,Popover stuck after routing to another page, popov stuck rout anoth page,Steps to reporduce:1. Go to Installer-&gt;Welcome step2. Focus on cluster name textfield3. Hover on 'learn more' label4. Press Enter to route to the next pageResult:Popover remains visible on other pages, step reporduc go instal gt welcom step focu cluster name textfield hover learn label press enter rout next page result popov remain visibl page,0,0,0,0,0,0,
2938,Update stack definition for MAPREDUCE2, updat stack definit mapreduc,Update stack definition for MAPREDUCE2 and the default site xml files., updat stack definit mapreduc default site xml file,0,0,0,0,0,0,
2939,Update 1.3.2 stack definition for repo url, updat stack definit repo url,Update 1.3.2 stack definition for repo url, updat stack definit repo url,0,0,0,0,0,0,
2943,Oozie smoke tests fail on Ambari with NPE in Oozie Server., oozi smoke test fail ambari npe oozi server,Oozie smoke tests fail on Ambari with NPE in Oozie Server., oozi smoke test fail ambari npe oozi server,0,0,0,0,0,0,
2947,Use a specific build number for the stck builds., use specif build number stck build,Use a specific build number for the stck builds., use specif build number stck build,0,0,0,0,0,0,
2948,Mapreduce pid directory cutomization fails, mapreduc pid directori cutom fail,Changing mapreduce log directory prefix results in Live status showing history server not started., chang mapreduc log directori prefix result live statu show histori server start,0,0,0,0,0,0,
2972,Provide read-only view of security wizard entries, provid read view secur wizard entri,After enabling security  when browsing back to Admin &gt; Security  it would be good to see the tabs with the values entered during wizard setup (read-only)  just so it's easy to see what the user specifically entered during the wizard setup., after enabl secur brows back admin gt secur would good see tab valu enter wizard setup read easi see user specif enter wizard setup,0,0,0,0,0,0,
2973,Ambari server and agent are not stopped during package uninstall, ambari server agent stop packag uninstal,When ambari-agent and ambari-server packages are uninstalled  uninstall scriplets don't stop running services. That's why processes remain running even after package removal. That may cause issues during upgrade (if administrator misses the 'stop services' step)[root@host01 ~]$ ambari-server statusUsing python /usr/bin/python2.6Ambari-server statusAmbari Server runningFound Ambari Server PID: '7850 at: /var/run/ambari-server/ambari-server.pid[root@host01]# ambari-agent statusFound ambari-agent PID: 3521ambari-agent running.Agent PID at: /var/run/ambari-agent/ambari-agent.pidAgent out at: /var/log/ambari-agent/ambari-agent.outAgent log at: /var/log/ambari-agent/ambari-agent.log[root@host01]# rpm -e ambari-server[root@host01]# rpm -e ambari-agent[root@host01]# ps aux | grep ambari | grep -v grep | grep -v postgresroot 7850 2.1 13.2 3031480 254212 ? Sl 20:51 0:36 /usr/jdk64/jdk1.6.0_31/bin/java -server -XX:NewRatio=3 -XX:+UseConcMarkSweepGC -XX:-UseGCOverheadLimit -XX:CMSInitiatingOccupancyFraction=60 -Xms512m -Xmx2048m -cp /etc/ambari-server/conf:/usr/lib/ambari-server/*:/sbin:/bin:/usr/sbin:/usr/bin:/usr/lib/ambari-server/* org.apache.ambari.server.controller.AmbariServerroot 3521 0.7 1.1 499760 22544 ? Sl 20:53 0:12 /usr/bin/python2.6 /usr/lib/python2.6/site-packages/ambari_agent/main.py start restart --expected-hostname=host01, when ambari agent ambari server packag uninstal uninstal scriplet stop run servic that process remain run even packag remov that may caus issu upgrad administr miss stop servic step root host ambari server statu use python usr bin python ambari server statu ambari server run found ambari server pid var run ambari server ambari server pid root host ambari agent statu found ambari agent pid ambari agent run agent pid var run ambari agent ambari agent pid agent var log ambari agent ambari agent agent log var log ambari agent ambari agent log root host rpm e ambari server root host rpm e ambari agent root host ps aux grep ambari grep v grep grep v postgresroot sl usr jdk jdk bin java server xx new ratio xx use conc mark sweep gc xx use gc overhead limit xx cm initi occup fraction xm xmx cp etc ambari server conf usr lib ambari server sbin bin usr sbin usr bin usr lib ambari server org apach ambari server control ambari serverroot sl usr bin python usr lib python site packag ambari agent main py start restart expect hostnam host,0,0,0,0,0,0,
2986,Should turn on predicate pushdown by default., should turn predic pushdown default,Should turn on predicate pushdown by default., should turn predic pushdown default,0,0,0,0,0,0,
3009,Trim and/or validate config parameter values to prevent failures due to extra spaces, trim valid config paramet valu prevent failur due extra space,Automatically trim whitespaces (both leading and trailing) for: Database Host in Oozie and Hive Database Name in Oozie and Hive Database URL in Oozie and Hive All directories  including log dir  pid dir data dir  etc (I believe we already trim  split  and join for the UI config type 'directories'. We should trim on the single-line directory values  if we are not doing so already).Automatically trim all trailing spaces (but not leading spaces) for all config values with the following exceptions: Password fields Values that consist of spaces only (such as ' '), automat trim whitespac lead trail databas host oozi hive databas name oozi hive databas url oozi hive all directori includ log dir pid dir data dir etc i believ alreadi trim split join ui config type directori we trim singl line directori valu alreadi automat trim trail space lead space config valu follow except password field valu consist space,1,0,0,0,0,0,
3019,Ambari should always point to latest repo, ambari alway point latest repo,Ambari should always point to latest repo., ambari alway point latest repo,0,0,0,0,0,0,
3021,Customize Services page->Misc tab: Popup with related properties does not opened after 'Group User' value changing, custom servic page misc tab popup relat properti open group user valu chang,,,0,0,0,0,0,0,
3024,Oozie oozie-site.xml misssing two xsd values causing shell and sla workflows to fail, oozi oozi site xml misss two xsd valu caus shell sla workflow fail,Following two values need to be added to oozie-site.xml in order to get sla and shell workflows to run successfully.shell-action-0.2.xsdoozie-sla-0.1.xsd oozie-sla-0.2.xsdFollowing is the property name&lt;property&gt;&lt;name&gt;oozie.service.SchemaService.wf.ext.schemas&lt;/name&gt;&lt;value&gt;shell-action-0.1.xsd email-action-0.1.xsd hive-action-0.2.xsd sqoop-action-0.2.xsd ssh-action-0.1.xsd distcp-action-0.1.xsd&lt;/value&gt;&lt;/property&gt;, follow two valu need ad oozi site xml order get sla shell workflow run success shell action xsdoozi sla xsd oozi sla xsd follow properti name lt properti gt lt name gt oozi servic schema servic wf ext schema lt name gt lt valu gt shell action xsd email action xsd hive action xsd sqoop action xsd ssh action xsd distcp action xsd lt valu gt lt properti gt,0,0,0,0,0,0,
3026,Ambari server setup with silent option prints error statement for the first time, ambari server setup silent option print error statement first time,command: ambari-server setup -sCompleting setup...Configuring database...Enter advanced database configuration [y/n] &#40;n&#41;? ERROR: Connection properties not set in config file.Default properties detected. Using built-in database.Checking PostgreSQL...Running initdb: This may take upto a minute.About to start PostgreSQLConfiguring local database...Configuring PostgreSQL...Restarting PostgreSQLAmbari Server 'setup' completed successfully.Running ambari-server setup command again doesn't reproduce the error statement., command ambari server setup complet setup configur databas enter advanc databas configur n n error connect properti set config file default properti detect use built databas check postgr sql run initdb thi may take upto minut about start postgr sql configur local databas configur postgr sql restart postgr sql ambari server setup complet success run ambari server setup command reproduc error statement,1,0,0,0,0,0,
3047,Enhance host clean up to handle tmp files and folders, enhanc host clean handl tmp file folder,Host cleanup should: remove files and folders in tmp folder based on what users are being cleaned remove default hadoop group, host cleanup remov file folder tmp folder base user clean remov default hadoop group,1,0,0,0,0,0,
3056,Advanced config orders should be consistent in Install Wizard > Customize Services and Monitoring > Services > Config, advanc config order consist instal wizard custom servic monitor servic config,Compare the config parameter ordering in Install Wizard &gt; Customize Services service configs and Monitoring &gt; Services &gt; Config; they are different.We should use the same parameter ordering that we use in Install Wizard &gt; Customize Services for Monitoring &gt; Services &gt; Config., compar config paramet order instal wizard gt custom servic servic config monitor gt servic gt config differ we use paramet order use instal wizard gt custom servic monitor gt servic gt config,1,0,0,0,0,0,
3061,Do not use regex to determine folder name by full path for dfs_domain_socket_path, do use regex determin folder name full path df domain socket path,The point is to use custom puppet function instead of regex for line$dfs_domain_socket_path_dir = regsubst($hdp-hadoop::params::dfs_domain_socket_path  '/[^//]+$'  '').It could be implemented with ruby:File.split('/path/to/file') # =&gt; ['/path/to'  'file'], the point use custom puppet function instead regex line df domain socket path dir regsubst hdp hadoop param df domain socket path it could implement rubi file split path file gt path file,1,0,0,0,0,0,
3068,Warning messages not cleared when task fails, warn messag clear task fail,Installed a cluster  namenode start fails  install exits with warnings.If i go an look at the specific task that fails  the puppet warnings are still present. Seems like those puppet warnings didn't get cleared.In a task failure case  having the warnings clear is when it's most important., instal cluster namenod start fail instal exit warn if go look specif task fail puppet warn still present seem like puppet warn get clear in task failur case warn clear import,1,0,0,0,0,0,
3069,Fix Unit tests, fix unit test,,,1,0,0,0,0,0,
3073,Amabri Client refactoring -2, amabri client refactor,1) The last patch had an issue with the testcase because of which it did not compile. I might have accidently added a line while I was saving some of the files.(this is fixed in the new patch)The trunk currently fails for ambari-client.https://cwiki.apache.org/confluence/display/AMBARI/Ambari+python+Client has all the exposed methods, the last patch issu testcas compil i might accid ad line i save file fix new patch the trunk current fail ambari client http cwiki apach org confluenc display ambari ambari python client expos method,1,0,0,0,0,0,
3074,Ambari wont start NodeManager because one of multiple folders not created, ambari wont start node manag one multipl folder creat,yarn-site having:'yarn.nodemanager.local-dirs' : '/grid/0/hadoop/yarn /grid/1/hadoop/yarn /grid/2/hadoop/yarn /grid/3/hadoop/yarn /grid/4/hadoop/yarn /grid/5/hadoop/yarn' 'yarn.nodemanager.log-dirs' : '/grid/0/hadoop/yarn /grid/1/hadoop/yarn /grid/2/hadoop/yarn /grid/3/hadoop/yarn /grid/4/hadoop/yarn /grid/5/hadoop/yarn' Now /grid/3 was mounted as read-only due to some disk errors. Though other folders got successfully created  Ambari will not start the NodeManager process.notice: /Stage[1]/Hdp::Snappy::Package/Hdp::Snappy::Package::Ln[32]/Hdp::Exec[hdp::snappy::package::ln 32]/Exec[hdp::snappy::package::ln 32]/returns: executed successfullynotice: /Stage[2]/Hdp-yarn::Nodemanager/Hdp-yarn::Nodemanager::Create_nm_dirs[/grid/3/hadoop/yarn]/Hdp::Directory_recursive_create[/grid/3/hadoop/yarn]/Hdp::Exec[mkdir -p /grid/3/hadoop/yarn]/Exec[mkdir -p /grid/3/hadoop/yarn]/returns: mkdir: cannot create directory '/grid/3/hadoop': Read-only file systemerr: /Stage[2]/Hdp-yarn::Nodemanager/Hdp-yarn::Nodemanager::Create_nm_dirs[/grid/3/hadoop/yarn]/Hdp::Directory_recursive_create[/grid/3/hadoop/yarn]/Hdp::Exec[mkdir -p /grid/3/hadoop/yarn]/Exec[mkdir -p /grid/3/hadoop/yarn]/returns: change from notrun to 0 failed: mkdir -p /grid/3/hadoop/yarn returned 1 instead of one of [0] at /var/lib/ambari-agent/puppet/modules/hdp/manifests/init.pp:479notice: /Stage[2]/Hdp-yarn::Nodemanager/Hdp-yarn::Nodemanager::Create_nm_dirs[/grid/3/hadoop/yarn]/Hdp::Directory_recursive_create[/grid/3/hadoop/yarn]/Hdp::Exec[mkdir -p /grid/3/hadoop/yarn]/Anchor[hdp::exec::mkdir -p /grid/3/hadoop/yarn::end]: Dependency Exec[mkdir -p /grid/3/hadoop/yarn] has failures: truewarning: /Stage[2]/Hdp-yarn::Nodemanager/Hdp-yarn::Nodemanager::Create_nm_dirs[/grid/3/hadoop/yarn]/Hdp::Directory_recursive_create[/grid/3/hadoop/yarn]/Hdp::Exec[mkdir -p /grid/3/hadoop/yarn]/Anchor[hdp::exec::mkdir -p /grid/3/hadoop/yarn::end]: Skipping because of failed dependenciesnotice: /Stage[2]/Hdp-yarn::Nodemanager/Hdp-yarn::Nodemanager::Create_nm_dirs[/grid/3/hadoop/yarn]/Hdp::Directory_recursive_create[/grid/3/hadoop/yarn]/Hdp::Directory[/grid/3/hadoop/yarn]/File[/grid/3/hadoop/yarn]: Dependency Exec[mkdir -p /grid/3/hadoop/yarn] has failures: truewarning: /Stage[2]/Hdp-yarn::Nodemanager/Hdp-yarn::Nodemanager::Create_nm_dirs[/grid/3/hadoop/yarn]/Hdp::Directory_recursive_create[/grid/3/hadoop/yarn]/Hdp::Directory[/grid/3/hadoop/yarn]/File[/grid/3/hadoop/yarn]: Skipping because of failed dependenciesnotice: /Stage[2]/Hdp-yarn::Initialize/Hdp-yarn::Generate_common_configs[yarn-common-configs]/Configgenerator::Configfile[capacity-scheduler]/File[/etc/hadoop/conf/capacity-scheduler.xml]/content: content changed '{md5}e5d17c21c7a5e1db9f3af35cba71df0a' to '{md5}2ca1d267a46f1aecac726caabaa16774'notice: /Stage[2]/Hdp-yarn::Initialize/Hdp-yarn::Generate_common_configs[yarn-common-configs]/Configgenerator::Configfile[capacity-scheduler]/File[/etc/hadoop/conf/capacity-scheduler.xml]/owner: owner changed 'hdfs' to 'yarn'notice: /Stage[2]/Hdp-yarn::Initialize/Hdp-yarn::Generate_common_configs[yarn-common-configs]/Configgenerator::Configfile[core-site]/File[/etc/hadoop/conf/core-site.xml]/content: content changed '{md5}86d742a780d59a957ea0a283dec03784' to '{md5}8506e4402ba8140ea4f9fed97b6f94e2'notice: /Stage[2]/Hdp-yarn::Initialize/Hdp-yarn::Generate_common_configs[yarn-common-configs]/Configgenerator::Configfile[yarn-site]/File[/etc/hadoop/conf/yarn-site.xml]/content: content changed '{md5}d84a967ce47a6b77734ed8f53d817c6e' to '{md5}42940cca6e8f64ae5de50524fb131274'notice: /Stage[2]/Hdp-yarn::Nodemanager/Hdp-yarn::Service[nodemanager]/Anchor[hdp-yarn::service::nodemanager::begin]: Dependency Exec[mkdir -p /grid/3/hadoop/yarn] has failures: truewarning: /Stage[2]/Hdp-yarn::Nodemanager/Hdp-yarn::Service[nodemanager]/Anchor[hdp-yarn::service::nodemanager::begin]: Skipping because of failed dependenciesnotice: /Stage[2]/Hdp-yarn::Nodemanager/Hdp-yarn::Service[nodemanager]/Hdp::Directory_recursive_create[/var/log/hadoop-yarn]/Hdp::Exec[mkdir -p /var/log/hadoop-yarn]/Anchor[hdp::exec::mkdir -p /var/log/hadoop-yarn::begin]: Dependency Exec[mkdir -p /grid/3/hadoop/yarn] has failures: truewarning: /Stage[2]/Hdp-yarn::Nodemanager/Hdp-yarn::Service[nodemanager]/Hdp::Directory_recursive_create[/var/log/hadoop-yarn]/Hdp::Exec[mkdir -p /var/log/hadoop-yarn]/Anchor[hdp::exec::mkdir -p /var/log/hadoop-yarn::begin]: Skipping because of failed dependenciesnotice: /Stage[2]/Hdp-yarn::Nodemanager/Hdp-yarn::Service[nodemanager]/Hdp::Directory_recursive_create[/var/log/hadoop-yarn]/Hdp::Exec[mkdir -p /var/log/hadoop-yarn]/Exec[mkdir -p /var/log/hadoop-yarn]: Dependency Exec[mkdir -p /grid/3/hadoop/yarn] has failures: truewarning: /Stage[2]/Hdp-yarn::Nodemanager/Hdp-yarn::Service[nodemanager]/Hdp::Directory_recursive_create[/var/log/hadoop-yarn]/Hdp::Exec[mkdir -p /var/log/hadoop-yarn]/Exec[mkdir -p /var/log/hadoop-yarn]: Skipping because of failed dependenciesnotice: /Stage[2]/Hdp-yarn::Nodemanager/Hdp-yarn::Service[nodemanager]/Hdp::Directory_recursive_create[/var/log/hadoop-yarn]/Hdp::Exec[mkdir -p /var/log/hadoop-yarn]/Anchor[hdp::exec::mkdir -p /var/log/hadoop-yarn::end]: Dependency Exec[mkdir -p /grid/3/hadoop/yarn] has failures: truewarning: /Stage[2]/Hdp-yarn::Nodemanager/Hdp-yarn::Service[nodemanager]/Hdp::Directory_recursive_create[/var/log/hadoop-yarn]/Hdp::Exec[mkdir -p /var/log/hadoop-yarn]/Anchor[hdp::exec::mkdir -p /var/log/hadoop-yarn::end]: Skipping because of failed dependenciesnotice: /Stage[2]/Hdp-yarn::Nodemanager/Hdp-yarn::Service[nodemanager]/Hdp::Directory_recursive_create[/var/log/hadoop-yarn]/Hdp::Directory[/var/log/hadoop-yarn]/File[/var/log/hadoop-yarn]: Dependency Exec[mkdir -p /grid/3/hadoop/yarn] has failures: truewarning: /Stage[2]/Hdp-yarn::Nodemanager/Hdp-yarn::Service[nodemanager]/Hdp::Directory_recursive_create[/var/log/hadoop-yarn]/Hdp::Directory[/var/log/hadoop-yarn]/File[/var/log/hadoop-yarn]: Skipping because of failed dependenciesnotice: /Stage[2]/Hdp-yarn::Nodemanager/Hdp-yarn::Service[nodemanager]/Hdp::Directory_recursive_create[/var/run/hadoop-yarn/yarn]/Hdp::Exec[mkdir -p /var/run/hadoop-yarn/yarn]/Anchor[hdp::exec::mkdir -p /var/run/hadoop-yarn/yarn::begin]: Dependency Exec[mkdir -p /grid/3/hadoop/yarn] has failures: truewarning: /Stage[2]/Hdp-yarn::Nodemanager/Hdp-yarn::Service[nodemanager]/Hdp::Directory_recursive_create[/var/run/hadoop-yarn/yarn]/Hdp::Exec[mkdir -p /var/run/hadoop-yarn/yarn]/Anchor[hdp::exec::mkdir -p /var/run/hadoop-yarn/yarn::begin]: Skipping because of failed dependenciesnotice: /Stage[2]/Hdp-yarn::Nodemanager/Hdp-yarn::Service[nodemanager]/Hdp::Directory_recursive_create[/var/run/hadoop-yarn/yarn]/Hdp::Exec[mkdir -p /var/run/hadoop-yarn/yarn]/Exec[mkdir -p /var/run/hadoop-yarn/yarn]: Dependency Exec[mkdir -p /grid/3/hadoop/yarn] has failures: truewarning: /Stage[2]/Hdp-yarn::Nodemanager/Hdp-yarn::Service[nodemanager]/Hdp::Directory_recursive_create[/var/run/hadoop-yarn/yarn]/Hdp::Exec[mkdir -p /var/run/hadoop-yarn/yarn]/Exec[mkdir -p /var/run/hadoop-yarn/yarn]: Skipping because of failed dependenciesnotice: /Stage[2]/Hdp-yarn::Nodemanager/Hdp-yarn::Service[nodemanager]/Hdp::Directory_recursive_create[/var/run/hadoop-yarn/yarn]/Hdp::Exec[mkdir -p /var/run/hadoop-yarn/yarn]/Anchor[hdp::exec::mkdir -p /var/run/hadoop-yarn/yarn::end]: Dependency Exec[mkdir -p /grid/3/hadoop/yarn] has failures: truewarning: /Stage[2]/Hdp-yarn::Nodemanager/Hdp-yarn::Service[nodemanager]/Hdp::Directory_recursive_create[/var/run/hadoop-yarn/yarn]/Hdp::Exec[mkdir -p /var/run/hadoop-yarn/yarn]/Anchor[hdp::exec::mkdir -p /var/run/hadoop-yarn/yarn::end]: Skipping because of failed dependenciesnotice: /Stage[2]/Hdp-yarn::Nodemanager/Hdp-yarn::Service[nodemanager]/Hdp::Directory_recursive_create[/var/run/hadoop-yarn/yarn]/Hdp::Directory[/var/run/hadoop-yarn/yarn]/File[/var/run/hadoop-yarn/yarn]: Dependency Exec[mkdir -p /grid/3/hadoop/yarn] has failures: truewarning: /Stage[2]/Hdp-yarn::Nodemanager/Hdp-yarn::Service[nodemanager]/Hdp::Directory_recursive_create[/var/run/hadoop-yarn/yarn]/Hdp::Directory[/var/run/hadoop-yarn/yarn]/File[/var/run/hadoop-yarn/yarn]: Skipping because of failed dependenciesnotice: /Stage[2]/Hdp-yarn::Nodemanager/Hdp-yarn::Service[nodemanager]/Hdp::Exec[su - yarn -c 'export HADOOP_LIBEXEC_DIR=/usr/lib/hadoop/libexec &amp;&amp; /usr/lib/hadoop-yarn/sbin/yarn-daemon.sh --config /etc/hadoop/conf start nodemanager']/Anchor[hdp::exec::su - yarn -c 'export HADOOP_LIBEXEC_DIR=/usr/lib/hadoop/libexec &amp;&amp; /usr/lib/hadoop-yarn/sbin/yarn-daemon.sh --config /etc/hadoop/conf start nodemanager'::begin]: Dependency Exec[mkdir -p /grid/3/hadoop/yarn] has failures: truewarning: /Stage[2]/Hdp-yarn::Nodemanager/Hdp-yarn::Service[nodemanager]/Hdp::Exec[su - yarn -c 'export HADOOP_LIBEXEC_DIR=/usr/lib/hadoop/libexec &amp;&amp; /usr/lib/hadoop-yarn/sbin/yarn-daemon.sh --config /etc/hadoop/conf start nodemanager']/Anchor[hdp::exec::su - yarn -c 'export HADOOP_LIBEXEC_DIR=/usr/lib/hadoop/libexec &amp;&amp; /usr/lib/hadoop-yarn/sbin/yarn-daemon.sh --config /etc/hadoop/conf start nodemanager'::begin]: Skipping because of failed dependenciesnotice: /Stage[2]/Hdp-yarn::Nodemanager/Hdp-yarn::Service[nodemanager]/Hdp::Exec[su - yarn -c 'export HADOOP_LIBEXEC_DIR=/usr/lib/hadoop/libexec &amp;&amp; /usr/lib/hadoop-yarn/sbin/yarn-daemon.sh --config /etc/hadoop/conf start nodemanager']/Exec[su - yarn -c 'export HADOOP_LIBEXEC_DIR=/usr/lib/hadoop/libexec &amp;&amp; /usr/lib/hadoop-yarn/sbin/yarn-daemon.sh --config /etc/hadoop/conf start nodemanager']: Dependency Exec[mkdir -p /grid/3/hadoop/yarn] has failures: truewarning: /Stage[2]/Hdp-yarn::Nodemanager/Hdp-yarn::Service[nodemanager]/Hdp::Exec[su - yarn -c 'export HADOOP_LIBEXEC_DIR=/usr/lib/hadoop/libexec &amp;&amp; /usr/lib/hadoop-yarn/sbin/yarn-daemon.sh --config /etc/hadoop/conf start nodemanager']/Exec[su - yarn -c 'export HADOOP_LIBEXEC_DIR=/usr/lib/hadoop/libexec &amp;&amp; /usr/lib/hadoop-yarn/sbin/yarn-daemon.sh --config /etc/hadoop/conf start nodemanager']: Skipping because of failed dependenciesnotice: /Stage[2]/Hdp-yarn::Nodemanager/Hdp-yarn::Service[nodemanager]/Hdp::Exec[su - yarn -c 'export HADOOP_LIBEXEC_DIR=/usr/lib/hadoop/libexec &amp;&amp; /usr/lib/hadoop-yarn/sbin/yarn-daemon.sh --config /etc/hadoop/conf start nodemanager']/Anchor[hdp::exec::su - yarn -c 'export HADOOP_LIBEXEC_DIR=/usr/lib/hadoop/libexec &amp;&amp; /usr/lib/hadoop-yarn/sbin/yarn-daemon.sh --config /etc/hadoop/conf start nodemanager'::end]: Dependency Exec[mkdir -p /grid/3/hadoop/yarn] has failures: truewarning: /Stage[2]/Hdp-yarn::Nodemanager/Hdp-yarn::Service[nodemanager]/Hdp::Exec[su - yarn -c 'export HADOOP_LIBEXEC_DIR=/usr/lib/hadoop/libexec &amp;&amp; /usr/lib/hadoop-yarn/sbin/yarn-daemon.sh --config /etc/hadoop/conf start nodemanager']/Anchor[hdp::exec::su - yarn -c 'export HADOOP_LIBEXEC_DIR=/usr/lib/hadoop/libexec &amp;&amp; /usr/lib/hadoop-yarn/sbin/yarn-daemon.sh --config /etc/hadoop/conf start nodemanager'::end]: Skipping because of failed dependenciesnotice: /Stage[2]/Hdp-yarn::Nodemanager/Hdp-yarn::Service[nodemanager]/Hdp::Exec[sleep 5; ls /var/run/hadoop-yarn/yarn/yarn-yarn-nodemanager.pid &gt;/dev/null 2&gt;&amp;1 &amp;&amp; ps 'cat /var/run/hadoop-yarn/yarn/yarn-yarn-nodemanager.pid' &gt;/dev/null 2&gt;&amp;1]/Anchor[hdp::exec::sleep 5; ls /var/run/hadoop-yarn/yarn/yarn-yarn-nodemanager.pid &gt;/dev/null 2&gt;&amp;1 &amp;&amp; ps 'cat /var/run/hadoop-yarn/yarn/yarn-yarn-nodemanager.pid' &gt;/dev/null 2&gt;&amp;1::begin]: Dependency Exec[mkdir -p /grid/3/hadoop/yarn] has failures: truewarning: /Stage[2]/Hdp-yarn::Nodemanager/Hdp-yarn::Service[nodemanager]/Hdp::Exec[sleep 5; ls /var/run/hadoop-yarn/yarn/yarn-yarn-nodemanager.pid &gt;/dev/null 2&gt;&amp;1 &amp;&amp; ps 'cat /var/run/hadoop-yarn/yarn/yarn-yarn-nodemanager.pid' &gt;/dev/null 2&gt;&amp;1]/Anchor[hdp::exec::sleep 5; ls /var/run/hadoop-yarn/yarn/yarn-yarn-nodemanager.pid &gt;/dev/null 2&gt;&amp;1 &amp;&amp; ps 'cat /var/run/hadoop-yarn/yarn/yarn-yarn-nodemanager.pid' &gt;/dev/null 2&gt;&amp;1::begin]: Skipping because of failed dependenciesnotice: /Stage[2]/Hdp-yarn::Nodemanager/Hdp-yarn::Service[nodemanager]/Hdp::Exec[sleep 5; ls /var/run/hadoop-yarn/yarn/yarn-yarn-nodemanager.pid &gt;/dev/null 2&gt;&amp;1 &amp;&amp; ps 'cat /var/run/hadoop-yarn/yarn/yarn-yarn-nodemanager.pid' &gt;/dev/null 2&gt;&amp;1]/Exec[sleep 5; ls /var/run/hadoop-yarn/yarn/yarn-yarn-nodemanager.pid &gt;/dev/null 2&gt;&amp;1 &amp;&amp; ps 'cat /var/run/hadoop-yarn/yarn/yarn-yarn-nodemanager.pid' &gt;/dev/null 2&gt;&amp;1]: Dependency Exec[mkdir -p /grid/3/hadoop/yarn] has failures: truewarning: /Stage[2]/Hdp-yarn::Nodemanager/Hdp-yarn::Service[nodemanager]/Hdp::Exec[sleep 5; ls /var/run/hadoop-yarn/yarn/yarn-yarn-nodemanager.pid &gt;/dev/null 2&gt;&amp;1 &amp;&amp; ps 'cat /var/run/hadoop-yarn/yarn/yarn-yarn-nodemanager.pid' &gt;/dev/null 2&gt;&amp;1]/Exec[sleep 5; ls /var/run/hadoop-yarn/yarn/yarn-yarn-nodemanager.pid &gt;/dev/null 2&gt;&amp;1 &amp;&amp; ps 'cat /var/run/hadoop-yarn/yarn/yarn-yarn-nodemanager.pid' &gt;/dev/null 2&gt;&amp;1]: Skipping because of failed dependenciesnotice: /Stage[2]/Hdp-yarn::Nodemanager/Hdp-yarn::Service[nodemanager]/Hdp::Exec[sleep 5; ls /var/run/hadoop-yarn/yarn/yarn-yarn-nodemanager.pid &gt;/dev/null 2&gt;&amp;1 &amp;&amp; ps 'cat /var/run/hadoop-yarn/yarn/yarn-yarn-nodemanager.pid' &gt;/dev/null 2&gt;&amp;1]/Anchor[hdp::exec::sleep 5; ls /var/run/hadoop-yarn/yarn/yarn-yarn-nodemanager.pid &gt;/dev/null 2&gt;&amp;1 &amp;&amp; ps 'cat /var/run/hadoop-yarn/yarn/yarn-yarn-nodemanager.pid' &gt;/dev/null 2&gt;&amp;1::end]: Dependency Exec[mkdir -p /grid/3/hadoop/yarn] has failures: truewarning: /Stage[2]/Hdp-yarn::Nodemanager/Hdp-yarn::Service[nodemanager]/Hdp::Exec[sleep 5; ls /var/run/hadoop-yarn/yarn/yarn-yarn-nodemanager.pid &gt;/dev/null 2&gt;&amp;1 &amp;&amp; ps 'cat /var/run/hadoop-yarn/yarn/yarn-yarn-nodemanager.pid' &gt;/dev/null 2&gt;&amp;1]/Anchor[hdp::exec::sleep 5; ls /var/run/hadoop-yarn/yarn/yarn-yarn-nodemanager.pid &gt;/dev/null 2&gt;&amp;1 &amp;&amp; ps 'cat /var/run/hadoop-yarn/yarn/yarn-yarn-nodemanager.pid' &gt;/dev/null 2&gt;&amp;1::end]: Skipping because of failed dependenciesnotice: /Stage[2]/Hdp-yarn::Nodemanager/Hdp-yarn::Service[nodemanager]/Anchor[hdp-yarn::service::nodemanager::end]: Dependency Exec[mkdir -p /grid/3/hadoop/yarn] has failures: truewarning: /Stage[2]/Hdp-yarn::Nodemanager/Hdp-yarn::Service[nodemanager]/Anchor[hdp-yarn::service::nodemanager::end]: Skipping because of failed dependenciesnotice: /Stage[2]/Hdp-yarn::Nodemanager/Anchor[hdp-yarn::nodemanager::end]: Dependency Exec[mkdir -p /grid/3/hadoop/yarn] has failures: truewarning: /Stage[2]/Hdp-yarn::Nodemanager/Anchor[hdp-yarn::nodemanager::end]: Skipping because of failed dependenciesnotice: /Stage[2]/Hdp-yarn::Initialize/Hdp-yarn::Generate_common_configs[yarn-common-configs]/Configgenerator::Configfile[mapred-site]/File[/etc/hadoop/conf/mapred-site.xml]/content: content changed '{md5}093cb1899b3c3b9dc4a7c1c93729c18b' to '{md5}4c462999cc47e6f6ba0e6381d71d81ba'notice: /Stage[2]/Hdp-yarn::Initialize/Hdp-yarn::Generate_common_configs[yarn-common-configs]/Configgenerator::Configfile[mapred-site]/File[/etc/hadoop/conf/mapred-site.xml]/owner: owner changed 'mapred' to 'yarn'notice: Finished catalog run in 2.39 seconds, yarn site yarn nodemanag local dir grid hadoop yarn grid hadoop yarn grid hadoop yarn grid hadoop yarn grid hadoop yarn grid hadoop yarn yarn nodemanag log dir grid hadoop yarn grid hadoop yarn grid hadoop yarn grid hadoop yarn grid hadoop yarn grid hadoop yarn now grid mount read due disk error though folder got success creat ambari start node manag process notic stage hdp snappi packag hdp snappi packag ln hdp exec hdp snappi packag ln exec hdp snappi packag ln return execut successfullynotic stage hdp yarn nodemanag hdp yarn nodemanag creat nm dir grid hadoop yarn hdp directori recurs creat grid hadoop yarn hdp exec mkdir p grid hadoop yarn exec mkdir p grid hadoop yarn return mkdir cannot creat directori grid hadoop read file systemerr stage hdp yarn nodemanag hdp yarn nodemanag creat nm dir grid hadoop yarn hdp directori recurs creat grid hadoop yarn hdp exec mkdir p grid hadoop yarn exec mkdir p grid hadoop yarn return chang notrun fail mkdir p grid hadoop yarn return instead one var lib ambari agent puppet modul hdp manifest init pp notic stage hdp yarn nodemanag hdp yarn nodemanag creat nm dir grid hadoop yarn hdp directori recurs creat grid hadoop yarn hdp exec mkdir p grid hadoop yarn anchor hdp exec mkdir p grid hadoop yarn end depend exec mkdir p grid hadoop yarn failur truewarn stage hdp yarn nodemanag hdp yarn nodemanag creat nm dir grid hadoop yarn hdp directori recurs creat grid hadoop yarn hdp exec mkdir p grid hadoop yarn anchor hdp exec mkdir p grid hadoop yarn end skip fail dependenciesnotic stage hdp yarn nodemanag hdp yarn nodemanag creat nm dir grid hadoop yarn hdp directori recurs creat grid hadoop yarn hdp directori grid hadoop yarn file grid hadoop yarn depend exec mkdir p grid hadoop yarn failur truewarn stage hdp yarn nodemanag hdp yarn nodemanag creat nm dir grid hadoop yarn hdp directori recurs creat grid hadoop yarn hdp directori grid hadoop yarn file grid hadoop yarn skip fail dependenciesnotic stage hdp yarn initi hdp yarn gener common config yarn common config configgener configfil capac schedul file etc hadoop conf capac schedul xml content content chang md e c c e db f af cba df md ca f aecac caabaa notic stage hdp yarn initi hdp yarn gener common config yarn common config configgener configfil capac schedul file etc hadoop conf capac schedul xml owner owner chang hdf yarn notic stage hdp yarn initi hdp yarn gener common config yarn common config configgener configfil core site file etc hadoop conf core site xml content content chang md ea dec md e ba ea f fed b f e notic stage hdp yarn initi hdp yarn gener common config yarn common config configgener configfil yarn site file etc hadoop conf yarn site xml content content chang md ce b ed f c e md cca e f ae de fb notic stage hdp yarn nodemanag hdp yarn servic nodemanag anchor hdp yarn servic nodemanag begin depend exec mkdir p grid hadoop yarn failur truewarn stage hdp yarn nodemanag hdp yarn servic nodemanag anchor hdp yarn servic nodemanag begin skip fail dependenciesnotic stage hdp yarn nodemanag hdp yarn servic nodemanag hdp directori recurs creat var log hadoop yarn hdp exec mkdir p var log hadoop yarn anchor hdp exec mkdir p var log hadoop yarn begin depend exec mkdir p grid hadoop yarn failur truewarn stage hdp yarn nodemanag hdp yarn servic nodemanag hdp directori recurs creat var log hadoop yarn hdp exec mkdir p var log hadoop yarn anchor hdp exec mkdir p var log hadoop yarn begin skip fail dependenciesnotic stage hdp yarn nodemanag hdp yarn servic nodemanag hdp directori recurs creat var log hadoop yarn hdp exec mkdir p var log hadoop yarn exec mkdir p var log hadoop yarn depend exec mkdir p grid hadoop yarn failur truewarn stage hdp yarn nodemanag hdp yarn servic nodemanag hdp directori recurs creat var log hadoop yarn hdp exec mkdir p var log hadoop yarn exec mkdir p var log hadoop yarn skip fail dependenciesnotic stage hdp yarn nodemanag hdp yarn servic nodemanag hdp directori recurs creat var log hadoop yarn hdp exec mkdir p var log hadoop yarn anchor hdp exec mkdir p var log hadoop yarn end depend exec mkdir p grid hadoop yarn failur truewarn stage hdp yarn nodemanag hdp yarn servic nodemanag hdp directori recurs creat var log hadoop yarn hdp exec mkdir p var log hadoop yarn anchor hdp exec mkdir p var log hadoop yarn end skip fail dependenciesnotic stage hdp yarn nodemanag hdp yarn servic nodemanag hdp directori recurs creat var log hadoop yarn hdp directori var log hadoop yarn file var log hadoop yarn depend exec mkdir p grid hadoop yarn failur truewarn stage hdp yarn nodemanag hdp yarn servic nodemanag hdp directori recurs creat var log hadoop yarn hdp directori var log hadoop yarn file var log hadoop yarn skip fail dependenciesnotic stage hdp yarn nodemanag hdp yarn servic nodemanag hdp directori recurs creat var run hadoop yarn yarn hdp exec mkdir p var run hadoop yarn yarn anchor hdp exec mkdir p var run hadoop yarn yarn begin depend exec mkdir p grid hadoop yarn failur truewarn stage hdp yarn nodemanag hdp yarn servic nodemanag hdp directori recurs creat var run hadoop yarn yarn hdp exec mkdir p var run hadoop yarn yarn anchor hdp exec mkdir p var run hadoop yarn yarn begin skip fail dependenciesnotic stage hdp yarn nodemanag hdp yarn servic nodemanag hdp directori recurs creat var run hadoop yarn yarn hdp exec mkdir p var run hadoop yarn yarn exec mkdir p var run hadoop yarn yarn depend exec mkdir p grid hadoop yarn failur truewarn stage hdp yarn nodemanag hdp yarn servic nodemanag hdp directori recurs creat var run hadoop yarn yarn hdp exec mkdir p var run hadoop yarn yarn exec mkdir p var run hadoop yarn yarn skip fail dependenciesnotic stage hdp yarn nodemanag hdp yarn servic nodemanag hdp directori recurs creat var run hadoop yarn yarn hdp exec mkdir p var run hadoop yarn yarn anchor hdp exec mkdir p var run hadoop yarn yarn end depend exec mkdir p grid hadoop yarn failur truewarn stage hdp yarn nodemanag hdp yarn servic nodemanag hdp directori recurs creat var run hadoop yarn yarn hdp exec mkdir p var run hadoop yarn yarn anchor hdp exec mkdir p var run hadoop yarn yarn end skip fail dependenciesnotic stage hdp yarn nodemanag hdp yarn servic nodemanag hdp directori recurs creat var run hadoop yarn yarn hdp directori var run hadoop yarn yarn file var run hadoop yarn yarn depend exec mkdir p grid hadoop yarn failur truewarn stage hdp yarn nodemanag hdp yarn servic nodemanag hdp directori recurs creat var run hadoop yarn yarn hdp directori var run hadoop yarn yarn file var run hadoop yarn yarn skip fail dependenciesnotic stage hdp yarn nodemanag hdp yarn servic nodemanag hdp exec su yarn c export hadoop libexec dir usr lib hadoop libexec amp amp usr lib hadoop yarn sbin yarn daemon sh config etc hadoop conf start nodemanag anchor hdp exec su yarn c export hadoop libexec dir usr lib hadoop libexec amp amp usr lib hadoop yarn sbin yarn daemon sh config etc hadoop conf start nodemanag begin depend exec mkdir p grid hadoop yarn failur truewarn stage hdp yarn nodemanag hdp yarn servic nodemanag hdp exec su yarn c export hadoop libexec dir usr lib hadoop libexec amp amp usr lib hadoop yarn sbin yarn daemon sh config etc hadoop conf start nodemanag anchor hdp exec su yarn c export hadoop libexec dir usr lib hadoop libexec amp amp usr lib hadoop yarn sbin yarn daemon sh config etc hadoop conf start nodemanag begin skip fail dependenciesnotic stage hdp yarn nodemanag hdp yarn servic nodemanag hdp exec su yarn c export hadoop libexec dir usr lib hadoop libexec amp amp usr lib hadoop yarn sbin yarn daemon sh config etc hadoop conf start nodemanag exec su yarn c export hadoop libexec dir usr lib hadoop libexec amp amp usr lib hadoop yarn sbin yarn daemon sh config etc hadoop conf start nodemanag depend exec mkdir p grid hadoop yarn failur truewarn stage hdp yarn nodemanag hdp yarn servic nodemanag hdp exec su yarn c export hadoop libexec dir usr lib hadoop libexec amp amp usr lib hadoop yarn sbin yarn daemon sh config etc hadoop conf start nodemanag exec su yarn c export hadoop libexec dir usr lib hadoop libexec amp amp usr lib hadoop yarn sbin yarn daemon sh config etc hadoop conf start nodemanag skip fail dependenciesnotic stage hdp yarn nodemanag hdp yarn servic nodemanag hdp exec su yarn c export hadoop libexec dir usr lib hadoop libexec amp amp usr lib hadoop yarn sbin yarn daemon sh config etc hadoop conf start nodemanag anchor hdp exec su yarn c export hadoop libexec dir usr lib hadoop libexec amp amp usr lib hadoop yarn sbin yarn daemon sh config etc hadoop conf start nodemanag end depend exec mkdir p grid hadoop yarn failur truewarn stage hdp yarn nodemanag hdp yarn servic nodemanag hdp exec su yarn c export hadoop libexec dir usr lib hadoop libexec amp amp usr lib hadoop yarn sbin yarn daemon sh config etc hadoop conf start nodemanag anchor hdp exec su yarn c export hadoop libexec dir usr lib hadoop libexec amp amp usr lib hadoop yarn sbin yarn daemon sh config etc hadoop conf start nodemanag end skip fail dependenciesnotic stage hdp yarn nodemanag hdp yarn servic nodemanag hdp exec sleep ls var run hadoop yarn yarn yarn yarn nodemanag pid gt dev null gt amp amp amp ps cat var run hadoop yarn yarn yarn yarn nodemanag pid gt dev null gt amp anchor hdp exec sleep ls var run hadoop yarn yarn yarn yarn nodemanag pid gt dev null gt amp amp amp ps cat var run hadoop yarn yarn yarn yarn nodemanag pid gt dev null gt amp begin depend exec mkdir p grid hadoop yarn failur truewarn stage hdp yarn nodemanag hdp yarn servic nodemanag hdp exec sleep ls var run hadoop yarn yarn yarn yarn nodemanag pid gt dev null gt amp amp amp ps cat var run hadoop yarn yarn yarn yarn nodemanag pid gt dev null gt amp anchor hdp exec sleep ls var run hadoop yarn yarn yarn yarn nodemanag pid gt dev null gt amp amp amp ps cat var run hadoop yarn yarn yarn yarn nodemanag pid gt dev null gt amp begin skip fail dependenciesnotic stage hdp yarn nodemanag hdp yarn servic nodemanag hdp exec sleep ls var run hadoop yarn yarn yarn yarn nodemanag pid gt dev null gt amp amp amp ps cat var run hadoop yarn yarn yarn yarn nodemanag pid gt dev null gt amp exec sleep ls var run hadoop yarn yarn yarn yarn nodemanag pid gt dev null gt amp amp amp ps cat var run hadoop yarn yarn yarn yarn nodemanag pid gt dev null gt amp depend exec mkdir p grid hadoop yarn failur truewarn stage hdp yarn nodemanag hdp yarn servic nodemanag hdp exec sleep ls var run hadoop yarn yarn yarn yarn nodemanag pid gt dev null gt amp amp amp ps cat var run hadoop yarn yarn yarn yarn nodemanag pid gt dev null gt amp exec sleep ls var run hadoop yarn yarn yarn yarn nodemanag pid gt dev null gt amp amp amp ps cat var run hadoop yarn yarn yarn yarn nodemanag pid gt dev null gt amp skip fail dependenciesnotic stage hdp yarn nodemanag hdp yarn servic nodemanag hdp exec sleep ls var run hadoop yarn yarn yarn yarn nodemanag pid gt dev null gt amp amp amp ps cat var run hadoop yarn yarn yarn yarn nodemanag pid gt dev null gt amp anchor hdp exec sleep ls var run hadoop yarn yarn yarn yarn nodemanag pid gt dev null gt amp amp amp ps cat var run hadoop yarn yarn yarn yarn nodemanag pid gt dev null gt amp end depend exec mkdir p grid hadoop yarn failur truewarn stage hdp yarn nodemanag hdp yarn servic nodemanag hdp exec sleep ls var run hadoop yarn yarn yarn yarn nodemanag pid gt dev null gt amp amp amp ps cat var run hadoop yarn yarn yarn yarn nodemanag pid gt dev null gt amp anchor hdp exec sleep ls var run hadoop yarn yarn yarn yarn nodemanag pid gt dev null gt amp amp amp ps cat var run hadoop yarn yarn yarn yarn nodemanag pid gt dev null gt amp end skip fail dependenciesnotic stage hdp yarn nodemanag hdp yarn servic nodemanag anchor hdp yarn servic nodemanag end depend exec mkdir p grid hadoop yarn failur truewarn stage hdp yarn nodemanag hdp yarn servic nodemanag anchor hdp yarn servic nodemanag end skip fail dependenciesnotic stage hdp yarn nodemanag anchor hdp yarn nodemanag end depend exec mkdir p grid hadoop yarn failur truewarn stage hdp yarn nodemanag anchor hdp yarn nodemanag end skip fail dependenciesnotic stage hdp yarn initi hdp yarn gener common config yarn common config configgener configfil mapr site file etc hadoop conf mapr site xml content content chang md cb b c b dc c c c b md c cc e f ba e ba notic stage hdp yarn initi hdp yarn gener common config yarn common config configgener configfil mapr site file etc hadoop conf mapr site xml owner owner chang mapr yarn notic finish catalog run second,1,0,0,0,0,0,
3076,Jobs run date and duration not sorting correctly, job run date durat sort correctli,Input and output bytes columns have been removed from the UI  but the web service doesn't know about that yet so it doesn't map the column index to a sort field correctly., input output byte column remov ui web servic know yet map column index sort field correctli,1,0,0,0,0,0,
3077,Jobs summary has 'oldest' and 'youngest' run dates swapped, job summari oldest youngest run date swap,The web service is labeling these incorrectly when they are retrieved from the db., the web servic label incorrectli retriev db,1,0,0,0,0,0,
3093,Incorrect units of measure for 'HBase Master Heap' widget on Dashboard, incorrect unit measur h base master heap widget dashboard,'HBase Master Heap' widget on Dashboard displays parameters in TB (param1.png)  but real values are in MB (param0.png)., h base master heap widget dashboard display paramet tb param png real valu mb param png,0,0,0,0,0,0,
3095,Incorrect color of rack indicators on 'Heatmaps' page, incorrect color rack indic heatmap page,Precondition: hadoop is installed. Total disk space for both machines is 100 GB. Used disk space is about 5-15%.Steps: Go to 'Heatmaps' page. Select 'Host Disk Space Used %' metric ('Maximum' input field has '100' value). Both indicators has green color (disk space usage is in 0-20% category). Choose 'Maximum' input field value to '99'.Result:One indicator has red color (79.2% - 99% category)  but real disk space usage is 8.3% and indicator should be green., precondit hadoop instal total disk space machin gb use disk space step go heatmap page select host disk space use metric maximum input field valu both indic green color disk space usag categori choos maximum input field valu result one indic red color categori real disk space usag indic green,0,0,0,0,0,0,
3099,Cannot change JMX ports using Ambari configuration API, cannot chang jmx port use ambari configur api,Ambari 1.2.5  attached the configs to the cluster  in order to provide ability for override behavior at the service and host component levels.The JMX ports read from the service configs can no longer be modified from their default values since the existing code reads the service configurations which do not exist., ambari attach config cluster order provid abil overrid behavior servic host compon level the jmx port read servic config longer modifi default valu sinc exist code read servic configur exist,1,0,0,0,0,0,
3105,Random change of blocks for Summary and Alerts and health checks on some Service pages, random chang block summari alert health check servic page,The height of blocks 'Alerts and health checks' is changed sometimes after Service page refresh.It can change to normal sizes after another tries.Produced only in Firefox., the height block alert health check chang sometim servic page refresh it chang normal size anoth tri produc firefox,0,0,0,0,0,0,
3112,Security wizard: disabling security does not return to initial condition after enabling security fails., secur wizard disabl secur return initi condit enabl secur fail,Steps to reproduce:1. enable security WITHOUT pre-configuring kerberos on cluster and see failures on '3. Start Services';2. disable security.In the end DataNode fails on ALL hosts without a possibility to get started.When you try to start DataNode manually it also ends with error:err: /Stage[2]/Hdp-hadoop::Datanode/Hdp-hadoop::Service[datanode]/Hdp::Exec[su - hdfs -c 'export HADOOP_LIBEXEC_DIR=/usr/lib/hadoop/libexec &amp;&amp; /usr/lib/hadoop/sbin/hadoop-daemon.sh --config /etc/hadoop/conf start datanode']/Exec[su - hdfs -c 'export HADOOP_LIBEXEC_DIR=/usr/lib/hadoop/libexec &amp;&amp; /usr/lib/hadoop/sbin/hadoop-daemon.sh --config /etc/hadoop/conf start datanode']/returns: change from notrun to 0 failed: su - hdfs -c 'export HADOOP_LIBEXEC_DIR=/usr/lib/hadoop/libexec &amp;&amp; /usr/lib/hadoop/sbin/hadoop-daemon.sh --config /etc/hadoop/conf start datanode' returned 1 instead of one of [0] at /var/lib/ambari-agent/puppet/modules/hdp/manifests/init.pp:479, step reproduc enabl secur without pre configur kerbero cluster see failur start servic disabl secur in end data node fail all host without possibl get start when tri start data node manual also end error err stage hdp hadoop datanod hdp hadoop servic datanod hdp exec su hdf c export hadoop libexec dir usr lib hadoop libexec amp amp usr lib hadoop sbin hadoop daemon sh config etc hadoop conf start datanod exec su hdf c export hadoop libexec dir usr lib hadoop libexec amp amp usr lib hadoop sbin hadoop daemon sh config etc hadoop conf start datanod return chang notrun fail su hdf c export hadoop libexec dir usr lib hadoop libexec amp amp usr lib hadoop sbin hadoop daemon sh config etc hadoop conf start datanod return instead one var lib ambari agent puppet modul hdp manifest init pp,0,0,0,1,0,0,
3115,HBase Read/Write request metrics seem to have changed for 2.x stack, h base read write request metric seem chang x stack,See the screenshot for HBase Cumulative Requests graph.The graph does not seem like it's displaying cumulative read/write requests  but rather showing the rate... so the label no longer makes sense.This is on the 2.0.5 stack. On the 1.x  I believe this graph would show cumulative read/write requests (which were less useful).So for 2.x stack  we should probably change the label for this graph to say Reads/Writes per Second (or whatever we are showing - need to confirm)., see screenshot h base cumul request graph the graph seem like display cumul read write request rather show rate label longer make sens thi stack on x i believ graph would show cumul read write request less use so x stack probabl chang label graph say read write per second whatev show need confirm,1,0,0,0,0,0,
3119,NullPointerException thrown while retrieving ganglia properties, null pointer except thrown retriev ganglia properti,API Call:curl -u admin:admin http://localhost:8080/api/v1/clusters/c1/services?fields=components/ServiceComponentInfo components/host_components components/host_components/HostRoles components/host_components/metrics/jvm/memHeapUsedM components/host_components/metrics/jvm/memHeapCommittedM components/host_components/metrics/mapred/jobtracker/trackers_decommissioned components/host_components/metrics/cpu/cpu_wio components/host_components/metrics/rpc/RpcQueueTime_avg_time components/host_components/metrics/flume/flume components/host_components/metrics/yarn/QueueAmbari log:20:54:57 044 WARN [qtp912472968-20] ServletHandler:514 - /api/v1/clusters/c1/servicesjava.lang.NullPointerException at org.apache.ambari.server.controller.ganglia.GangliaPropertyProvider.getRRDRequests(GangliaPropertyProvider.java:225) at org.apache.ambari.server.controller.ganglia.GangliaPropertyProvider.populateResources(GangliaPropertyProvider.java:110) at org.apache.ambari.server.controller.internal.VersioningPropertyProvider.populateResources(VersioningPropertyProvider.java:98), api call curl u admin admin http localhost api v cluster c servic field compon servic compon info compon host compon compon host compon host role compon host compon metric jvm mem heap use m compon host compon metric jvm mem heap commit m compon host compon metric mapr jobtrack tracker decommiss compon host compon metric cpu cpu wio compon host compon metric rpc rpc queue time avg time compon host compon metric flume flume compon host compon metric yarn queue ambari log warn qtp servlet handler api v cluster c servicesjava lang null pointer except org apach ambari server control ganglia ganglia properti provid get rrd request ganglia properti provid java org apach ambari server control ganglia ganglia properti provid popul resourc ganglia properti provid java org apach ambari server control intern version properti provid popul resourc version properti provid java,0,0,0,0,0,0,
3124,Incorrect behavior after entering wrong current password while editing user;, incorrect behavior enter wrong current password edit user,1. Go to Admin page -&gt; Users tab2. Click Edit for some user3. Try to enter wrong value for Current Password field.The message does not tell me that my entered current password is wrong  so it can be confusing for user.Another scenario:1. Go to Admin page -&gt; Users tab2. Click Edit for some user3. Try to enter wrong value for Current Password field and don't fill fields for a new passwordIt don't really change the password but it still allows to enter wrong Current Password without any message, go admin page gt user tab click edit user tri enter wrong valu current password field the messag tell enter current password wrong confus user anoth scenario go admin page gt user tab click edit user tri enter wrong valu current password field fill field new password it realli chang password still allow enter wrong current password without messag,1,0,0,0,0,0,
3135,Out of memory issues with Request API on large cluster, out memori issu request api larg cluster,Number of ExecutionCommandEntity objects keep growing and result in Out of memory on large cluster (100 nodes).Script to re-create the issue:&#91;root@domain user&#93;# cat test1.shfor i in{0..100}doecho 'doing $i'curl -u admin:admin 'http://domain.net:8080/api/v1/clusters/c1/requests?to=end&amp;page_size=10&amp;fields= tasks/Tasks/*' &gt; /dev/nullsleep 5done, number execut command entiti object keep grow result out memori larg cluster node script creat issu root domain user cat test shfor doecho curl u admin admin http domain net api v cluster c request end amp page size amp field task task gt dev nullsleep done,1,0,0,0,1,0,
3136,Reduce the size of ExecutionCommand entity, reduc size execut command entiti,Currently  each ExecutionCommandEntity stores the whole config blob. This is a severe duplication of data as tagged configuration information is already available as immutable entry.We need to reduce the footprint of ExecutionCommandEntity by storing only configuration tags. The tags can be replaced with actual configuration value when the command is handed off to the agent.We need to ensure that when Ambari is upgraded and there is a mix of ExecutionCommandEntity instances with and without embedded config - it works., current execut command entiti store whole config blob thi sever duplic data tag configur inform alreadi avail immut entri we need reduc footprint execut command entiti store configur tag the tag replac actual configur valu command hand agent we need ensur ambari upgrad mix execut command entiti instanc without embed config work,1,0,0,0,0,0,
3145,ambari-agent service script should return non-zero when the agent is not running, ambari agent servic script return non zero agent run,The ambari-agent service script should return non-zero when the agent is not running. For example  if a customer wants to have puppet ensure the service is always running  it will not start a killed service because it thinks it's already running when it returns 0.&#91;root@host-123-123-123 init.d&#93;# service ambari-agent statusambari-agent currently not runningUsage: /usr/sbin/ambari-agent {start|stop|restart|status}&#91;root@host-123-123-123 init.d&#93;# echo $?0For comparison...&#91;root@host-123-123-123 init.d&#93;# service winbind statuswinbindd is stopped&#91;root@host-123-123-123 init.d&#93;# echo $?3Possible fix:AMBARI_AGENT_PID_PATH='/var/run/ambari-agent/ambari-agent.pid';RES='3';if [ -f $AMBARI_AGENT_PID_PATH ]then RES='cat $AMBARI_AGENT_PID_PATH | xargs ps -f -p | wc -l'; AMBARI_AGENT_PID='cat $AMBARI_AGENT_PID_PATH';else RES=-1;fiif [ $RES -eq '2' ]then echo 'OK: Ambari agent is running &#91;PID:$AMBARI_AGENT_PID&#93;'; exit 0;else echo 'CRITICAL: Ambari agent is not running &#91;$AMBARI_AGENT_PID_PATH not found&#93;'; exit 2;fi, the ambari agent servic script return non zero agent run for exampl custom want puppet ensur servic alway run start kill servic think alreadi run return root host init servic ambari agent statusambari agent current run usag usr sbin ambari agent start stop restart statu root host init echo for comparison root host init servic winbind statuswinbindd stop root host init echo possibl fix ambari agent pid path var run ambari agent ambari agent pid re f ambari agent pid path re cat ambari agent pid path xarg ps f p wc l ambari agent pid cat ambari agent pid path els re fiif re eq echo ok ambari agent run pid ambari agent pid exit els echo critic ambari agent run ambari agent pid path found exit fi,1,0,0,0,0,0,
3147,Modify ganglia config to match the data resolution of the older version, modifi ganglia config match data resolut older version,New ganglia version retains almost 50 times more data for higher resolution metrics collection. Ambari needs to modify the configuration to match the older resolution as the new default config has a very high disk space requirement.Looks like the default config for ganglia changed fromRRAs 'RRA:AVERAGE:0.5:1:244' 'RRA:AVERAGE:0.5:24:244' 'RRA:AVERAGE:0.5:168:244' 'RRA:AVERAGE:0.5:672:244' 'RRA:AVERAGE:0.5:5760:374'toRRAs 'RRA:AVERAGE:0.5:1:5856' 'RRA:AVERAGE:0.5:4:20160' 'RRA:AVERAGE:0.5:40:52704'Its an increase from 1350 data points to 78720. After reverting to older configuration the file size for a single metrics and its summary info are-rw-rw-rw- 1 nobody nobody 12224 Sep 7 18:29 ./HDPNameNode/c6402.ambari.apache.org/disk_free.rrd-rw-rw-rw- 1 nobody nobody 23656 Sep 7 18:29 ./HDPNameNode/__SummaryInfo__/disk_free.rrdIn contrast it was-rw-r--r-- 1 root root 630768 Sep 7 17:48 /tmp/rrds/HDPNameNode/c6402.ambari.apache.org/disk_free.rrd-rw-r--r-- 1 root root 1261000 Sep 7 17:47 /tmp/rrds/HDPNameNode/__SummaryInfo__/disk_free.rrdThe recommendation is to revert back to the old config (i.e. do not use the new default config). Confirming that with an older installation of Ambari., new ganglia version retain almost time data higher resolut metric collect ambari need modifi configur match older resolut new default config high disk space requir look like default config ganglia chang rr as rra averag rra averag rra averag rra averag rra averag rr as rra averag rra averag rra averag it increas data point after revert older configur file size singl metric summari info rw rw rw nobodi nobodi sep hdp name node c ambari apach org disk free rrd rw rw rw nobodi nobodi sep hdp name node summari info disk free rrd in contrast rw r r root root sep tmp rrd hdp name node c ambari apach org disk free rrd rw r r root root sep tmp rrd hdp name node summari info disk free rrd the recommend revert back old config e use new default config confirm older instal ambari,1,0,0,0,0,0,
3148,Oozie install fails with 'could not connect to database' when choosing 'use existing mysql database ' and choosing 'new mysql database' for Hive within Ambari, oozi instal fail could connect databas choos use exist mysql databas choos new mysql databas hive within ambari,PROBLEM: When installing with Ambari and selecting an existing MySQL database for oozie and a new MySQL database for Hive  then the Ambariinstall fails when installing the oozie database. The error thrown is 'could not connect to database' [it appears to drop the 'existing' databaseand you are required to start MySQL on the machine and create the database]BUSINESS IMPACT: This will affect all customers who choose an existing MySQL database for oozie and a new MySQL DB for hive when installing a cluster using AmbariSTEPS TO REPRODUCE: Choose an exisiting oozie MySQL database and point Ambari at this  and select a new Hive MySQL database on the installation optionsACTUAL BEHAVIOR: It seems that it drops the exisiting oozie database and then fails with an errot of could not connect (This is due to MySQL being down  but also the Oozie database is not there anymore)After starting MySQL and creating the database then the installer can continue from where it left off.EXPECTED BEHAVIOR: The installer should not stop MySQL and drop the oozie database if you select create a new Hive database and exisiting MySQL database for oozie.SUPPORT ANALYSIS: Reproduced in the lab in HDP 1.3.2 by following the steps above., problem when instal ambari select exist my sql databas oozi new my sql databas hive ambariinstal fail instal oozi databas the error thrown could connect databas appear drop exist databaseand requir start my sql machin creat databas busi impact thi affect custom choos exist my sql databas oozi new my sql db hive instal cluster use ambari step to reproduc choos exisit oozi my sql databas point ambari select new hive my sql databas instal option actual behavior it seem drop exisit oozi databas fail errot could connect thi due my sql also oozi databas anymor after start my sql creat databas instal continu left expect behavior the instal stop my sql drop oozi databas select creat new hive databas exisit my sql databas oozi support analysi reproduc lab hdp follow step,0,0,0,0,0,0,
3151,ambari-server command output should point to Apache Ambari documentation, ambari server command output point apach ambari document,ambari-server command outputs should point to a generic link for current ambari documentation., ambari server command output point gener link current ambari document,1,0,0,0,0,0,
3153,Secure cluster: Yarn service check fails after configuring yarn for spnego authentication., secur cluster yarn servic check fail configur yarn spnego authent,Yarn smoke test uses REST api exposed by ResourceManager to get its status. After configuring web authentication yarn client that is assigned yarn service check needs to negotiate 401 HTTP authentication response received while using REST api., yarn smoke test use rest api expos resourc manag get statu after configur web authent yarn client assign yarn servic check need negoti http authent respons receiv use rest api,1,0,0,0,0,0,
3154,ZKFailoverController should be shown as a component that can be started/stopped in Host Details page, zk failov control shown compon start stop host detail page,,,0,0,0,0,0,0,
3160,WebHCat alert does not nave any description, web h cat alert nave descript,Steps to reproduce1. Go to Services page2. click on different services. They all have a status and a message for status in 'Alerts and Health Checks' list (as example hive.png)3. WebHCat service has a status but does not have a status message, step reproduc go servic page click differ servic they statu messag statu alert health check list exampl hive png web h cat servic statu statu messag,0,0,0,0,0,0,
3191,Cannot delete a stopped host_component in INSTALLED state, cannot delet stop host compon instal state,I have a host with 4 stopped host_components. When I issue a DELETE on say http://c6401:8080/api/v1/clusters/vmc/hosts/c6404.ambari.apache.org/host_components/DATANODEThe response is:{ 'status' : 500  'message' : 'org.apache.ambari.server.controller.spi.SystemException: An internal system exception occurred: To remove master or slave components they must be in MAINTENANCE/INIT/INSTALL_FAILED/UNKNOWN state. Current=INSTALLED.'}, i host stop host compon when i issu delet say http c api v cluster vmc host c ambari apach org host compon datanod the respons statu messag org apach ambari server control spi system except an intern system except occur to remov master slave compon must mainten init instal fail unknown state current instal,1,0,0,0,0,0,
3198,ambari-server reset is broken on centos5.8, ambari server reset broken cento,ambari-server reset fails due to syntax error in centos 5.8 (postgres 8.1).psql:/var/lib/ambari-server/resources/Ambari-DDL-Postgres-DROP.sql:18: LINE 1: DROP DATABASE IF EXISTS ambari;The IF EXISTS clause was only added to DROP command in PotgreSQL8.2.+Show warnings if any SQL commands failed during server reset or upgradestack, ambari server reset fail due syntax error cento postgr psql var lib ambari server resourc ambari ddl postgr drop sql line drop databas if exist ambari the if exist claus ad drop command potgr sql show warn sql command fail server reset upgradestack,1,0,0,0,0,0,
3221,NameNode Uptime does not appear, name node uptim appear,Due to backend change of the position of NameNode startTime property  this will not show up., due backend chang posit name node start time properti show,1,0,0,0,0,0,
3229,Ambari does not set the correct value for 'templeton.storage.class' in webhcat-site.xml, ambari set correct valu templeton storag class webhcat site xml,Ambari does not set the correct value for 'templeton.storage.class' in webhcat-site.xmlIn an Ambari deployed cluster currently the following value in /etc/hcatalog/conf/webhcat-site.xml is set incorrectly: &lt;property&gt; &lt;name&gt;templeton.storage.class&lt;/name&gt; &lt;value&gt;org.apache.hcatalog.templeton.tool.ZooKeeperStorage&lt;/value&gt; &lt;/property&gt;With the change from HIVE-4895 this should be: &lt;property&gt; &lt;name&gt;templeton.storage.class&lt;/name&gt; &lt;value&gt;org.apache.hive.hcatalog.templeton.tool.ZooKeeperStorage&lt;/value&gt; &lt;/property&gt;All jobs involving MapReduce fail because of this issue., ambari set correct valu templeton storag class webhcat site xml in ambari deploy cluster current follow valu etc hcatalog conf webhcat site xml set incorrectli lt properti gt lt name gt templeton storag class lt name gt lt valu gt org apach hcatalog templeton tool zoo keeper storag lt valu gt lt properti gt with chang hive lt properti gt lt name gt templeton storag class lt name gt lt valu gt org apach hive hcatalog templeton tool zoo keeper storag lt valu gt lt properti gt all job involv map reduc fail issu,1,0,0,0,0,0,
3240,URLStreamProvider reads are flooding the log, url stream provid read flood log,These logs do not need to be at INFO level. When user is using the UI then theer is roughly one log per second.06:56:34 232 INFO [pool-1-thread-12] URLStreamProvider:81 - readFrom spec:http://c6402.ambari.apache.org:50075/jmx06:56:34 235 INFO [pool-1-thread-5] URLStreamProvider:81 - readFrom spec:http://c6401.ambari.apache.org:50075/jmx06:56:34 248 INFO [qtp1620999494-18] URLStreamProvider:81 - readFrom spec:http://c6401.ambari.apache.org/cgi-bin/rrd.py?c=HDPSlaves&amp;h=c6401.ambari.apache.org c6402.ambari.apache.org&amp;m=cpu_wio jvm.JvmMetrics.MemHeapUsedM rpc.rpc.RpcQueueTimeAvgTime jvm.JvmMetrics.MemHeapCommittedM&amp;e=now&amp;pt=true..., these log need info level when user use ui theer roughli one log per second info pool thread url stream provid read from spec http c ambari apach org jmx info pool thread url stream provid read from spec http c ambari apach org jmx info qtp url stream provid read from spec http c ambari apach org cgi bin rrd py c hdp slave amp h c ambari apach org c ambari apach org amp cpu wio jvm jvm metric mem heap use m rpc rpc rpc queue time avg time jvm jvm metric mem heap commit m amp e amp pt true,0,0,0,0,0,0,
3251,When Bind DN credentials are incorrect - we should log it, when bind dn credenti incorrect log,When integrating Ambari with LDAP if you specify the Bind DN  or Bind credentials that are invalid there is no logging to identify that the authentication fails  so the following search for the logging in user DN will fail. I had to use wireshark to figure out why the integration wasn't working., when integr ambari ldap specifi bind dn bind credenti invalid log identifi authent fail follow search log user dn fail i use wireshark figur integr work,1,0,0,0,0,0,
3252,Setup the krb5 Jaas configuration using 'ambari-server setup-security', setup krb jaa configur use ambari server setup secur,1. Add 'ambari-server setup-security' to replace all of the following operations:'setup-https|setup-ganglia-https|setup-nagios-https|encrypt-passwords'2. Add new operation 'setup-kerberos-auth' to ask the user for: ambari.keytab ambari.principalRelated to  https://issues.apache.org/jira/browse/AMBARI-2941, add ambari server setup secur replac follow oper setup http setup ganglia http setup nagio http encrypt password add new oper setup kerbero auth ask user ambari keytab ambari princip relat http issu apach org jira brows ambari,1,0,0,1,0,0,
3255,Read-only views of security admin tab became editable after visiting other tabs on admin page, read view secur admin tab becam edit visit tab admin page,STR: Go to Admin page -&gt; Security tab Switch to another tab on admin page (Misc tab  HA tab  etc.) Go back to Security tab--------------------Verify that all input fields on this page are read-onlyExpected Result: Everything should be read-only.Actual Result: Some fields become editable ., str go admin page gt secur tab switch anoth tab admin page misc tab ha tab etc go back secur tab verifi input field page read expect result everyth read actual result some field becom edit,0,0,0,1,0,0,
3258,Provide UI page to enable/disable experimental functionality, provid ui page enabl disabl experiment function,We have various experimental functionality provided in UI. We need an easy UI page http://server:8080/#/experimental to enable/disable these functionalities.This will make it easy for users to test these functionalities and give feedback.Refresh of Ambari UI will clear the changes., we variou experiment function provid ui we need easi ui page http server experiment enabl disabl function thi make easi user test function give feedback refresh ambari ui clear chang,1,0,0,0,0,0,
3260,Fix text for custom JCE policy setup, fix text custom jce polici setup,Current text:-c JCE_POLICY  --jce-policy=JCE_POLICY Use specified jce_policy. Must be valid on all hostsThis is required only on ambari-server  the agents will download from the server., current text c jce polici jce polici jce polici use specifi jce polici must valid host thi requir ambari server agent download server,1,0,0,0,0,0,
3261,Cleanup UX for advanced database in 'ambari-server setup', cleanup ux advanc databas ambari server setup,Cleanup UX...Enter advanced database configuration [y/n] (n)? y==============================================================================Choose one of the following options:[1] - PostgreSQL (Embedded)[2] - Oracle==============================================================================Enter choice (1):, cleanup ux enter advanc databas configur n n choos one follow option postgr sql embed oracl enter choic,1,0,0,0,0,0,
3276,Dashboard page: buttons are shifted if screen width is more than 1200px, dashboard page button shift screen width px,,,0,0,0,0,0,0,
3279,Strange behavior of 'JobTracker CPU WIO' dashboard widget, strang behavior job tracker cpu wio dashboard widget,This happened because when fixing an old issue  jobTrackerCpu got ignored about that fix., thi happen fix old issu job tracker cpu got ignor fix,1,0,0,0,0,1,
3285,Help Text for NameService ID when enabling HA is random in responding to mouse movement and clicks., help text name servic id enabl ha random respond mous movement click,,,0,0,0,0,0,0,
3292,Security wizard: On NameNode HA mode  General category should have spnego principal and keytab field, secur wizard on name node ha mode gener categori spnego princip keytab field,Earlier dfs.web.authentication.kerberos.keytab field was being used for NameNode and SNameNode component. So we planned to pull this key to NameNode category when HA is enabled as it's the only component then using the key.After HDFS-5091 fix  journalNode also uses this key.So instead of pulling this config key in NameNode section  it should be kept in General category and the description of this principal and keytab location field should be changed accordingly., earlier df web authent kerbero keytab field use name node s name node compon so plan pull key name node categori ha enabl compon use key after hdf fix journal node also use key so instead pull config key name node section kept gener categori descript princip keytab locat field chang accordingli,0,0,0,0,0,0,
3296,SSH key in logs?, ssh key log,Notice after agent registration (automatic with SSH key)  I see the SSH key in the popup. Also see it in the ambari-agent.log.Not sure we want to capture this in the log and show in the UI?, notic agent registr automat ssh key i see ssh key popup also see ambari agent log not sure want captur log show ui,1,0,0,0,0,0,
3301,Unavailable stacks should be hidden., unavail stack hidden,Unavailable stacks should be hidden., unavail stack hidden,1,0,0,0,0,0,
3302,Parameterize the repo url for latest stack, parameter repo url latest stack,Allow the latest stack repo url to be parameterized for the build. This allows the build to cater to the stack repo as it goes through dev/private/public builds each with different URLs., allow latest stack repo url parameter build thi allow build cater stack repo goe dev privat public build differ ur ls,1,0,0,0,0,0,
3304,Nagios alert text for NodeManagers should say 'live', nagio alert text node manag say live,Services &gt; YARNSee screen shot.Should say 'Percent NodeManagers live', servic gt yarn see screen shot should say percent node manag live,1,0,0,0,0,0,
3307,Fix Unit tests and create new test for step3 installer, fix unit test creat new test step instal,,,1,0,0,0,0,0,
3310,HDFS service check should not be disabled when NN HA is enabled and one NN is down, hdf servic check disabl nn ha enabl one nn,HDFS service check should not be disabled when NN HA is enabled and one NN is down. In fact  service check passing is an indication that NN is available with one NN down., hdf servic check disabl nn ha enabl one nn in fact servic check pass indic nn avail one nn,1,0,0,0,0,0,
3315,Security wizard: 'Create Principals and Keytabs' step doesn't save state after page refresh, secur wizard creat princip keytab step save state page refresh,,,1,0,0,1,0,0,
3318,Use correct case for YARN, use correct case yarn,,,1,0,0,0,0,0,
3324,UI optimization: constrain hostComponents model loading, ui optim constrain host compon model load,Constrain loading hostComponents into model  so then it loads only on initial loading or when added new hostComponents., constrain load host compon model load initi load ad new host compon,1,0,0,0,0,0,
3328,Unit test for agents fail/hang at TestActionQueue and TestStackUpgrade., unit test agent fail hang test action queue test stack upgrad,Running mvn test on agent fails/hangs under Mac Os, run mvn test agent fail hang mac os,1,0,0,0,0,0,
3332,switching to Configs tab causes Quick Links to disappear, switch config tab caus quick link disappear,In Ambari Web  browse to HDFS  YARN  MapReduce2  etc. Click on the Configs tab  the Quick Links option disappears.See the same regardless of 1.3.2 or 2.0.6  and tried on Firefox and Chrome, in ambari web brows hdf yarn map reduc etc click config tab quick link option disappear see regardless tri firefox chrome,1,0,0,0,0,0,
3333,'Services'  'Dashboard' 'Navigation errors, servic dashboard navig error,Navigation from Hosts to Services page or from Hosts to Dashboard pagesometimes fails (nothing happens besides highlighting 'Services' tab) and sometimes navigates to an empty page.Uncaught Error: assertion failed: calling set on destroyed object ember-latest.js:43Ember.assert ember-latest.js:43set ember-latest.js:1386Ember.Observable.Ember.Mixin.create.set ember-latest.js:7769App.MainServiceMenuView.Em.CollectionView.extend.renderOnRoute menu.js:52invokeAction ember-latest.js:3174iterateSet ember-latest.js:3156sendEvent ember-latest.js:3273notifyObservers ember-latest.js:1865Ember.notifyObservers ember-latest.js:1980propertyDidChange ember-latest.js:2613set ember-latest.js:1419(anonymous function) ember-latest.js:10459f.event.dispatch jquery-1.7.2.min.js:3h.handle.i jquery-1.7.2.min.js:3, navig host servic page host dashboard pagesometim fail noth happen besid highlight servic tab sometim navig empti page uncaught error assert fail call set destroy object ember latest js ember assert ember latest js set ember latest js ember observ ember mixin creat set ember latest js app main servic menu view em collect view extend render on rout menu js invok action ember latest js iter set ember latest js send event ember latest js notifi observ ember latest js ember notifi observ ember latest js properti did chang ember latest js set ember latest js anonym function ember latest js f event dispatch jqueri min js h handl jqueri min js,1,0,0,0,0,0,
3336,HDFS health status when HA config'd, hdf health statu ha config,When NameNode HA is config'd:1) HDFS status green if and only if there is Active NameNode; red otherwise2) green -&gt; HDFS Service Stop enabled  HDFS Service Start disabled. red -&gt; HDFS Service Start enabled  HDFS Service Stop disabled, when name node ha config hdf statu green activ name node red otherwis green gt hdf servic stop enabl hdf servic start disabl red gt hdf servic start enabl hdf servic stop disabl,1,0,0,0,0,0,
3337,When invalid jce policy file path is specified  ambari-server setup silently switches over to downloading the file from public repo, when invalid jce polici file path specifi ambari server setup silent switch download file public repo,When an non-existent file is provided as jce-policy parameter we should get ambari-server setup process failed instead of downloading the file from public repo.Also  if the path is a folder then we should gracefully error out instead of allowing shutil.copy to fail with an error stack., when non exist file provid jce polici paramet get ambari server setup process fail instead download file public repo also path folder grace error instead allow shutil copi fail error stack,1,0,0,0,0,0,
3350,ambari-agent RPM claims ownership of /usr/sbin, ambari agent rpm claim ownership usr sbin,*This also affects trunk*The ambari-agent.spec (generated from rpm-maven-plugin) claims ownership of /usr/sbin $ grep sbin target/rpm/ambari-agent/SPECS/ambari-agent.spec | grep attr%attr(755 root root) /usr/sbinThis is a problem because the filesystem RPM owns /usr/sbin.According to rpm-maven-plugin documentation&#91;0&#93;  this is because the only file under /usr/sbin is ambari-agent and'directoryIncludedIf the value is true then the attribute string will be written for the directory if the sources identify all of the files in the directory (that is  no other mapping contributed files to the directory). This is the default behavior.'The 'no other mapping contributed files to the directory' bit is important.The solution is to add directoryInclude=false to the mapping.&#91;0&#93; http://mojo.codehaus.org/rpm-maven-plugin/map-params.html, thi also affect trunk the ambari agent spec gener rpm maven plugin claim ownership usr sbin grep sbin target rpm ambari agent spec ambari agent spec grep attr attr root root usr sbin thi problem filesystem rpm own usr sbin accord rpm maven plugin document file usr sbin ambari agent directori includ if valu true attribut string written directori sourc identifi file directori map contribut file directori thi default behavior the map contribut file directori bit import the solut add directori includ fals map http mojo codehau org rpm maven plugin map param html,1,0,0,0,0,0,
3356,wrong property name for https address of NN in hdfs-site.xml, wrong properti name http address nn hdf site xml,The property which defines the https address of namenode is dfs.namenode.https-address. In ambari   the property name is mentioned as 'dfs.https.namenode.https-address', the properti defin http address namenod df namenod http address in ambari properti name mention df http namenod http address,1,0,0,0,0,0,
3362,Modify the config mappings in the upgrade script to reflect the latest, modifi config map upgrad script reflect latest,Modify the upgrade mappings for hdfs-site  core-site  mapred-site  and global to reflect the latest., modifi upgrad map hdf site core site mapr site global reflect latest,1,0,0,0,0,0,
3363,wrong path being set for JSVC_HOME on suse OS in hadoop-env.sh., wrong path set jsvc home suse os hadoop env sh,JSVC_HOME is being set to /usr/lib/hadoop/sbin/Linux-amd64-64/ instead of /usr/lib/bigtop-utils., jsvc home set usr lib hadoop sbin linux amd instead usr lib bigtop util,1,0,0,0,0,0,
3397,Deploy progress bar is not correct, deploy progress bar correct,All ajax-requests are completed  but progress bar doesn't filled to 100%., all ajax request complet progress bar fill,1,0,0,0,0,0,
3398,The number of alerts in MapReduce item in menu gets out on the next line., the number alert map reduc item menu get next line,STD:Make the browser window's width less than actual width of the page.Go to Services page.Stop all services.Result:The number of alerts in MapReduce item in menu gets out on the next line., std make browser window width less actual width page go servic page stop servic result the number alert map reduc item menu get next line,1,0,0,0,0,0,
3402,ambari-server setup silently fails when it cannot connect to the remote oracle host, ambari server setup silent fail cannot connect remot oracl host,The oracle db was installed on a host where port 1521 was not accessible to the ambari-server host. However  'ambari-server setup' did not report failure.Enter advanced database configuration [y/n] (n)? y==============================================================================Choose one of the following options:[1] - PostgreSQL (Embedded)[2] - Oracle==============================================================================Enter choice (1): 2Hostname (localhost): test-sm1.iad1Port (1521):Select Oracle identifier type:1 - Service Name2 - SID(1):Service Name (ambari): XEUsername (ambari):Enter Database Password (bigdata):Copying JDBC drivers to server resources...Configuring remote database connection properties...Copying JDBC drivers to server resources...Ambari Server 'setup' completed successfully., the oracl db instal host port access ambari server host howev ambari server setup report failur enter advanc databas configur n n choos one follow option postgr sql embed oracl enter choic hostnam localhost test sm iad port select oracl identifi type servic name sid servic name ambari xe usernam ambari enter databas password bigdata copi jdbc driver server resourc configur remot databas connect properti copi jdbc driver server resourc ambari server setup complet success,1,0,0,0,0,0,
3433,Add hcat.bin to pig.properties for hcat integration, add hcat bin pig properti hcat integr,Add hcat.bin to pig.properties for hcat integration.hcat.bin=/usr/bin/hcat, add hcat bin pig properti hcat integr hcat bin usr bin hcat,1,0,0,0,0,0,
3434,On 2.x stack  dfs.block.local-path-access.user should not be set in hdfs-site, on x stack df block local path access user set hdf site,,,1,0,0,0,0,0,
3435,YARN cluster should not have shared directories between yarn.nodemanager.local-dirs and yarn.nodemanager.log-dirs, yarn cluster share directori yarn nodemanag local dir yarn nodemanag log dir,,,1,0,0,1,0,0,
3437,Incorrect alert for NodeManager, incorrect alert node manag,Error with RM nagios alert for rpc latency.[1380746845] SERVICE NOTIFICATION: nagiosadmin;&lt;hostaname&gt;;RESOURCEMANAGER::ResourceManager RPC latency;CRITICAL;notify-service-by-email;CRITICAL: Data inaccessible  Status code = 200, error rm nagio alert rpc latenc servic notif nagiosadmin lt hostanam gt resourcemanag resourc manag rpc latenc critic notifi servic email critic data inaccess statu code,1,0,0,0,0,0,
3443,'Assign Slaves and Clients' step. 'all | none' click error, assign slave client step none click error,Clicking on 'all|none' affects disabled checkboxes., click none affect disabl checkbox,1,0,0,0,0,0,
3446,When SSL is enabled on Hadoop JMX endpoints ResourceManager quick links become unavailable, when ssl enabl hadoop jmx endpoint resourc manag quick link becom unavail,When hadoop.ssl.enabled=true  ResourceManager port is 8090. When it is false  the port is still 8088., when hadoop ssl enabl true resourc manag port when fals port still,1,0,0,0,0,0,
3456,Text of installation stage doesn't correspond to reality, text instal stage correspond realiti,STD:On the latter stages of installing cluster  refresh page 'Install  Start and Test'..Result:Appeared 'Next' button and progress of installation is setted to 100%  but message of installation on the second host says that the Nagios Server is not installed yet. After clicking 'Next' all seems good and Nagios Server is installed normally., std on latter stage instal cluster refresh page instal start test result appear next button progress instal set messag instal second host say nagio server instal yet after click next seem good nagio server instal normal,1,0,0,0,0,0,
3457,When multiple MR2 Clients are installed  the label is a bit off, when multipl mr client instal label bit,When multiple MR2 Clients are installed  the MR2 summary panel has a label like: '3 MapReduce2 Client s Installed' (with an unnecessary space), when multipl mr client instal mr summari panel label like map reduc client instal unnecessari space,1,0,0,0,0,0,
3477,JavaScript errors during service tab changing, java script error servic tab chang,Steps:Open browser console.Run start or stop operation for any service.Switch between some services repeatedly.Result:'Calling set on destroyed view' was appeared in console., step open browser consol run start stop oper servic switch servic repeatedli result call set destroy view appear consol,1,0,0,0,0,0,
3488,Status does not show up for newly added hosts, statu show newli ad host,The problem is that host can have actual status only when service mapper recieve response(could be long latency  about 10 - 12 seconds) with new hostComponents and then status mapper compute them and set status to host., the problem host actual statu servic mapper reciev respons could long latenc second new host compon statu mapper comput set statu host,1,0,0,0,0,0,
3490,Remove RCO management logic at ambari-agent, remov rco manag logic ambari agent,In 1.5.0 release  ambari-agent will not need to process RCO to re-order/parallelizing tasks. Let's remove the code/unit-test and keep them aside in a JIRA targeted for release after 1.5.0. If possible  let's remove upgrade related code as well as there is no plan for automatic stack upgrade for Baikal. We can keep the python executor as there is a requirement for python executor., in releas ambari agent need process rco order parallel task let remov code unit test keep asid jira target releas if possibl let remov upgrad relat code well plan automat stack upgrad baikal we keep python executor requir python executor,1,0,0,0,0,0,
3491,HBase Master/RegionServer can no longer be started after reconfiguring HBase or HDFS with NameNode HA enabled, h base master region server longer start reconfigur h base hdf name node ha enabl,,,1,0,0,0,0,0,
3498,Hbase secure config properties in HDP-2.x stack revert back to non-secure values on reconfiguration, hbase secur config properti hdp x stack revert back non secur valu reconfigur,,,1,0,0,0,0,0,
3507,'Assign Slaves' step. Error with installed NodeManagers, assign slave step error instal node manag,Installed NodeManagers don't appears as selected checkboxes on the 'Assign Slaves' step., instal node manag appear select checkbox assign slave step,0,0,0,0,0,0,
3512,Nagios doesn't start after upgrade [SLES11  1.3.2->2.0.6], nagio start upgrad sle,check_cpu needs to be disabled for Suse for JobHistory server and ResourceManager, check cpu need disabl suse job histori server resourc manag,0,0,0,0,0,0,
3521,Incorrect status counters on cluster deploy, incorrect statu counter cluster deploy,In host stauts filter label shows incorrect number of hosts after deploy failed.Wrong progress bar color  on fail color should be red or yellow  instead of blue., in host staut filter label show incorrect number host deploy fail wrong progress bar color fail color red yellow instead blue,0,0,0,0,0,0,
3528,DB url isn't calculated automatically, db url calcul automat,Select Hive or Oozie and go to step 'Customize services'.'Open' Oozie tab.Database Url is 'jdbc'.Click 'Existing MySQL Database'.Click 'New Derby Database'.Database Url became jdbc:derby:${oozie.data.dir}/${oozie.db.schema.name}-db;create=true.Expect:proper value should be right after step is loaded., select hive oozi go step custom servic open oozi tab databas url jdbc click exist my sql databas click new derbi databas databas url becam jdbc derbi oozi data dir oozi db schema name db creat true expect proper valu right step load,0,0,0,0,0,0,
3534,Hadoop Core Health Check script needs to be included in Ambari HDP installations, hadoop core health check script need includ ambari hdp instal,,,0,0,0,0,0,0,
3535,skip 'Customize Services' step for services that can't be customized, skip custom servic step servic custom,Services like PIG  Sqoop can't be customized.Wizard should check services-list (that user want to add) and if no one service can't be customized  should skip 'Customize' step ('Back' click on the next step should also be verified)., servic like pig sqoop custom wizard check servic list user want add one servic custom skip custom step back click next step also verifi,0,0,0,0,0,0,
3537,Allow log4j properties to be applied via the API in Ambari for hadoop/oozie/hbase/hive/zookeeper/pig, allow log j properti appli via api ambari hadoop oozi hbase hive zookeep pig,Allow log4j properties to be applied via the API in Ambari for hadoop/oozie/hbase/hive/zookeeper/pig., allow log j properti appli via api ambari hadoop oozi hbase hive zookeep pig,0,0,0,0,0,0,
3553,NameNode HA wizard: Refreshing the wizard displays incorrect manual commands., name node ha wizard refresh wizard display incorrect manual command,Install HDFS with customized hostname hdfs1. Start NameNode HA wizard and Refresh on step-2 (select host). Proceed ahead. Create checkpoint step asks to run command with incorrect user name:sudo su -l hdfs -c 'hdfs dfsadmin -safemode enter' Above command returns safemode: Access denied for user hdfs. Superuser privilege is required Actual command should be:sudo su -l hdfs1 -c 'hdfs dfsadmin -safemode enter', instal hdf custom hostnam hdf start name node ha wizard refresh step select host proce ahead creat checkpoint step ask run command incorrect user name sudo su l hdf c hdf dfsadmin safemod enter abov command return safemod access deni user hdf superus privileg requir actual command sudo su l hdf c hdf dfsadmin safemod enter,1,0,0,0,0,0,
3569,'Config' step refresh, config step refresh,Go to Config Step on the addServiceWizard.Refresh page.Got JS error  because selected services where not saved.Expect: get page with config list for selected services., go config step add servic wizard refresh page got js error select servic save expect get page config list select servic,0,0,0,0,0,0,
3582,Cleanup UI restart calculations using actual_configs, cleanup ui restart calcul use actual config,As documented in AMBARI-3531  the restart flags will be provided in host_components itself and services will have an API to get restart host_components easily. Due to this  there is no need for actual_configs on the client  and the code to calculate diffs with global properties., as document ambari restart flag provid host compon servic api get restart host compon easili due need actual config client code calcul diff global properti,1,0,0,0,0,0,
3584,Reassign Master: Misc UI display fixes, reassign master misc ui display fix,This ticket mainly covers UI label and message changes in reassign master wizard., thi ticket mainli cover ui label messag chang reassign master wizard,1,0,0,0,0,0,
3589,Common storage for different wizards, common storag differ wizard,Some wizards (installer  addHosts  addServices) use common local storage objects.But each should has separated object (for example  based on controllerName)., some wizard instal add host add servic use common local storag object but separ object exampl base control name,1,0,0,0,0,0,
3594,Service reconfiguration fails for multiple services, servic reconfigur fail multipl servic,Service reconfiguration fails for HDFS  MapReduce and Hive service with js error. For other services it fails silently without any error (no API call is triggered)., servic reconfigur fail hdf map reduc hive servic js error for servic fail silent without error api call trigger,1,0,0,0,0,0,
3609,os_type_check.sh for RHEL is too restrictive (Server vs Workstation), os type check sh rhel restrict server vs workstat,The /etc/redhat-release on my RHEL6.4 dev box containsRed Hat Enterprise Linux Workstation release 6.4 (Santiago)os_type_check.sh is checking for 'Server' on rhel boxes. It should simply check for Red Hat Enterprise Linux and ignore text up to the version number., the etc redhat releas rhel dev box contain red hat enterpris linux workstat releas santiago os type check sh check server rhel box it simpli check red hat enterpris linux ignor text version number,1,0,0,0,0,0,
3611,Hosts: clarify which filter is in effect, host clarifi filter effect,1. Use a shadow with high contrast to make the current filter more visible.2. Show how many hosts total / how many are in the current view  and a 'Clear all filters' link., use shadow high contrast make current filter visibl show mani host total mani current view clear filter link,1,0,0,0,0,0,
3613,Enable HA wizard loads after sign in, enabl ha wizard load sign,Steps: Go to 'Admin' page -&gt; 'High Availability' tab and run 'Enable NameNode HA' wizard. Close wizard. Sign out (or reopen browser). Sign in.Result:After sign in was opened first page of 'Enable NameNode HA' wizard instead 'Dashboard' page., step go admin page gt high avail tab run enabl name node ha wizard close wizard sign reopen browser sign result after sign open first page enabl name node ha wizard instead dashboard page,0,0,0,0,0,0,
3615,Ambari agent creates empty folder /var/ambari-agent, ambari agent creat empti folder var ambari agent,Ambari agent creates an empty directory /var/ambari-agent during installation.This directory isn't needed  /var/run/ambari-agent is used instead., ambari agent creat empti directori var ambari agent instal thi directori need var run ambari agent use instead,1,0,0,0,0,0,
3618,host actions UI changes based on new stop/start all and delete func, host action ui chang base new stop start delet func,1. Do the right-float on the action menus on the Components section.2. Rename buttons: on SERVICE PAGES: Maintenance --&gt;Service Actions...on HOST PAGES: Maintenance --&gt; Host Actions...on HOST PAGES / COMPONENT SECTION: Actions --&gt; Actions..., do right float action menu compon section renam button servic page mainten gt servic action host page mainten gt host action host page compon section action gt action,1,0,0,0,0,0,
3621,cleanup dialog for unable to delete host, cleanup dialog unabl delet host,make dialog according to left mockup, make dialog accord left mockup,1,0,0,0,0,0,
3623,LiveStatus of the component is not updated when username is changed, live statu compon updat usernam chang,Steps to reproduce: On installer wizard  make install phase fail by killing any install task of master component. Go back and change hdfs username to hdfs1. Proceed ahead and installer wizard completes successfully. HDFS service is red. Nagios shows no alerts  but API returns INSTALLED status for all hdfs host components. UI impact: On starting HDFS  all tasks completes successfully with 100% green progress bar but service status always remains red. Restarting agent resolves the issue.Looks like AmbariConfig.servicesToPidNames is not getting updated when username is changed., step reproduc on instal wizard make instal phase fail kill instal task master compon go back chang hdf usernam hdf proce ahead instal wizard complet success hdf servic red nagio show alert api return instal statu hdf host compon ui impact on start hdf task complet success green progress bar servic statu alway remain red restart agent resolv issu look like ambari config servic to pid name get updat usernam chang,1,0,0,0,0,0,
3631,traceback when attempting to stop ambari-agent as non-root, traceback attempt stop ambari agent non root,I attempted to stop the ambari-agent without going to root first. Prints a pretty bad traceback.&#91;vagrant@c6403 ~&#93;$ ambari-agent stop/usr/sbin/ambari-agent: line 66: /var/lib/ambari-agent/ambari-env.sh: Permission deniedVerifying Python version compatibility...Using python /usr/bin/python2.6Found ambari-agent PID: 2996Stopping ambari-agentTraceback (most recent call last):File '/usr/lib/python2.6/site-packages/ambari_agent/main.py'  line 235  in &lt;module&gt;main()File '/usr/lib/python2.6/site-packages/ambari_agent/main.py'  line 190  in mainsetup_logging(options.verbose)File '/usr/lib/python2.6/site-packages/ambari_agent/main.py'  line 73  in setup_loggingrotateLog = logging.handlers.RotatingFileHandler(logfile  'a'  10000000  25)File '/usr/lib64/python2.6/logging/handlers.py'  line 112  in initBaseRotatingHandler.init(self  filename  mode  encoding  delay)File '/usr/lib64/python2.6/logging/handlers.py'  line 64  in initlogging.FileHandler.init(self  filename  mode  encoding  delay)File '/usr/lib64/python2.6/logging/init.py'  line 827  in __initStreamHandler.init(self  self._open())File '/usr/lib64/python2.6/logging/init.py'  line 846  in _openstream = open(self.baseFilename  self.mode)IOError: &#91;Errno 13&#93; Permission denied: '/var/log/ambari-agent/ambari-agent.log'Removing PID file at /var/run/ambari-agent/ambari-agent.pidrm: cannot remove '/var/run/ambari-agent/ambari-agent.pid': Permission deniedambari-agent successfully stopped, i attempt stop ambari agent without go root first print pretti bad traceback vagrant c ambari agent stop usr sbin ambari agent line var lib ambari agent ambari env sh permiss deni verifi python version compat use python usr bin python found ambari agent pid stop ambari agent traceback recent call last file usr lib python site packag ambari agent main py line lt modul gt main file usr lib python site packag ambari agent main py line mainsetup log option verbos file usr lib python site packag ambari agent main py line setup loggingrot log log handler rotat file handler logfil file usr lib python log handler py line init base rotat handler init self filenam mode encod delay file usr lib python log handler py line initlog file handler init self filenam mode encod delay file usr lib python log init py line init stream handler init self self open file usr lib python log init py line openstream open self base filenam self mode io error errno permiss deni var log ambari agent ambari agent log remov pid file var run ambari agent ambari agent pidrm cannot remov var run ambari agent ambari agent pid permiss deniedambari agent success stop,1,0,0,0,0,0,
3645,HA cluster: some dashboard's widgets contain 'Null'  'NaN' values after services stop, ha cluster dashboard widget contain null na n valu servic stop,Steps:Stop YARN service.Go to 'Dashboard'.Result:'NodeManagers Live' widget contains 'null' values.Similar problem is present for 'HBase Ave Load' widget - 'NaN' value., step stop yarn servic go dashboard result node manag live widget contain null valu similar problem present h base ave load widget na n valu,1,0,0,0,0,0,
3648,Failed to start Hive Metastore (centos5.8  Stack 2.0), fail start hive metastor cento stack,Occurred during install as warning (could not start the service). Clicked next to continue  when into Ambari  then tried to start Hive there as well  same issue., occur instal warn could start servic click next continu ambari tri start hive well issu,0,0,0,0,0,0,
3650,Poll for host_components which have stale_configs, poll host compon stale config,UI needs to know which host_components need restart due to stale_configs (saved but not picked up). Server API provide stale_configs flag per host-component. We need this polled and maintained on client model., ui need know host compon need restart due stale config save pick server api provid stale config flag per host compon we need poll maintain client model,1,0,0,0,0,0,
3674,UI does not update active hbase master in display, ui updat activ hbase master display,When we have 3 HBase masters we show in UI that one of them is active. When the master is stopped  the other HBase master is not marked as active in UI. In API it does become active., when h base master show ui one activ when master stop h base master mark activ ui in api becom activ,1,0,0,0,0,0,
3675,Default value of 'Default virtual memory for a job's map-task' is not valid, default valu default virtual memori job map task valid,The default value of 'Default virtual memory for a job's map-task' (in Customize Services page -&gt; MapReduce2 tab -&gt; General) was '619.5'.Warning hint says 'Must contain digits only'Value depends on quantity of installed components., the default valu default virtual memori job map task custom servic page gt map reduc tab gt gener warn hint say must contain digit valu depend quantiti instal compon,1,0,0,0,0,0,
3679,Better error message needed when incompatible ambari-agents installed, better error messag need incompat ambari agent instal,On a few days old cluster I attempted to add a host. The add host failed due to ambari-server trying to install ambari-agent-1.4.1.17 and the repo having ambari-agent-1.4.1.23-1.The message in /var/run/ambari-server/bootstrap/11/hostname.log was:STDERRscp /usr/lib/python2.6/site-packages/ambari_server/setupAgent.py done for host srimanth1-5.c.pramod-thangali.internal  exitcode=0Copying files finishedRunning setup agent...STDOUTError: Nothing to do{'exitstatus': 1  'log': ('Loaded plugins: downloadonly  fastestmirror  security/nDetermining fastest mirrors/n * base: www.gtlib.gatech.edu/n * extras: centos.mirror.netriplex.com/n * updates: mirror.cogentco.com/nSetting up Install Process/nNo package ambari-agent-1.4.1.17 available./n'  None)}, on day old cluster i attempt add host the add host fail due ambari server tri instal ambari agent repo ambari agent the messag var run ambari server bootstrap hostnam log stder rscp usr lib python site packag ambari server setup agent py done host srimanth c pramod thangali intern exitcod copi file finish run setup agent stdout error noth exitstatu log load plugin downloadonli fastestmirror secur n determin fastest mirror n base www gtlib gatech edu n extra cento mirror netriplex com n updat mirror cogentco com n set instal process n no packag ambari agent avail n none,1,0,0,0,0,0,
3686,NameNode HA wizard (Configure Components step): Task 'Reconfigure HDFS' always fail  and user cannot proceed to next step, name node ha wizard configur compon step task reconfigur hdf alway fail user cannot proceed next step,This task fails due to bad request:/api/v1/clusters/c1/hosts/HDFS_CLIENT/host_components/dev01.hortonworks.comShould be: api/v1/clusters/c1/hosts/dev01.hortonworks.com/host_components/HDFS_CLIENT, thi task fail due bad request api v cluster c host hdf client host compon dev hortonwork com should api v cluster c host dev hortonwork com host compon hdf client,0,0,0,0,0,1,
3690,Typo in text label on Hosts page, typo text label host page,On the Hosts page  we have a typo in the label 'filterd'. Should be 'filtered'., on host page typo label filterd should filter,1,0,0,0,0,0,
3695,'Confirm hosts' shows 'ntpd not running' warning  but it's running on host, confirm host show ntpd run warn run host,STR: Install  setup and start Ambari server by default. Reach 'Choose services' phase of installer.Actual result:'Confirm hosts' shows warning that ntpd service isn't running on hosts  but it's running in console by command service ntpd status, str instal setup start ambari server default reach choos servic phase instal actual result confirm host show warn ntpd servic run host run consol command servic ntpd statu,1,0,0,0,0,0,
3698,Modify UI text for host cleanup, modifi ui text host cleanup,python /usr/lib/python2.6/site-packages/ambari_agent/HostCleanup.py -s -k 'users'To cleanup in interactive mode  remove *-s* option. To cleanup all resources  including _users_  remove *-k users* option. Use *--help* for a list of available options. The motivation is to provide the conservative option but minimal detail to allow for full clean up., python usr lib python site packag ambari agent host cleanup py k user to cleanup interact mode remov option to cleanup resourc includ user remov k user option use help list avail option the motiv provid conserv option minim detail allow full clean,1,0,0,0,0,0,
3701,Reduce logs emitted to report heartbeats from agents, reduc log emit report heartbeat agent,For a 657 node cluster:~10 minute for 10 MB and 20 log files store about 200 minutes (~3 hours) of log. This is not ideal if an error overnight needs to be investigated. We should try for the log to last 24 hours - ideally 72 hours to account for weekends.-rw-r--r-- 1 root root 10485854 Oct 17 17:35 ambari-server.log.6-rw-r--r-- 1 root root 10485836 Oct 17 17:44 ambari-server.log.5-rw-r--r-- 1 root root 10485811 Oct 17 17:52 ambari-server.log.4-rw-r--r-- 1 root root 10485793 Oct 17 18:01 ambari-server.log.3-rw-r--r-- 1 root root 10485793 Oct 17 18:10 ambari-server.log.2-rw-r--r-- 1 root root 10485854 Oct 17 18:18 ambari-server.log.1, for node cluster minut mb log file store minut hour log thi ideal error overnight need investig we tri log last hour ideal hour account weekend rw r r root root oct ambari server log rw r r root root oct ambari server log rw r r root root oct ambari server log rw r r root root oct ambari server log rw r r root root oct ambari server log rw r r root root oct ambari server log,1,0,0,0,0,0,
3708,Reconfigure of dynamic configs not showing modified values, reconfigur dynam config show modifi valu,Modifications of dynamic properties are being persisted on server  but default values are being shown in UI. Also  we should not validate dynamic configs which are of type string. mapreduce.map.java.opts mapreduce.reduce.java.opts yarn.app.mapreduce.am.command-opts, modif dynam properti persist server default valu shown ui also valid dynam config type string mapreduc map java opt mapreduc reduc java opt yarn app mapreduc command opt,1,0,0,0,0,0,
3713,When filtering on hosts  the table column sizes shift  should stay fixed., when filter host tabl column size shift stay fix,For the 'defaultsProvider' and 'serviceValidator' functionalities  we need unit tests, for default provid servic valid function need unit test,1,0,0,0,0,0,
3715,Reassign Master Wizard does not display folder and hosts on 'Manual commands' page after browser reopening, reassign master wizard display folder host manual command page browser reopen,Steps: Open 'Reassign Master Wizard' for NameNode or SNameNode. Go to 'Manual commands' page. Close browser and open it again.Result: Was opened 'Manual commands' page  but hostnames and foldername were replaced with '{1}'  '{2}' etc.Attached picture for other page  but behavior is similar., step open reassign master wizard name node s name node go manual command page close browser open result wa open manual command page hostnam foldernam replac etc attach pictur page behavior similar,1,0,0,0,0,0,
3720,Provide read-only view of repo options in Ambari Web, provid read view repo option ambari web,If a user is using local repos  and customizes Advanced Repository Options during install  the user might need this info to debug post install (since it is used in Add Hosts)  for example.Note: We should show this information regardless if the user customizes repos or not during install., if user use local repo custom advanc repositori option instal user might need info debug post instal sinc use add host exampl note we show inform regardless user custom repo instal,1,0,0,0,0,0,
3724,Incorrect host status when slave down, incorrect host statu slave,When host has slave down status 'No Heartbeat' status is shown instead of 'Slave Down'., when host slave statu no heartbeat statu shown instead slave down,1,0,0,0,0,0,
3726,Restart indicators for services and hosts disappear after some time., restart indic servic host disappear time,Goto Service (that have stale configs) -&gt; configs   no restart indicators are shown. Refresh page  for 10-15 seconds you see indicators then they disappear. The same for host detail page, goto servic stale config gt config restart indic shown refresh page second see indic disappear the host detail page,1,0,1,0,0,0,
3729,Ganglia monitor started with second or third attempt on secure cluster, ganglia monitor start second third attempt secur cluster,,,1,0,0,0,0,0,
3738,Background ops dialog checkbox UI cleanup, background op dialog checkbox ui cleanup,The OK and the text should be on the same centerline row., the ok text centerlin row,1,0,0,0,0,0,
3739,Remove Exception message printed to log for successful starts, remov except messag print log success start,Following error messages are printed to log with default log level and are misleading.04:19:52 438 ERROR [main] MasterKeyServiceImpl:109 - Master key is not provided as a System property or an environment varialble.04:19:52 439 INFO [main] Configuration:415 - Credential provider creation failed.Master key initialization failed., follow error messag print log default log level mislead error main master key servic impl master key provid system properti environ varialbl info main configur credenti provid creation fail master key initi fail,1,0,0,0,0,0,
3742,HBase Links widget has 'more' button out of bounds and looks broken when there are multiple masters, h base link widget button bound look broken multipl master,When there are multiple HBase Masters  the HBase Links widget looks broken. Let's get rid of the 'and X Standby Masters' static text as it is not a link and not very useful.So the widget would look like:'HBase Master''X RegionServers''Master Web UI', when multipl h base master h base link widget look broken let get rid x standbi master static text link use so widget would look like h base master x region server master web ui,1,0,0,0,0,0,
3752,MR jobs are hanging on a 2-node cluster with default configuration, mr job hang node cluster default configur,This is a 2-node cluster with 2GB of RAM each. Cluster deployment goes fine but MR jobs do not complete resulting in service check failures for MR  OOZIE  Pig  etc., thi node cluster gb ram cluster deploy goe fine mr job complet result servic check failur mr oozi pig etc,1,0,0,0,1,0,
3759,Add host wizard: After successfully bootstrapping host  'next' button is disabled, add host wizard after success bootstrap host next button disabl,See screenshot, see screenshot,1,0,0,0,0,0,
3760,Provide config-group support in add-host wizard, provid config group support add host wizard,When adding hosts  a user should be able to select which config-groups this host belongs to. Configurations of that group (Default or config-group) will be applied on that host., when ad host user abl select config group host belong configur group default config group appli host,1,0,0,0,0,0,
3761,'Uncaught exception' in JS while navigating through services on Services page, uncaught except js navig servic servic page,This one was discovered while quick navigating through services on Services page.To reproduce just try to click on services links fast.After that service content is not displayed., thi one discov quick navig servic servic page to reproduc tri click servic link fast after servic content display,1,0,0,0,0,0,
3770,Need better error log message when agent unable to reach server, need better error log messag agent unabl reach server,http://hortonworks.com/community/forums/topic/installing-hdp2-0-6-on-centos6-4/The current ERROR in the agent log can be cryptic., http hortonwork com commun forum topic instal hdp cento the current error agent log cryptic,1,0,0,0,0,0,
3771,Ambari should allow changing Ganglia cache location, ambari allow chang ganglia cach locat,Ambari allows changing Ganglia directory during installation  but not after the cluster is installed. We should allow changing this directory after cluster installed, ambari allow chang ganglia directori instal cluster instal we allow chang directori cluster instal,1,0,0,0,0,0,
3779,During cluster install cannot go past Step0, dure cluster instal cannot go past step,UI makes a call to http://c6401.ambari.apache.org:8080/api/v1/stacks2/HDP/versions?fields=Versions operatingSystems/repositories/Repositories.The API is missing the operatingSystems info  except for the suse11 one:{ 'href' : 'http://c6401.ambari.apache.org:8080/api/v1/stacks2/HDP/versions?fields=Versions operatingSystems/repositories/Repositories'  'items' : [ { 'href' : 'http://c6401.ambari.apache.org:8080/api/v1/stacks2/HDP/versions/1.2.0'  'Versions' : { 'active' : false  'min_upgrade_version' : null  'parent_stack_version' : null  'stack_name' : 'HDP'  'stack_version' : '1.2.0' }  'operatingSystems' : [ ] }  { 'href' : 'http://c6401.ambari.apache.org:8080/api/v1/stacks2/HDP/versions/1.2.1'  'Versions' : { 'active' : false  'min_upgrade_version' : null  'parent_stack_version' : null  'stack_name' : 'HDP'  'stack_version' : '1.2.1' }  'operatingSystems' : [ ] }  { 'href' : 'http://c6401.ambari.apache.org:8080/api/v1/stacks2/HDP/versions/1.3.0'  'Versions' : { 'active' : false  'min_upgrade_version' : null  'parent_stack_version' : null  'stack_name' : 'HDP'  'stack_version' : '1.3.0' }  'operatingSystems' : [ ] }  { 'href' : 'http://c6401.ambari.apache.org:8080/api/v1/stacks2/HDP/versions/1.3.2'  'Versions' : { 'active' : true  'min_upgrade_version' : null  'parent_stack_version' : null  'stack_name' : 'HDP'  'stack_version' : '1.3.2' }  'operatingSystems' : [ ] }  { 'href' : 'http://c6401.ambari.apache.org:8080/api/v1/stacks2/HDP/versions/1.3.3'  'Versions' : { 'active' : true  'min_upgrade_version' : null  'parent_stack_version' : null  'stack_name' : 'HDP'  'stack_version' : '1.3.3' }  'operatingSystems' : [ ] }  { 'href' : 'http://c6401.ambari.apache.org:8080/api/v1/stacks2/HDP/versions/2.0.5'  'Versions' : { 'active' : false  'min_upgrade_version' : null  'parent_stack_version' : null  'stack_name' : 'HDP'  'stack_version' : '2.0.5' }  'operatingSystems' : [ ] }  { 'href' : 'http://c6401.ambari.apache.org:8080/api/v1/stacks2/HDP/versions/2.0.6'  'Versions' : { 'active' : true  'min_upgrade_version' : null  'parent_stack_version' : null  'stack_name' : 'HDP'  'stack_version' : '2.0.6' }  'operatingSystems' : [ { 'href' : 'http://c6401.ambari.apache.org:8080/api/v1/stacks2/HDP/versions/2.0.6/operatingSystems/centos5'  'OperatingSystems' : { 'os_type' : 'centos5'  'stack_name' : 'HDP'  'stack_version' : '2.0.6' }  'repositories' : [ ] }  { 'href' : 'http://c6401.ambari.apache.org:8080/api/v1/stacks2/HDP/versions/2.0.6/operatingSystems/centos6'  'OperatingSystems' : { 'os_type' : 'centos6'  'stack_name' : 'HDP'  'stack_version' : '2.0.6' }  'repositories' : [ ] }  { 'href' : 'http://c6401.ambari.apache.org:8080/api/v1/stacks2/HDP/versions/2.0.6/operatingSystems/oraclelinux5'  'OperatingSystems' : { 'os_type' : 'oraclelinux5'  'stack_name' : 'HDP'  'stack_version' : '2.0.6' }  'repositories' : [ ] }  { 'href' : 'http://c6401.ambari.apache.org:8080/api/v1/stacks2/HDP/versions/2.0.6/operatingSystems/oraclelinux6'  'OperatingSystems' : { 'os_type' : 'oraclelinux6'  'stack_name' : 'HDP'  'stack_version' : '2.0.6' }  'repositories' : [ ] }  { 'href' : 'http://c6401.ambari.apache.org:8080/api/v1/stacks2/HDP/versions/2.0.6/operatingSystems/redhat5'  'OperatingSystems' : { 'os_type' : 'redhat5'  'stack_name' : 'HDP'  'stack_version' : '2.0.6' }  'repositories' : [ ] }  { 'href' : 'http://c6401.ambari.apache.org:8080/api/v1/stacks2/HDP/versions/2.0.6/operatingSystems/redhat6'  'OperatingSystems' : { 'os_type' : 'redhat6'  'stack_name' : 'HDP'  'stack_version' : '2.0.6' }  'repositories' : [ ] }  { 'href' : 'http://c6401.ambari.apache.org:8080/api/v1/stacks2/HDP/versions/2.0.6/operatingSystems/sles11'  'OperatingSystems' : { 'os_type' : 'sles11'  'stack_name' : 'HDP'  'stack_version' : '2.0.6' }  'repositories' : [ ] }  { 'href' : 'http://c6401.ambari.apache.org:8080/api/v1/stacks2/HDP/versions/2.0.6/operatingSystems/suse11'  'OperatingSystems' : { 'os_type' : 'suse11'  'stack_name' : 'HDP'  'stack_version' : '2.0.6' }  'repositories' : [ { 'href' : 'http://c6401.ambari.apache.org:8080/api/v1/stacks2/HDP/versions/2.0.6/operatingSystems/suse11/repositories/HDP-2.0.6'  'Repositories' : { 'base_url' : 'http://public-repo-1.hortonworks.com/HDP/suse11/2.x/updates/2.0.6.0'  'default_base_url' : 'http://public-repo-1.hortonworks.com/HDP/suse11/2.x/updates/2.0.6.0'  'mirrors_list' : null  'os_type' : 'suse11'  'repo_id' : 'HDP-2.0.6'  'repo_name' : 'HDP'  'stack_name' : 'HDP'  'stack_version' : '2.0.6' } } ] } ] } ]}, ui make call http c ambari apach org api v stack hdp version field version oper system repositori repositori the api miss oper system info except suse one href http c ambari apach org api v stack hdp version field version oper system repositori repositori item href http c ambari apach org api v stack hdp version version activ fals min upgrad version null parent stack version null stack name hdp stack version oper system href http c ambari apach org api v stack hdp version version activ fals min upgrad version null parent stack version null stack name hdp stack version oper system href http c ambari apach org api v stack hdp version version activ fals min upgrad version null parent stack version null stack name hdp stack version oper system href http c ambari apach org api v stack hdp version version activ true min upgrad version null parent stack version null stack name hdp stack version oper system href http c ambari apach org api v stack hdp version version activ true min upgrad version null parent stack version null stack name hdp stack version oper system href http c ambari apach org api v stack hdp version version activ fals min upgrad version null parent stack version null stack name hdp stack version oper system href http c ambari apach org api v stack hdp version version activ true min upgrad version null parent stack version null stack name hdp stack version oper system href http c ambari apach org api v stack hdp version oper system cento oper system os type cento stack name hdp stack version repositori href http c ambari apach org api v stack hdp version oper system cento oper system os type cento stack name hdp stack version repositori href http c ambari apach org api v stack hdp version oper system oraclelinux oper system os type oraclelinux stack name hdp stack version repositori href http c ambari apach org api v stack hdp version oper system oraclelinux oper system os type oraclelinux stack name hdp stack version repositori href http c ambari apach org api v stack hdp version oper system redhat oper system os type redhat stack name hdp stack version repositori href http c ambari apach org api v stack hdp version oper system redhat oper system os type redhat stack name hdp stack version repositori href http c ambari apach org api v stack hdp version oper system sle oper system os type sle stack name hdp stack version repositori href http c ambari apach org api v stack hdp version oper system suse oper system os type suse stack name hdp stack version repositori href http c ambari apach org api v stack hdp version oper system suse repositori hdp repositori base url http public repo hortonwork com hdp suse x updat default base url http public repo hortonwork com hdp suse x updat mirror list null os type suse repo id hdp repo name hdp stack name hdp stack version,1,0,0,0,0,0,
3791,Provide add/remove/rename/duplicate actions in manage-config-groups dialog, provid add remov renam duplic action manag config group dialog,Various config-group actions (add/remove/rename/duplicate) should be provided in the manage-config-groups dialog. Any of these should not rely on the Save button  but are immediately persisted via API., variou config group action add remov renam duplic provid manag config group dialog ani reli save button immedi persist via api,0,0,0,0,0,0,
3793,Do not store disks_info in DB. Store it as dynamic info in memory that can be used to show on the UI., do store disk info db store dynam info memori use show ui,Currently  we store disks info in DB and this information is never updated after registration. As disks details can change (space availability changes  disks get mounted/unmounted  etc.) the persisted information is not useful.We should instead hold the details in memory and refresh it at certain intervals (e.g. once every 10 minutes) and then alert if space availability hits some lower limit., current store disk info db inform never updat registr as disk detail chang space avail chang disk get mount unmount etc persist inform use we instead hold detail memori refresh certain interv e g everi minut alert space avail hit lower limit,0,0,0,0,0,0,
3805,'Add service' if nothing to add, add servic noth add,Disable and gray out the Add Services button if there aren't any more services to be added.Upon hover  show a tooltip saying 'No more services to be added'.Although we hide the 'Add Component' button when no more components are to be added  we don't actually like that and want to move towards 'disabling/graying out with hover tooltip' pattern., disabl gray add servic button servic ad upon hover show tooltip say no servic ad although hide add compon button compon ad actual like want move toward disabl gray hover tooltip pattern,0,0,0,0,0,0,
3815,Remove  Rename actions enabled for 'Default' config group, remov renam action enabl default config group,In the Manager Configuration Groups dialog I selected the Default config-group  and the actions to remove and rename config-group are enabled. These ops are not allowed for Default config-group., in manag configur group dialog i select default config group action remov renam config group enabl these op allow default config group,0,0,0,0,0,0,
3824,Provide change config-group action on host configs, provid chang config group action host config,On a host's configs page  provide a Change action beside config-group to switch from one group to another  or to Default., on host config page provid chang action besid config group switch one group anoth default,0,0,0,0,0,0,
3836,Unable to close manage-config-groups dialog when only Default group present, unabl close manag config group dialog default group present,I had only the Default config-group when I launched the manage config-groups dialog. When I clicked on Cancel or X  it would not close dialog with the following error:Uncaught TypeError: Cannot read property 'id' of null This was in method updateConfigGroupOnServicePage() at the first line belowselectedConfigGroup = managedConfigGroups.findProperty('id'  selectedConfigGroup.id); if(selectedConfigGroup){ mainServiceInfoConfigsController.set('selectedConfigGroup'  selectedConfigGroup); }else{ mainServiceInfoConfigsController.set('selectedConfigGroup'  managedConfigGroups.findProperty('isDefault'  true)); }, i default config group i launch manag config group dialog when i click cancel x would close dialog follow error uncaught type error cannot read properti id null thi method updat config group on servic page first line belowselect config group manag config group find properti id select config group id select config group main servic info config control set select config group select config group els main servic info config control set select config group manag config group find properti default true,0,0,0,0,0,0,
3838,Services sidebar for host configs needs vertical gap, servic sidebar host config need vertic gap,When you go to host configs page  there is no gap between the services sidebar and the tabs. This should be changed so that the config-group bar and services sidebar have the same gap from the tabs at top., when go host config page gap servic sidebar tab thi chang config group bar servic sidebar gap tab top,0,0,0,0,0,0,
3842,Host configs page should properly order services, host config page properli order servic,When App.supports.hostOverridesHost is enabled  and you visit the configs page for a host  the services are in some random order. They should be in the same order as the services page.Also  there is a small empty entry in the menu  which gives like a 5px extra space between some services. You can even hover on this empty entry and it will highlight., when app support host overrid host enabl visit config page host servic random order they order servic page also small empti entri menu give like px extra space servic you even hover empti entri highlight,0,0,0,0,0,0,
3844,Error in saving host for newly created config group, error save host newli creat config group,After creating new Config Group in Manage Configuration Groups dialog try to add host to this group and save. Also sometimes '+' button to add hosts is disabled for newly created group., after creat new config group manag configur group dialog tri add host group save also sometim button add host disabl newli creat group,0,0,0,0,0,0,
3857,Clicking on Settings link navigates to login page for a non-admin user., click set link navig login page non admin user,This happens because non-admin users are not authorized to make POST/PUT calls to any resource  including 'persist'.For now  let's hide 'Settings' if the user is a non-admin user., thi happen non admin user author make post put call resourc includ persist for let hide set user non admin user,0,0,0,0,0,0,
3864,JS Error in 'Add Host Wizard' if we proceed with failed registered hosts, js error add host wizard proceed fail regist host,1. Add two hosts in Add Host Wizard.2. In Conform Hosts step  one registered successfully  the other failed.3. Proceed to next  JS error happened when deploy  wizard UI hang up., add two host add host wizard in conform host step one regist success fail proce next js error happen deploy wizard ui hang,0,0,0,0,0,0,
3868,Add host fails after configuring NN HA with JavaScript error, add host fail configur nn ha java script error,,,0,0,0,0,0,0,
3873,Unittests for User resource an all it's attributes, unittest user resourc attribut,Test resource User of resource management . Test all actions and all the attributes. Please make sure we mock to check both cases when user already exists and when it's not.Note here we should not call directly provider methods like action_create  but make resource management library do that for us  by calling env.run(). In other case test won't cover resources definitions  and other import logic, test resourc user resourc manag test action attribut pleas make sure mock check case user alreadi exist note call directli provid method like action creat make resourc manag librari us call env run in case test cover resourc definit import logic,0,0,0,0,0,0,
3877,Duplicate config-group action not duplicate configs, duplic config group action duplic config,Override like 3 configs in a config group and save. Now go to the Manage Config Groups dialog and select this group and click on Duplicate action. A popup with name and description pops up and hitting OK creates the duplicated config group.Though this duplicated config group has the correct name/description  it does not have the duplicated configs (3 that we overrode). When doing the POST call  we should populate the desired_configs to be the exact same as the source config-group., overrid like config config group save now go manag config group dialog select group click duplic action a popup name descript pop hit ok creat duplic config group though duplic config group correct name descript duplic config overrod when post call popul desir config exact sourc config group,0,0,0,0,0,0,
3878,ResourceManager Heap metrics is not correct on Ambari console, resourc manag heap metric correct ambari consol,PROBLEM: The ResourceManager Heap metrics on Ambari web console doesn't show the correct value  not only the current heap usage doesn't reflect the correct usage  but also the total heap size doesn't match what we configure for the ResourceManager Heap size  even the total heap size number is changing constantly., problem the resourc manag heap metric ambari web consol show correct valu current heap usag reflect correct usag also total heap size match configur resourc manag heap size even total heap size number chang constantli,0,0,0,0,0,0,
3881,UI incorrect behavior during upgrade, ui incorrect behavior upgrad,This issue was discovered while upgrading the cluster to hash 87adc8c2d29b20a30f01e54c12f67dcbbe34b32e of 1.4.2 branch. The logic inside configuration_controller.js function getConfigsByTags(tagObject) has a reference to undefined variable and js error was encountered. This happened when any of the service page was rendered and program flow from quick_view_link_view.js function didInsertElement() -&gt; setQuickLinks() -&gt; loadTags() -&gt; loadTagsSuccess(data) -&gt; getSecurityProperties() -&gt; configurationController.getConfigsByTags(tag), thi issu discov upgrad cluster hash adc c b f e c f dcbbe b e branch the logic insid configur control js function get config by tag tag object refer undefin variabl js error encount thi happen servic page render program flow quick view link view js function insert element gt set quick link gt load tag gt load tag success data gt get secur properti gt configur control get config by tag tag,0,0,0,0,0,0,
3882,Background operations popup window minimum size should be fixed when narrowing down the browser, background oper popup window minimum size fix narrow browser,,,0,0,0,0,0,0,
3888,Incorrect restart required tooltip view, incorrect restart requir tooltip view,When components and hosts required to restart we show refresh icon near the service name. On icon hover event we show tooltip with count of components and hosts., when compon host requir restart show refresh icon near servic name on icon hover event show tooltip count compon host,0,0,0,0,0,0,
3890,Background operations: two scrollbars  if width is lower then 1450px, background oper two scrollbar width lower px,Screenshot attached., screenshot attach,0,0,0,0,0,0,
3897,Restart indicator flags not showing up for HDP 1.3.x stack services, restart indic flag show hdp x stack servic,When config-groups are used for HDP 1.3.x stack services  the stale_configs are always false., when config group use hdp x stack servic stale config alway fals,0,0,0,0,0,0,
3899,'HDFS Short-circuit read' config property is repeated, hdf short circuit read config properti repeat,,,0,0,0,0,0,0,
3900,Modify messages from 'reassign master' to 'move master', modifi messag reassign master move master,,,0,0,0,0,0,0,
3911,Security Wizard: Service Configuration page is broken, secur wizard servic configur page broken,Trying to enable security  the configurations page is blank., tri enabl secur configur page blank,0,0,0,0,0,0,
3914,Add Host wizard stuck on configuration step, add host wizard stuck configur step,Cluster should have service(HDFS PIG SQOOP) which doesn't have any slave or client host-components.1. Run Add Host wizard2. Proceed to configuration stepResult: Wizard popup stuck in loading processJS error:Uncaught TypeError: Cannot call method 'get' of undefined app.js:10245(anonymous function) app.js:10245App.AddHostController.App.WizardController.extend.loadServiceConfigGroups app.js:10242(anonymous function) app.js:43659f.Callbacks.o vendor.js:95f.Callbacks.p.add vendor.js:95(anonymous function) app.js:43657f.Callbacks.o vendor.js:95f.Callbacks.p.fireWith vendor.js:95f.Callbacks.p.fire vendor.js:95(anonymous function), cluster servic hdf pig sqoop slave client host compon run add host wizard proce configur step result wizard popup stuck load process js error uncaught type error cannot call method get undefin app js anonym function app js app add host control app wizard control extend load servic config group app js anonym function app js f callback vendor js f callback p add vendor js anonym function app js f callback vendor js f callback p fire with vendor js f callback p fire vendor js anonym function,0,0,0,0,0,0,
3921,Hovers stay after manage-config-groups dialog is closed, hover stay manag config group dialog close,I opened the manage-config-groups dialog and hovered on one of the actions (remove host from config-group). Then I saved or cancelled to close the dialog. The hover still remains in the middle of page.I have seen this in other usages as well. We need to make sure that all hovers are closed when the focus is lost., i open manag config group dialog hover one action remov host config group then i save cancel close dialog the hover still remain middl page i seen usag well we need make sure hover close focu lost,0,0,0,0,0,0,
3925,Adding host to multiple groups at the same time fails, ad host multipl group time fail,Steps:1. Create 2 config groups2. Rename 1 config group3. Add hosts to renamed config group4. Add host to other config group.Result:Save stops working. Error in the JS console.Error in JS console:Uncaught TypeError: Cannot call method 'sort' of undefined manage_config_groups_controller.js:460(anonymous function) manage_config_groups_controller.js:460(anonymous function) manage_config_groups_controller.js:458ComputedPropertyPrototype.get ember-latest.js:2949get ember-latest.js:1355getPath ember-latest.js:1477get ember-latest.js:1348Ember.Observable.Ember.Mixin.create.get ember-latest.js:7695App.ModalPopup.show.onPrimary item.js:251newFunc ember-latest.js:949ActionHelper.registeredActions.(anonymous function).handler ember-latest.js:19458(anonymous function) ember-latest.js:11250f.event.dispatch jquery-1.7.2.min.js:3h.handle.i, step creat config group renam config group add host renam config group add host config group result save stop work error js consol error js consol uncaught type error cannot call method sort undefin manag config group control js anonym function manag config group control js anonym function manag config group control js comput properti prototyp get ember latest js get ember latest js get path ember latest js get ember latest js ember observ ember mixin creat get ember latest js app modal popup show primari item js new func ember latest js action helper regist action anonym function handler ember latest js anonym function ember latest js f event dispatch jqueri min js h handl,0,0,0,0,0,0,
3930,Missing host message on cluster deploy, miss host messag cluster deploy,When execute any service check  host message become empty. Server return  unsupported on UI  command of task - 'SERVICE_CHECK'., when execut servic check host messag becom empti server return unsupport ui command task servic check,0,0,0,0,0,0,
3938,JS error when switching config groups in Hive / Oozie service config pages, js error switch config group hive oozi servic config page,,,0,0,0,0,0,0,
3953,HBase Master alerts are confusing in multi-master environment, h base master alert confus multi master environ,When multiple HBase Masters are set up  HBase service-level alert section shows multiples of the following: HBase Master process HBase Master Web UI HBase Master CPU utilizationThe label is exactly the same for all masters  so you can't distinguish which alert is for which master. Ambari should mirror what ambari does for NameNode alerts so that the user can tell them apart (append hostname in the alert label)., when multipl h base master set h base servic level alert section show multipl follow h base master process h base master web ui h base master cpu util the label exactli master distinguish alert master ambari mirror ambari name node alert user tell apart append hostnam alert label,0,0,0,0,0,0,
3954,hbase.zookeeper.quorum changing inconsistently on hosts after adding ZookeeperServer, hbase zookeep quorum chang inconsist host ad zookeep server,Issue 1:Steps followed: 1. Install a 3-node cluster with Hbase and Zookeeper and 3 zookeeper servers. 2. After installation on each host the property hbase.zookeeper.quorum in /etc/hbase/conf/hbase-site.xml has:&lt;name&gt;hbase.zookeeper.quorum&lt;/name&gt; &lt;value&gt;c6401.ambari.apache.org c6402.ambari.apache.org c6403.ambari.apache.org&lt;/value&gt;3. After adding a host with Hbase Region Server  and after that a ZookeeperServer to it  the property on the added c6404.ambari.apache.org host has the same value  as in 2. 4. After restarting HBase master on c6401 (which is proposed by UI)  the property value on c6401 becomes:  &lt;name&gt;hbase.zookeeper.quorum&lt;/name&gt; &lt;value&gt;c6401.ambari.apache.org&lt;/value&gt;. On other hosts it remains unchanged. 5. After restarting HbaseRegionServers on the rest of the hosts (not proposed by ui)  the property changes too  to the same value  as in 4. __________________________________________________________Issue 2:Property templeton.zookeeper.hosts in /etc/hcatalog/conf/webhcat-site.xmlPrior to adding a zookeeperServer on host c6404  config on WebHCat server lookes like:[root@c6402 vagrant]# cat /etc/hcatalog/conf/webhcat-site.xml |grep templeton.zookeeper.hosts -C 2 &lt;configuration&gt; &lt;property&gt; &lt;name&gt;templeton.zookeeper.hosts&lt;/name&gt; &lt;value&gt;c6401.ambari.apache.org:2181 c6402.ambari.apache.org:2181 c6403.ambari.apache.org:2181&lt;/value&gt; &lt;/property&gt;After adding a ZookeeperServer on c6404 and restarting WebHCatServer on c6402: [root@c6402 vagrant]# cat /etc/hcatalog/conf/webhcat-site.xml |grep templeton.zookeeper.hosts -C 2 &lt;/property&gt; &lt;property&gt; &lt;name&gt;templeton.zookeeper.hosts&lt;/name&gt; &lt;value&gt;c6401.ambari.apache.org&lt;/value&gt; &lt;/property&gt;__________________________________________________________Issue 3. On a 3-node cluster with HA-enbaled  property ha.zookeeper.quorum in /etc/hadoop/conf/core-site.xml  has the same value on all hosts:  &lt;name&gt;ha.zookeeper.quorum&lt;/name&gt; &lt;value&gt;c6401.ambari.apache.org:2181 c6402.ambari.apache.org:2181 c6403.ambari.apache.org:2181&lt;/value&gt;It doesn't change on either of the hosts after adding c6404 to the cluster  installing ZookeeperServer on it and restarting HDFS., issu step follow instal node cluster hbase zookeep zookeep server after instal host properti hbase zookeep quorum etc hbase conf hbase site xml lt name gt hbase zookeep quorum lt name gt lt valu gt c ambari apach org c ambari apach org c ambari apach org lt valu gt after ad host hbase region server zookeep server properti ad c ambari apach org host valu after restart h base master c propos ui properti valu c becom lt name gt hbase zookeep quorum lt name gt lt valu gt c ambari apach org lt valu gt on host remain unchang after restart hbase region server rest host propos ui properti chang valu issu properti templeton zookeep host etc hcatalog conf webhcat site xml prior ad zookeep server host c config web h cat server look like root c vagrant cat etc hcatalog conf webhcat site xml grep templeton zookeep host c lt configur gt lt properti gt lt name gt templeton zookeep host lt name gt lt valu gt c ambari apach org c ambari apach org c ambari apach org lt valu gt lt properti gt after ad zookeep server c restart web h cat server c root c vagrant cat etc hcatalog conf webhcat site xml grep templeton zookeep host c lt properti gt lt properti gt lt name gt templeton zookeep host lt name gt lt valu gt c ambari apach org lt valu gt lt properti gt issu on node cluster ha enbal properti ha zookeep quorum etc hadoop conf core site xml valu host lt name gt ha zookeep quorum lt name gt lt valu gt c ambari apach org c ambari apach org c ambari apach org lt valu gt it chang either host ad c cluster instal zookeep server restart hdf,0,0,0,0,0,0,
3981,Services mysteriously disappear after Stack upgrade, servic mysteri disappear stack upgrad,,,0,0,0,0,0,0,
3982,Background operations window  called from wizard doesn't react to 'Do not show this dialog...' flag, background oper window call wizard react do show dialog flag,STR:Go through the Reassign NameNode wizard.On the last step click on the Start All Services link.Change the state of flag Do not show this dialog again when starting a background operation.Click OK.Click Start All Services link again.Result: State of flag was not changed., str go reassign name node wizard on last step click start all servic link chang state flag do show dialog start background oper click ok click start all servic link result state flag chang,0,0,0,0,0,0,
3984,Config Groups: Background popup show up needs to be integrated when restarting components, config group background popup show need integr restart compon,1. Change config group for a service.2. Click 'Stop components' on service config page.3. Click 'Start components' on the same page.Actual results:Background Operations popup will always show up. (as attached)Expected results:Load the 'do not show this dialog..' flag first  then determine if show this popup., chang config group servic click stop compon servic config page click start compon page actual result background oper popup alway show attach expect result load show dialog flag first determin show popup,0,0,0,0,0,0,
3986,YARN and MapReduce2 configs is not displayed, yarn map reduc config display,In Services -&gt; Configs YARN and MapReduce2 configs is not displayed., in servic gt config yarn map reduc config display,0,0,0,0,0,0,
3987,Resource providers are set with wrong stack version., resourc provid set wrong stack version,AbstractProviderModule.updateClusterVersion sets the cluster version for the resource providers with the following ... PropertyHelper.MetricsVersion version = clusterVersion.startsWith('HDP-1') ? PropertyHelper.MetricsVersion.HDP1 : PropertyHelper.MetricsVersion.HDP2;So  the Cluster/version property set to 'HDPLocal-1.3.2' will incorrectly be detected as HDP2. This causes the property providers to use the wrong metric mapping files which causes many JMX properties not to be set properly., abstract provid modul updat cluster version set cluster version resourc provid follow properti helper metric version version cluster version start with hdp properti helper metric version hdp properti helper metric version hdp so cluster version properti set hdp local incorrectli detect hdp thi caus properti provid use wrong metric map file caus mani jmx properti set properli,0,0,0,0,0,0,
3991,Manage config group links needed in save config-group confirmation, manag config group link need save config group confirm,When any service config-group is saved  we have a confirmation popup saying save was successful. We should enhance that popup to have a button to Manage Config Groups dialog  along with appropriate message. When button is clicked  the popup should go away and the Manage Config Groups dialog should show., when servic config group save confirm popup say save success we enhanc popup button manag config group dialog along appropri messag when button click popup go away manag config group dialog show,0,0,0,0,0,0,
3992,After making config changes w/o saving  prompt user if they try to navigate away, after make config chang w save prompt user tri navig away,1) Browse to Services &gt; HDFS &gt; Configs2) Change some props3) Do not click save4) Browse away  to Summary or to another serviceUser would have lost config changes. We should prompt before allowing user to navigate away from Configs.'You have unsaved changes. Save changes or discard?'&#91;Discard&#93; &#91;Save&#93;, brows servic gt hdf gt config chang prop do click save brows away summari anoth servic user would lost config chang we prompt allow user navig away config you unsav chang save chang discard discard save,0,0,0,0,0,0,
3997,Config-Group POST call should tolerate name reuse, config group post call toler name reus,Cluster wide we prohibit reuse of config-group name. However names can be reused across services. The API should be updated to tolerate POST/PUT of similar named config-groups., cluster wide prohibit reus config group name howev name reus across servic the api updat toler post put similar name config group,0,0,0,0,0,0,
3999,Long host names are inconvenient for viewing in background operations popup, long host name inconveni view background oper popup,Steps: Open background operations window and select any operation.Result: If the host name is too long  they are not placed on designated place.Solution:If the host name is too long  we show part of the string with '...' at the end.Also the string should keep in a single line all the time, step open background oper window select oper result if host name long place design place solut if host name long show part string end also string keep singl line time,0,0,0,0,0,0,
4003,Add Service Wizard: Customize Services configs are not displayed., add servic wizard custom servic config display,In 'Customize services' step in 'Add Service Wizard' config group and configs are not displayed. See screenshot., in custom servic step add servic wizard config group config display see screenshot,0,0,0,0,0,0,
4004,Duplicate hosts after closing addServiceWizard, duplic host close add servic wizard,Install cluster with 2 hosts (with HDFS  ZooKeeper).Go to Add Service Wizard.Select some configurable service.Go to Step 4 (Customize services).Close wizard.Go to Hosts page.Result:each host appears two times., instal cluster host hdf zoo keeper go add servic wizard select configur servic go step custom servic close wizard go host page result host appear two time,0,0,0,0,0,0,
4012,NameNode max heap is not showing in HDP 1.3.2 stack, name node max heap show hdp stack,Reason:We use in-consistent value to show Heap size for different components.And some of them are missing.Solution:Make sure all Heap size percentage value (including NN  RM  JT and HBase Master) use the same property., reason we use consist valu show heap size differ compon and miss solut make sure heap size percentag valu includ nn rm jt h base master use properti,0,0,0,0,0,0,
4024,HiveSchema file for Hive should be hive-schema-0.12.0.oracle.sql, hive schema file hive hive schema oracl sql,HiveSchema file for Hive should be hive-schema-0.12.0.oracle.sql, hive schema file hive hive schema oracl sql,0,0,0,0,0,0,
4028,Dashboard quick links polls desired_configs every 6s, dashboard quick link poll desir config everi,On the dashboard  calls to /clusters/{clusterName}?fields=Clusters/desired_configs are made every 6s. We need to verify if this really is necessary. Initial investigation revealed calls from quick-links  where links were set based on security being enabled. But there is no need to poll every 6s for this., on dashboard call cluster cluster name field cluster desir config made everi we need verifi realli necessari initi investig reveal call quick link link set base secur enabl but need poll everi,0,0,0,0,0,0,
4040,In installer  behavior of actions in manage config-groups dialog different from reconfigure, in instal behavior action manag config group dialog differ reconfigur,During reconfigure  in the Manage Config Groups dialog  the actions below the config-groups table (left table - Add/Remove/Duplicate/Rename) are immediate - you do not need to hit Save. Save is only for host membership changes. If host membership changes  the actions under left-table are disabled till Save. During install however  all actions are allowed till Save is hit. If you rename/duplicate and hit Cancel  all changes are lost - something which does not happen during reconfigure.The installer dialog should have similar behavior to reconfigure dialog., dure reconfigur manag config group dialog action config group tabl left tabl add remov duplic renam immedi need hit save save host membership chang if host membership chang action left tabl disabl till save dure instal howev action allow till save hit if renam duplic hit cancel chang lost someth happen reconfigur the instal dialog similar behavior reconfigur dialog,0,0,0,0,0,0,
4044,Refactor templates and popups, refactor templat popup,,,0,0,0,0,0,0,
4048,Minor manage-config-group dialog UI changes, minor manag config group dialog ui chang,Label on top of left hand config-group table should be removed Left hand table and right hand table should occupy 1/3 and 2/3 of the dialog., label top left hand config group tabl remov left hand tabl right hand tabl occupi dialog,0,0,0,0,0,0,
4052,Rename config-group dialog should allow only description change also, renam config group dialog allow descript chang also,We wanted to change just the config-group description  but were unable to change without changing the name. The rename config-group dialog should allow changing description only also., we want chang config group descript unabl chang without chang name the renam config group dialog allow chang descript also,0,0,0,0,0,0,
4054,Need to show stale-config indicator on hosts page, need show stale config indic host page,We show the restart indicator for stale-configs on services and individual host. Since we already have that information on the client  we need to show that on the Hosts page table. We need a filter to select stale-config hosts  and also show beside each host the stale-config indicator., we show restart indic stale config servic individu host sinc alreadi inform client need show host page tabl we need filter select stale config host also show besid host stale config indic,0,0,0,0,0,0,
4058,ambari-agent/server should start automatically upon reboot, ambari agent server start automat upon reboot,ambari-agent and server are not set to start automatically  so if a machine reboots it becomes inaccessible., ambari agent server set start automat machin reboot becom inaccess,0,0,0,0,0,0,
4069,Add hosts  if using Local Repository  UI incorrectly says 'no', add host use local repositori ui incorrectli say,On review page of install wizard/add host wizard -List the repos with their OS-Say 'Repositories'  not 'Local Repository'-Not a yes or no, on review page instal wizard add host wizard list repo os say repositori local repositori not ye,0,0,0,0,0,0,
4076,Installer's host list doesn't show masters at top, instal host list show master top,Installer's host list should show masters at top, instal host list show master top,0,0,0,0,0,0,
4089,HDFS/ZKFC relations in EmberData, hdf zkfc relat ember data,After enabling HA go to hosts page.Open host's page for host that has ZKFC.ZKFC doesn't have service.Expect:ZKFC should have HDFS as service., after enabl ha go host page open host page host zkfc zkfc servic expect zkfc hdf servic,0,0,0,0,0,0,
4092,Need tooltip showing error why local repo is bad, need tooltip show error local repo bad,I selected HDP 2.0.8 stack and Centos5 base-url was bad. The reason was given in the response - but it is not shown anywhere in UI. On the red exclamation mark we need a tooltip showing the error message from server along with HTTP error code., i select hdp stack cento base url bad the reason given respons shown anywher ui on red exclam mark need tooltip show error messag server along http error code,0,0,0,0,0,0,
4104,TestHardware.test_fqdnDomainHostname() fails, test hardwar test fqdn domain hostnam fail,The unit test failsFAIL: test_fqdnDomainHostname (TestHardware.TestHardware)----------------------------------------------------------------------Traceback (most recent call last): File '/home/dmitry/incubator-ambari/ambari-common/src/test/python/mock/mock.py'  line 1199  in patched return func(*args  **keywargs) File '/home/dmitry/incubator-ambari/ambari-agent/src/test/python/ambari_agent/TestHardware.py'  line 83  in test_fqdnDomainHostname self.assertEquals(result['hostname']  'ambari')AssertionError: 'dmitry-pc' != 'ambari', the unit test fail fail test fqdn domain hostnam test hardwar test hardwar traceback recent call last file home dmitri incub ambari ambari common src test python mock mock py line patch return func arg keywarg file home dmitri incub ambari ambari agent src test python ambari agent test hardwar py line test fqdn domain hostnam self assert equal result hostnam ambari assert error dmitri pc ambari,0,0,0,0,0,0,
4106,Should not allow blank config group name, should allow blank config group name,1) Create config group2) just enter a &lt;space&gt; for the name3) That enables the OK button to save4) You can save that config group w/o a name.is reproducible on installer (works fine after installation), creat config group enter lt space gt name that enabl ok button save you save config group w name reproduc instal work fine instal,0,0,0,0,0,0,
4118,Manage Config Group link appears on host detail page (and cannot dismiss it once opened), manag config group link appear host detail page cannot dismiss open,,,0,0,0,0,0,0,
4129,HA wizard not accessible after upgrade, ha wizard access upgrad,,,0,0,0,0,0,1,
4132,Stale config indicator not shown when reconfiguring WebHCat  Ganglia  Nagios  ZooKeeper, stale config indic shown reconfigur web h cat ganglia nagio zoo keeper,Steps: Create a config group for any of the mentioned services and add a host to the config group. Save the config group.Result:Restart indicators do not appear., step creat config group mention servic add host config group save config group result restart indic appear,0,0,0,0,0,0,
4135,Review page doesn't have info about Repositories, review page info repositori,Load Repositories info in the same way as for add Host Wizard., load repositori info way add host wizard,0,0,0,0,0,0,
4145,Ability to restart a component, abil restart compon,Add option to Actions menu for 'restart'. That should queue the stop and start tasks.We should assume 'RESTART' as a default command. The base/default implementation is to call STOP and then START., add option action menu restart that queue stop start task we assum restart default command the base default implement call stop start,0,0,0,0,0,0,
4152,Service Config page shows a blank page after enabling security (JS error), servic config page show blank page enabl secur js error,UI don't respond after enabling security, ui respond enabl secur,0,0,0,0,0,0,
4171,Fix UI Unit tests, fix ui unit test,,,0,0,0,0,0,0,
4177,YARN check execute fails on install, yarn check execut fail instal,YARN check execute fails on install. See attached logs.IP Address on Hosts Page is shown as OS NOT SUPPORTED (w/o js errors  cluster installed using regular Nano-CentOS5.9 image type), yarn check execut fail instal see attach log ip address host page shown os not support w js error cluster instal use regular nano cent os imag type,0,0,0,0,0,0,
4184,Fix versions of rpms in the stack to match the installed ones., fix version rpm stack match instal one,Fix versions of rpms in the stack to match the installed ones., fix version rpm stack match instal one,0,0,0,0,0,0,
4186,Global configs are not sent to server, global config sent server,On the Deploy step Global Configs are not sent to server.So  Nagios and Ganglia (if installed via add service wizard) don't have any configs in service config page., on deploy step global config sent server so nagio ganglia instal via add servic wizard config servic config page,0,0,0,0,0,0,
4195,Misalignment of 'Filter' combobox on installer when browser window is narrow, misalign filter combobox instal browser window narrow,,,0,0,0,0,0,0,
4200,Minor UI cleanup - remove double-borders  reduce text, minor ui cleanup remov doubl border reduc text,Remove double borders on alerts and host component page sections Remove some text to not wrap in 'delete host' dialog Remove 'Hosts' table header to clean-up config group dialog, remov doubl border alert host compon page section remov text wrap delet host dialog remov host tabl header clean config group dialog,0,0,0,0,0,0,
4206,Significant lag between host status update and slave/master component start/stop, signific lag host statu updat slave master compon start stop,Steps: Go to any host with slave component (DataNode  for example). Click 'Stop' options for slave component.Result: status marker for slave component start blinking red  but status marker for host behaves strangely: sometimes it also start blinking red immediately/ sometimes changes status to not-blinking red only after slave component stopping will be ended.The same for slave component starting., step go host slave compon data node exampl click stop option slave compon result statu marker slave compon start blink red statu marker host behav strang sometim also start blink red immedi sometim chang statu blink red slave compon stop end the slave compon start,0,0,0,0,0,0,
4207,Frontend: History server should be managed as separate component, frontend histori server manag separ compon,For stack 1.x history server will be as separate master component. Add ability to choose host on which this component will be installed. Set value for 'mapreduce.history.server.http.address'  i.e. &lt;historyserverHost&gt;:&lt;historyServerPort&gt;., for stack x histori server separ master compon add abil choos host compon instal set valu mapreduc histori server http address e lt historyserv host gt lt histori server port gt,0,0,0,0,0,0,
4211,Empty content of review step in Add Host wizard, empti content review step add host wizard,JS error: Uncaught TypeError: Cannot call method 'forEach' of undefined, js error uncaught type error cannot call method each undefin,0,0,0,0,0,0,
4217,Mirroring page redesign, mirror page redesign,Redesign existing Mirroring page with datasets table according to new attached mockups., redesign exist mirror page dataset tabl accord new attach mockup,0,0,0,0,0,0,
4220,Padding for config override properties, pad config overrid properti,,,0,0,0,0,0,0,
4224,When issuing Start/Stop of host components then predicate stale_config=true does not work, when issu start stop host compon predic stale config true work,See the series of curl calls below.There are no components with stale configs but 4 components are being stopped. There are only 4 because Ambari only allows INSTALL call on already installed clients[root@c6401 vagrant]# curl -i -uadmin:admin -H 'X-Requested-By: ambari' http://c6401.ambari.apache.org:8080/api/v1/clusters/c1/host_components?HostRoles/stale_configs=true{ 'href' : 'http://c6401.ambari.apache.org:8080/api/v1/clusters/c1/host_components?HostRoles/stale_configs=true'  'items' : [ ]}[root@c6401 vagrant]# curl -i -uadmin:admin -H 'X-Requested-By: ambari'-d '{'HostRoles': { 'state': 'INSTALLED'}}' -X PUT http://c6401.ambari.apache.org:8080/api/v1/clusters/c1/host_components?HostRoles/stale_configs=true{ 'href' : 'http://c6401.ambari.apache.org:8080/api/v1/clusters/c1/requests/14'  'Requests' : { 'id' : 14  'status' : 'InProgress' }}[root@c6401 vagrant]# curl -i -uadmin:admin -H 'X-Requested-By: ambari' http://c6401.ambari.apache.org:8080/api/v1/clusters/c1/requests/14{ 'href' : 'http://c6401.ambari.apache.org:8080/api/v1/clusters/c1/requests/14'  'Requests' : { 'aborted_task_count' : 0  'cluster_name' : 'c1'  'completed_task_count' : 0  'failed_task_count' : 0  'id' : 14  'progress_percent' : 9.0  'queued_task_count' : 4  'request_context' : ''  'request_status' : 'PENDING'  'task_count' : 4  'timed_out_task_count' : 0 }  'tasks' : [ { 'href' : 'http://c6401.ambari.apache.org:8080/api/v1/clusters/c1/requests/14/tasks/178'  'Tasks' : { 'cluster_name' : 'c1'  'id' : 178  'request_id' : 14 } }  { 'href' : 'http://c6401.ambari.apache.org:8080/api/v1/clusters/c1/requests/14/tasks/179'  'Tasks' : { 'cluster_name' : 'c1'  'id' : 179  'request_id' : 14 } }  { 'href' : 'http://c6401.ambari.apache.org:8080/api/v1/clusters/c1/requests/14/tasks/180'  'Tasks' : { 'cluster_name' : 'c1'  'id' : 180  'request_id' : 14 } }  { 'href' : 'http://c6401.ambari.apache.org:8080/api/v1/clusters/c1/requests/14/tasks/181'  'Tasks' : { 'cluster_name' : 'c1'  'id' : 181  'request_id' : 14 } } ]}, see seri curl call there compon stale config compon stop there ambari allow instal call alreadi instal client root c vagrant curl uadmin admin h x request by ambari http c ambari apach org api v cluster c host compon host role stale config true href http c ambari apach org api v cluster c host compon host role stale config true item root c vagrant curl uadmin admin h x request by ambari host role state instal x put http c ambari apach org api v cluster c host compon host role stale config true href http c ambari apach org api v cluster c request request id statu in progress root c vagrant curl uadmin admin h x request by ambari http c ambari apach org api v cluster c request href http c ambari apach org api v cluster c request request abort task count cluster name c complet task count fail task count id progress percent queu task count request context request statu pend task count time task count task href http c ambari apach org api v cluster c request task task cluster name c id request id href http c ambari apach org api v cluster c request task task cluster name c id request id href http c ambari apach org api v cluster c request task task cluster name c id request id href http c ambari apach org api v cluster c request task task cluster name c id request id,0,0,0,0,0,0,
4225,Starting/Stopping components based on restart indicator floods request history and execution queue, start stop compon base restart indic flood request histori execut queue,When starting/stopping components based on the Start/Stop Components buttons that show up after service reconfiguration  it creates one request per component (as shown in the Background Operations popup). This floods the request history and the user cannot see what has been done prior. From the user's standpoint  Start Components and Stop Components actions should each show up as one request.Also  this has major performance implications  since multiple requests cannot be processed in parallel by the server (unlike tasks within a single request)., when start stop compon base start stop compon button show servic reconfigur creat one request per compon shown background oper popup thi flood request histori user cannot see done prior from user standpoint start compon stop compon action show one request also major perform implic sinc multipl request cannot process parallel server unlik task within singl request,0,0,0,0,1,0,
4231,Storm: Update Dashboard / Services to support Storm, storm updat dashboard servic support storm,Define mock data and make this functional in App.testMode.E2E integration will be a separate task., defin mock data make function app test mode e e integr separ task,0,0,0,0,0,0,
4247,Restart marker does not show up sometimes in the Hosts page, restart marker show sometim host page,After reconfiguring  restart indicators do not show up on the Host pages sometimes.Clicking on an individual host shows the restart indicator in the Host Details page., after reconfigur restart indic show host page sometim click individu host show restart indic host detail page,0,0,0,0,0,0,
4265,Add env AMBARI_JVM_ARGS to ambari-server start, add env ambari jvm arg ambari server start,Useful if you want to set AMBARI_JVM_ARGS in the environment. For example: Setting http proxy that Ambari Serverexport AMBARI_JVM_ARGS='-Dhttp.proxyHost=the.proxy.host -Dhttp.proxyPort=1234'ambari-server start, use want set ambari jvm arg environ for exampl set http proxi ambari serverexport ambari jvm arg dhttp proxi host proxi host dhttp proxi port ambari server start,0,0,0,0,0,0,
4279,Status commands are not executed for new services, statu command execut new servic,For new services (those  that are not hardcoded at LiveStatus.py)  status commands are not executed., for new servic hardcod live statu py statu command execut,0,0,0,0,0,0,
4284,Alerts  Restart  Maintenance elements in the Hosts filters, alert restart mainten element host filter,Move this 3 elements to the second line (under All  Healthy...).Also refactor their code-realization., move element second line all healthi also refactor code realiz,0,0,0,0,0,0,
4299,Ambari server unit test failure, ambari server unit test failur,Results :Failed tests: testDoWork(org.apache.ambari.server.state.scheduler.BatchRequestJobTest): (..)Tests run: 1265  Failures: 1  Errors: 0  Skipped: 8, result fail test test do work org apach ambari server state schedul batch request job test test run failur error skip,0,0,0,0,0,0,
4300,Service tab: growing number of calls to update alerts, servic tab grow number call updat alert,After routing by services on Service page number of calls to server(to get Alerts) grows.Also mock json with alerts need to be added., after rout servic servic page number call server get alert grow also mock json alert need ad,0,0,0,0,0,0,
4306,Request Schedule status not updated for Point in time execution request, request schedul statu updat point time execut request,API call:curl -u admin:admin -H 'X-Requested-By:ambari' -i -X POST -d '[{'RequestSchedule':{'batch':[{'requests':[{'order_id' : '1' 'type':'POST' 'uri':'/api/v1/clusters/c1/requests' 'RequestBodyInfo':{'RequestInfo':{'context':'Restart Nagios' 'command':'RESTART' 'service_name':'NAGIOS' 'component_name':'NAGIOS_SERVER' 'hosts':'c6401.ambari.apache.org'}}}]} {'batch_settings':{'batch_separation_in_seconds':120 'task_failure_tolerance':1}}]}}]' http://c6401.ambari.apache.org:8080/api/v1/clusters/c1/request_schedulesRequest Schedule:{href: 'http://c6401.ambari.apache.org:8080/api/v1/clusters/c1/request_schedules/23' RequestSchedule: {batch: {batch_requests: [{order_id: 1 request_type: 'POST' request_uri: '/api/v1/clusters/c1/requests' request_body: '{'RequestInfo':{'context':'Restart Nagios' 'command':'RESTART' 'service_name':'NAGIOS' 'component_name':'NAGIOS_SERVER' 'hosts':'c6401.ambari.apache.org'}}' request_status: 'InProgress' return_code: 202}] batch_settings: {batch_separation_in_seconds: 120 task_failure_tolerance_limit: 1}} cluster_name: 'c1' description: null id: 23 last_execution_status: 'InProgress' schedule: null status: 'SCHEDULED'}}, api call curl u admin admin h x request by ambari x post request schedul batch request order id type post uri api v cluster c request request bodi info request info context restart nagio command restart servic name nagio compon name nagio server host c ambari apach org batch set batch separ second task failur toler http c ambari apach org api v cluster c request schedul request schedul href http c ambari apach org api v cluster c request schedul request schedul batch batch request order id request type post request uri api v cluster c request request bodi request info context restart nagio command restart servic name nagio compon name nagio server host c ambari apach org request statu in progress return code batch set batch separ second task failur toler limit cluster name c descript null id last execut statu in progress schedul null statu schedul,0,0,0,0,0,0,
4312,Storm: Install wizard. Add master components Storm UI Server  DRPC Server  LogViewer Server, storm instal wizard add master compon storm ui server drpc server log viewer server,Add new master components to Install Wizard -&gt; Assign Masters page., add new master compon instal wizard gt assign master page,0,0,0,0,0,0,
4316,Storm: Add Storm UI Server  DRPC Server  LogViewer Server components config categories., storm add storm ui server drpc server log viewer server compon config categori,Add master components Storm UI Server  DRPC Server  Log Viewer Server. Create config categories for components. Replace exist configs according to categories., add master compon storm ui server drpc server log viewer server creat config categori compon replac exist config accord categori,1,0,0,0,0,0,
4318,Service Restart All action cleanup, servic restart all action cleanup,1. There is no need to send request to restart clients.2. When user clicks on Restart All actions  components start restarting immediately. But all other actions shows confirmation popup like 'Are you sure?' (Start  Stop  Run Service Check etc)., there need send request restart client when user click restart all action compon start restart immedi but action show confirm popup like are sure start stop run servic check etc,1,0,0,0,0,0,
4319,Task timeout should be a configurable knob at the ambari-server, task timeout configur knob ambari server,Task timeout is a configurable knob at the ambari-agent.timeout_seconds = 600This opens up the possibility of different timeout value at different agent instances as well as different value between the server and the agent. This can lead to state where server may have timed out the tasks but agent may not have. The other way is benign.This config can be moved to the server and server can hand it off to the agent when agent registers. This also makes it easier to manage the timeout value., task timeout configur knob ambari agent timeout second thi open possibl differ timeout valu differ agent instanc well differ valu server agent thi lead state server may time task agent may the way benign thi config move server server hand agent agent regist thi also make easier manag timeout valu,1,0,0,0,0,0,
4333,Incorrect behavior of 'Save' button in 'Manage Configuration Groups' window, incorrect behavior save button manag configur group window,Steps: Go to 'Customize Services' page. Open 'Manage Configuration Groups' window for any service. Add new custom group and assign host to it. Save changes. Open 'Manage Configuration Groups' window again. Select custom group and remove assigned host.Result: 'Save' button is disabled  'Add hosts' also is disabled.Note: after selecting default group 'Save' button become active.Also if after all steps try to add hosts to custom group  'Save' button will not be active., step go custom servic page open manag configur group window servic add new custom group assign host save chang open manag configur group window select custom group remov assign host result save button disabl add host also disabl note select default group save button becom activ also step tri add host custom group save button activ,0,0,0,0,0,0,
4336,Move 1.3.4 stack to 1.3.3 using the python libraries., move stack use python librari,Move 1.3.4 stack to 1.3.3 using the pythin libraries., move stack use pythin librari,1,0,0,0,0,0,
4341,Rename 2.0.8 to 2.1.1 in the stack definition., renam stack definit,Rename 2.0.8 to 2.1.1 in the stack definition., renam stack definit,1,0,0,0,0,0,
4349,Slaves API-calls, slave api call,For Slaves (DataNodes  NodeManagers  RegionServers  or TaskTrackers): Start Stop Restart Decommission Recommission, for slave data node node manag region server task tracker start stop restart decommiss recommiss,1,0,0,0,0,0,
4354,HostCleanup should also clean /tmp/hadoop-*, host cleanup also clean tmp hadoop,HostCleanup should also check the following directories (and clean):/tmp/hadoop-*For example  if the /tmp/hadoop-nagios directory is present  but it doesn't have the right ownership/perms for nagios user  the Hive Metastore Nagios alert will occur. I saw this after doing an install on an un-clean machine.Hive Metastore statusCRIT for about a minuteCRITICAL: Error accessing Hive Metastore status [Error creating temp dir in hadoop.tmp.dir /tmp/hadoop-nagios due to Permission denied]An easy way to reproduce this:1) Perform install2) Go to Nagios server machine3) Change perms on /tmp/hadoop-nagios so that the nagios user does not have access4) The Nagios alert will fire, host cleanup also check follow directori clean tmp hadoop for exampl tmp hadoop nagio directori present right ownership perm nagio user hive metastor nagio alert occur i saw instal un clean machin hive metastor statu crit minut critic error access hive metastor statu error creat temp dir hadoop tmp dir tmp hadoop nagio due permiss deni an easi way reproduc perform instal go nagio server machin chang perm tmp hadoop nagio nagio user access the nagio alert fire,1,0,0,0,0,0,
4355,Add relocate resources scripts to the pom file, add reloc resourc script pom file,The relocate resources python script is not a part of the rpm package., the reloc resourc python script part rpm packag,1,0,0,0,0,0,
4361,Rolling restart failure tolerance should be percentage values, roll restart failur toler percentag valu,Currently when we make rolling restart API calls  the task_failure_tolerance value is given as count of hosts. Rather it should be percentage of total hosts., current make roll restart api call task failur toler valu given count host rather percentag total host,1,0,0,0,0,0,
4365,Action definitions should be provided as declarative resources - read from XML files, action definit provid declar resourc read xml file,Currently  action definition are stored in database. This is not in-line with the declarative definition of stack and custom commands (as part of stack). The custom actions are essentially same as custom commands except they are defined at the level of clusters. So we should move the custom actions to XML formatted files that ambari-server can read when starting up. This means that when one needs to make any edit they will have to restart ambari-server. This requirement is OK as adding/modifying custom actions is not a frequently done operation., current action definit store databas thi line declar definit stack custom command part stack the custom action essenti custom command except defin level cluster so move custom action xml format file ambari server read start thi mean one need make edit restart ambari server thi requir ok ad modifi custom action frequent done oper,1,0,0,0,0,0,
4367,Alerts block shows spinner if Nagios not installed, alert block show spinner nagio instal,When Nagios not installed Alerts block should show message that it's not installed instead of spinner., when nagio instal alert block show messag instal instead spinner,0,0,0,0,0,0,
4380,Hosts API-calls, host api call,For each host:P0: Start All Components Stop All ComponentsP1: Restart All Components, for host p start all compon stop all compon p restart all compon,1,0,0,0,0,0,
4383,Datanode data directory is not created correctly, datanod data directori creat correctli,dfs.datanode.data.dir must be handled as comma separated directories., df datanod data dir must handl comma separ directori,1,0,0,0,0,0,
4395,ambari-server should use repo when downloading jdk 7, ambari server use repo download jdk,,,0,0,0,0,0,0,
4396,Misc code cleanup, misc code cleanup,,,1,0,0,0,0,0,
4402,Delete Config Group Host mapping broken due to error introduced by perf patch, delet config group host map broken due error introduc perf patch,Unit test: org.apache.ambari.server.state.ConfigGroupTest#testRemoveHostThis unit test is not a part of 1.4.3 branch  it was added later. (trunk)Exception thrown during ConfigGroupImpl.removeHost()2014-01-06 17:46:35 989 ERROR [main] configgroup.ConfigGroupImpl (ConfigGroupImpl.java:removeHost(274)) - Failed to delete config group host mapping  clusterName = foo  id = 1  hostname = h1java.lang.IllegalArgumentException: Object: org.apache.ambari.server.orm.cache.ConfigGroupHostMappingImpl@cc34948d is not a known entity type. at org.eclipse.persistence.internal.sessions.UnitOfWorkImpl.performRemove(UnitOfWorkImpl.java:3538) at org.eclipse.persistence.internal.jpa.EntityManagerImpl.remove(EntityManagerImpl.java:518) at org.apache.ambari.server.orm.dao.ConfigGroupHostMappingDAO.removeByPK(ConfigGroupHostMappingDAO.java:250) at com.google.inject.persist.jpa.JpaLocalTxnInterceptor.invoke(JpaLocalTxnInterceptor.java:58) at org.apache.ambari.server.state.configgroup.ConfigGroupImpl.removeHost(ConfigGroupImpl.java:272) at com.google.inject.persist.jpa.JpaLocalTxnInterceptor.invoke(JpaLocalTxnInterceptor.java:66) at org.apache.ambari.server.state.cluster.ClustersImpl.deleteConfigGroupHostMapping(ClustersImpl.java:640) at org.apache.ambari.server.state.cluster.ClustersImpl.unmapHostFromCluster(ClustersImpl.java:615) at org.apache.ambari.server.state.ConfigGroupTest.testRemoveHost(ConfigGroupTest.java:203), unit test org apach ambari server state config group test test remov host thi unit test part branch ad later trunk except thrown config group impl remov host error main configgroup config group impl config group impl java remov host fail delet config group host map cluster name foo id hostnam h java lang illeg argument except object org apach ambari server orm cach config group host map impl cc known entiti type org eclips persist intern session unit of work impl perform remov unit of work impl java org eclips persist intern jpa entiti manag impl remov entiti manag impl java org apach ambari server orm dao config group host map dao remov by pk config group host map dao java com googl inject persist jpa jpa local txn interceptor invok jpa local txn interceptor java org apach ambari server state configgroup config group impl remov host config group impl java com googl inject persist jpa jpa local txn interceptor invok jpa local txn interceptor java org apach ambari server state cluster cluster impl delet config group host map cluster impl java org apach ambari server state cluster cluster impl unmap host from cluster cluster impl java org apach ambari server state config group test test remov host config group test java,0,0,0,0,0,1,
4404,mapreduce.task.io.sort.mb max value should not exceed 1024mb, mapreduc task io sort mb max valu exceed mb,https://issues.apache.org/jira/browse/MAPREDUCE-5028https://issues.apache.org/jira/browse/MAPREDUCE-2308, http issu apach org jira brows mapreduc http issu apach org jira brows mapreduc,1,0,0,0,1,0,
4405,Remove property fs.checkpoint.size during upgrade, remov properti fs checkpoint size upgrad,Property fs.checkpoint.size is deprecated. The upgrade script should remove it. Users can add the replacement themselves - dfs.namenode.checkpoint.txns., properti fs checkpoint size deprec the upgrad script remov user add replac df namenod checkpoint txn,1,0,0,0,0,0,
4408,Background operations dialog in weird state after exception, background oper dialog weird state except,Background operations dialog goes into a weird state after hitting an exception with request_schedule information. It always expects request_schedule information to be present., background oper dialog goe weird state hit except request schedul inform it alway expect request schedul inform present,1,0,1,0,0,0,
4416,HDFS start failed on 2.1.1 stack, hdf start fail stack,STR: Deployed minimal cluster with HDFS and ZK. HDFS start failed. Added YARN+MR2  Nagios and Ganglia. Picture with HDFS was the same.Output:Fail: Execution of 'ulimit -c unlimited &amp;&amp; if [ 'ulimit -c' != 'unlimited' ]; then exit 77; fi &amp;&amp; export HADOOP_LIBEXEC_DIR=/usr/lib/hadoop/libexec &amp;&amp; /usr/lib/hadoop/sbin/hadoop-daemon.sh --config /etc/hadoop/conf start secondarynamenode' returned 1. -bash: line 0: ulimit: core file size: cannot modify limit: Operation not permittedFull folders with logs are attached., str deploy minim cluster hdf zk hdf start fail ad yarn mr nagio ganglia pictur hdf output fail execut ulimit c unlimit amp amp ulimit c unlimit exit fi amp amp export hadoop libexec dir usr lib hadoop libexec amp amp usr lib hadoop sbin hadoop daemon sh config etc hadoop conf start secondarynamenod return bash line ulimit core file size cannot modifi limit oper permit full folder log attach,1,0,0,0,0,0,
4420,ORA-01795: maximum number of expressions in a list is 1000 for Oracle DB, ora maximum number express list oracl db,PROBLEM:ORA-01795: maximum number of expressions in a list is 1000 in Ambari Server log. Customer recently upgraded to Ambari 1.4.2Error is:08:54:51 320 ERROR [qtp1280560314-2070] ReadHandler:84 - Caught a runtime exception executing a queryLocal Exception Stack: Exception [EclipseLink-4002] (Eclipse Persistence Services - 2.4.0.v20120608-r11652): org.eclipse.persistence.exceptions.DatabaseExceptionInternal Exception: java.sql.SQLSyntaxErrorException: ORA-01795: maximum number of expressions in a list is 1000Error Code: 1795Call: SELECT task_id  attempt_count  event  exitcode  host_name  last_attempt_time  request_id  role  role_command  stage_id  start_time  status  std_error  std_out FROM host_role_command WHERE (task_id IN (? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ?)) ORDER BY task_id bind =&gt; [2551 parameters bound]Query: ReadAllQuery(referenceClass=HostRoleCommandEntity sql='SELECT task_id  attempt_count  event  exitcode  host_name  last_attempt_time  request_id  role  role_command  stage_id  start_time  status  std_error  std_out FROM host_role_command WHERE (task_id IN ?) ORDER BY task_id') at org.eclipse.persistence.exceptions.DatabaseException.sqlException(DatabaseException.java:333) at org.eclipse.persistence.internal.databaseaccess.DatabaseAccessor.basicExecuteCall(DatabaseAccessor.java:646) at org.eclipse.persistence.internal.databaseaccess.DatabaseAccessor.executeCall(DatabaseAccessor.java:537) at org.eclipse.persistence.internal.sessions.AbstractSession.basicExecuteCall(AbstractSession.java:1800)STEPS TO REPRODUCE: Over 1000 entries in the host_role_command and execution_command tables when oracle is used for Ambari backend databseACTUAL BEHAVIOR: Oracle throws the errorEXPECTED BEHAVIOR: There should be a limit to prevent this (possibly modify the syntax of the oracle query), problem ora maximum number express list ambari server log custom recent upgrad ambari error error qtp read handler caught runtim except execut queri local except stack except eclips link eclips persist servic v r org eclips persist except databas except intern except java sql sql syntax error except ora maximum number express list error code call select task id attempt count event exitcod host name last attempt time request id role role command stage id start time statu std error std from host role command where task id in order by task id bind gt paramet bound queri read all queri refer class host role command entiti sql select task id attempt count event exitcod host name last attempt time request id role role command stage id start time statu std error std from host role command where task id in order by task id org eclips persist except databas except sql except databas except java org eclips persist intern databaseaccess databas accessor basic execut call databas accessor java org eclips persist intern databaseaccess databas accessor execut call databas accessor java org eclips persist intern session abstract session basic execut call abstract session java step to reproduc over entri host role command execut command tabl oracl use ambari backend databs actual behavior oracl throw error expect behavior there limit prevent possibl modifi syntax oracl queri,1,0,0,0,0,0,
4425,Add upgradestack support for MySQL, add upgradestack support my sql,Similar to Oracle/Postgres we need to make sure that ambari-server upgradestack works for MySQL, similar oracl postgr need make sure ambari server upgradestack work my sql,1,0,0,0,0,0,
4433,Ambari version info should be visible in UI, ambari version info visibl ui,There is currently no easy way to find the version of Ambari that is in use from the Ambari Web interface. This info should be accessible via an 'about' link in Ambari Web, there current easi way find version ambari use ambari web interfac thi info access via link ambari web,1,0,0,0,0,0,
4501,health status yellow (lost heartbeat) not showing icon, health statu yellow lost heartbeat show icon,See attached. Using 1.5.0.335On Hosts page. Other pages seem fine., see attach use on host page other page seem fine,0,0,0,0,0,0,
4521,Bulk Ops: Restart on Slaves should popup rolling restart dialog, bulk op restart slave popup roll restart dialog,When performing Restart on Slaves via Bulk Ops menu on the Hosts page  Restart should popup the rolling restart dialog  just like it does when Restarting Slaves under Service Actions., when perform restart slave via bulk op menu host page restart popup roll restart dialog like restart slave servic action,1,0,0,0,0,0,
4523,Host registering failure from primary/agent os checking on centos6, host regist failur primari agent os check cento,I am using Ambari (1.4.3.38) for hadoop cluster installation and management. All the cluster nodes are built on centos 6.0.During the ambari server installation  ambari-server recognized the primary/cluster os as redhat6 (see ambari.properties). During the ambari agent bootstrap/host register  ambari-agent regonized the agent os as centos linux6 (see log). From log files (ambari-server.log  ambari-agent.log)  I found the inconsistence caused the warning of ambari-agent bootstrapping and failure of host registering.I'm still not sure why this happen  but I guess it's caused by the differene of os checking methods among ambari server side code  ambari-agent bootstrap script (os_type_check.sh based on os release file) and registering script (Controller.py/Register.py based on os hardware profile) .I just share to see if anyone can fix the issue.BTW  for me  to solve the problem  I manually edited the script files to make it work temporarily:To avoid warning of agent bootstrapping  in os_type_check.sh  add current_os=$RH6 above the echo line or add res=0 after case statement;To make the node register work  in Controller.py  add data=data.replace('centos linux' 'redhat') before sending registering request;Thanks., i use ambari hadoop cluster instal manag all cluster node built cento dure ambari server instal ambari server recogn primari cluster os redhat see ambari properti dure ambari agent bootstrap host regist ambari agent regon agent os cento linux see log from log file ambari server log ambari agent log i found inconsist caus warn ambari agent bootstrap failur host regist i still sure happen i guess caus differen os check method among ambari server side code ambari agent bootstrap script os type check sh base os releas file regist script control py regist py base os hardwar profil i share see anyon fix issu btw solv problem i manual edit script file make work temporarili to avoid warn agent bootstrap os type check sh add current os rh echo line add re case statement to make node regist work control py add data data replac cento linux redhat send regist request thank,1,0,0,0,0,0,
4526,Oozie Server installation fails when Falcon is selected, oozi server instal fail falcon select,,,1,0,0,0,0,0,
4534,Add ability to delete individual DataNode  TaskTracker  NodeManager  and RegionServer from Host Details page, add abil delet individu data node task tracker node manag region server host detail page,Note: We let the user delete Storm Supervisor already., note we let user delet storm supervisor alreadi,1,0,0,0,0,0,
4551,Alert count badge and restart indicator issues, alert count badg restart indic issu,The alert badge shown in the left nav shows up a bit strange (too little padding on the right). Also  when the restart indicator appears  the padding for the alert badge fixes itself  but the restart indicator appears too close. See attached., the alert badg shown left nav show bit strang littl pad right also restart indic appear pad alert badg fix restart indic appear close see attach,1,0,0,0,0,0,
4552,OOS status for component on host detail page makes button too big, oo statu compon host detail page make button big,,,1,0,0,0,0,0,
4556,Refactor and Unit tests for host summary, refactor unit test host summari,,,1,0,0,0,0,0,
4560,BG operation pop-up: JS error encountered on clicking on host in progress state., bg oper pop js error encount click host progress state,,,1,0,0,0,0,0,
4561,Falcon Client install task shows up as just 'install' rather than 'Falcon Client install', falcon client instal task show instal rather falcon client instal,See attached., see attach,0,0,0,0,0,0,
4566,Jobs: implement the TEZ DAG page/panel, job implement tez dag page panel,Need a graph implemented to show the Tez DAG for Hive queries, need graph implement show tez dag hive queri,1,0,0,0,0,0,
4567,Ambari should check that there is enough disk space during installation, ambari check enough disk space instal,Add a new category in 'Hosts Check' popup after registering hosts.So for '/' mountpoint  we check if the disk free space is larger than 2.0GB.And for '/usr/lib' or '/usr'  check the free space is larger than 1.0GB. Then show related warning message if any of them got space shortage., add new categori host check popup regist host so mountpoint check disk free space larger gb and usr lib usr check free space larger gb then show relat warn messag got space shortag,0,0,0,0,0,0,
4570,Various issues related to decommission support, variou issu relat decommiss support,Tracking few issues related to decommission support Even with HBase HA only one master should be used for decommission Alternatively  for HDFS/MR/YARN use all master host components Do not use 'default' method while using python based system resources Add support for command details and custom command names, track issu relat decommiss support even h base ha one master use decommiss altern hdf mr yarn use master host compon do use default method use python base system resourc add support command detail custom command name,1,0,0,0,0,0,
4574,Upon restart of ambari-server  the service status on Dashboard page remain unchanged, upon restart ambari server servic statu dashboard page remain unchang,After ambari-server restart  if FE remains at the Dashboard page  the service status (Yellow buttons) never gets updated even if the API indicates that the status are all Green/Red. In my case  never is 6 minutes.Upon refresh or moving to other tabs the view is promptly updated., after ambari server restart fe remain dashboard page servic statu yellow button never get updat even api indic statu green red in case never minut upon refresh move tab view promptli updat,1,0,0,0,0,0,
4575,Host Details > Actions pulldown likes to hide, host detail action pulldown like hide,Actions pull down on the Host Detail page likes to hide itself when it's open. This is a bit annoying., action pull host detail page like hide open thi bit annoy,1,0,0,0,0,0,
4600,Remove --jce-policy from warning statement, remov jce polici warn statement,We have removed option -c or --jce-policy. So the warning message should not use that option in the help statement.jce_download_fail_msg = ' Failed to download JCE Policy archive : {0}. ' / 'Please check that JCE Policy archive is available ' / 'at {1} . Also you may install JCE Policy archive manually using ' / '--jce-policy command line argument.'.format('{0}'  jce_url), we remov option c jce polici so warn messag use option help statement jce download fail msg fail download jce polici archiv pleas check jce polici archiv avail also may instal jce polici archiv manual use jce polici command line argument format jce url,0,0,0,0,0,0,
4601,adding more master components styling is missing, ad master compon style miss,,,0,0,0,0,0,0,
4603,Host names goes under text  icons, host name goe text icon,see attached, see attach,1,0,0,0,0,0,
4605,Host details: clients list disappears, host detail client list disappear,Go to host page with some clients installedRefresh itGot: clients are not displayed on the pageExpected: list of installed clients, go host page client instal refresh got client display page expect list instal client,0,0,0,0,0,0,
4608,medkit-icons are shifted out of their places  in hosts table in Safari., medkit icon shift place host tabl safari,,,0,0,0,0,0,0,
4612,Exception on deploing step: java.sql.BatchUpdateException: ORA-00942, except deplo step java sql batch updat except ora,Exception trace:18:27:19 191 WARN [qtp1643608425-22] ServletHandler:514 - /api/v1/clusters/c1/servicesjavax.persistence.RollbackException: Exception [EclipseLink-4002] (Eclipse Persistence Services - 2.4.0.v20120608-r11652): org.eclipse.persistence.exceptions.DatabaseExceptionInternal Exception: java.sql.BatchUpdateException: ORA-00942: table or view does not existError Code: 942 at org.eclipse.persistence.internal.jpa.transaction.EntityTransactionImpl.commitInternal(EntityTransactionImpl.java:102) at org.eclipse.persistence.internal.jpa.transaction.EntityTransactionImpl.commit(EntityTransactionImpl.java:63) at com.google.inject.persist.jpa.JpaLocalTxnInterceptor.invoke(JpaLocalTxnInterceptor.java:87), except trace warn qtp servlet handler api v cluster c servicesjavax persist rollback except except eclips link eclips persist servic v r org eclips persist except databas except intern except java sql batch updat except ora tabl view exist error code org eclips persist intern jpa transact entiti transact impl commit intern entiti transact impl java org eclips persist intern jpa transact entiti transact impl commit entiti transact impl java com googl inject persist jpa jpa local txn interceptor invok jpa local txn interceptor java,0,0,0,0,0,0,
4635,'SNN Process' alert displays after HA enabled successfully, snn process alert display ha enabl success,STR:1. Install  setup and start Ambari server by default.2. Deploy Hadoop by default (stack 2.0.6  choose all services to install).3. Enable HA.4. Wait for at least 150 seconds.4. Go to HDFS Service/Hosts page page.Actual result:'Secondary NameNode Process' alert displayed.Expected result:There should not be 'Secondary NameNode' alert on page., str instal setup start ambari server default deploy hadoop default stack choos servic instal enabl ha wait least second go hdf servic host page page actual result secondari name node process alert display expect result there secondari name node alert page,1,0,0,0,0,0,
4637,The user is not always redirected to the login page when unauthenticated, the user alway redirect login page unauthent,1. Try to enable HA with fail on 9th step for Start All services2. Use link http://&lt;host&gt;:8080/#/main/admin/highAvailability/enable/step13. HA Wizard is shown without previously loaded Login Page (see url and js errors in attached screenshot)Expexted result:Login Page should be loaded firts., tri enabl ha fail th step start all servic use link http lt host gt main admin high avail enabl step ha wizard shown without previous load login page see url js error attach screenshot expext result login page load firt,1,0,0,0,0,0,
4643,Restart All fails for Client only components, restart all fail client compon,Restart for a service calls stop and start on all components. Currently there is no implementation for stopping a client component. This leads to error message: 'Stop not implemented for component', restart servic call stop start compon current implement stop client compon thi lead error messag stop implement compon,1,0,0,0,0,0,
4658,Routes are incorrect after launching wizard, rout incorrect launch wizard,Steps to reproduce:1. Launch Add Service wizard2. Close wizard3. Go by link #/main/services/add/step1 (type it in address bar)The router goes to nonexistent page '#/main/services/add/summary'., step reproduc launch add servic wizard close wizard go link main servic add step type address bar the router goe nonexist page main servic add summari,1,0,0,0,0,0,
4662,Install Wizard: Assign Masters: some selects disabled, instal wizard assign master select disabl,Step to reproduce.Go untill to Customize Services Page.Click to Choose Services menu link.Go to Assign Masters page., step reproduc go until custom servic page click choos servic menu link go assign master page,0,0,0,0,0,0,
4673,Bulk Ops: add Supervisor to Bulk Ops on Hosts page, bulk op add supervisor bulk op host page,Add Supervisor to Hosts page's Bulk Ops menu if Storm is installed.Decommission and Recommission should be disabled  as Supervisors do not support these operations., add supervisor host page bulk op menu storm instal decommiss recommiss disabl supervisor support oper,1,0,0,0,0,0,
4682,Customize Services page of Add Service Wizard offers to customize already installed Oozie, custom servic page add servic wizard offer custom alreadi instal oozi,Steps to reproduce: Deploy cluster without Ooozie and some other customizable service. Go to the Add Service wizard. Add Oozie service. Fail starting Oozie server. Close add service wizard. Ensure that Oozie was added as service. Go again to Add Service Wizard and choose some customizable service. Go to Customize Services page.Result: Customize Services page proposes to customize Oozie  but it was already added to the cluster during installation., step reproduc deploy cluster without ooozi customiz servic go add servic wizard add oozi servic fail start oozi server close add servic wizard ensur oozi ad servic go add servic wizard choos customiz servic go custom servic page result custom servic page propos custom oozi alreadi ad cluster instal,1,0,0,0,0,0,
4687,Write unnitests for HDFS install script on HDP1 and HDP2, write unnitest hdf instal script hdp hdp,,,1,0,0,0,0,0,
4693,Hosts table: sort order arrows should be close to the respective column label, host tabl sort order arrow close respect column label,The sort order arrows are right-justified within the column.This makes it look like the arrows apply to the column next to it.Instead  we should show the arrows right next to the respective column label.Like:Name (arrows) IP Address (arrows)Not:Name (arrows) IP Address, the sort order arrow right justifi within column thi make look like arrow appli column next instead show arrow right next respect column label like name arrow ip address arrow not name arrow ip address,0,0,0,0,0,0,
4700,Oozie tests fails, oozi test fail,Oozie tests fail, oozi test fail,0,0,0,0,0,0,
4708,Actual configs not updated after restart of host component, actual config updat restart host compon,Updated HDFS configs aren't applied after RESTART.Steps: On a 3 node cluster  create a ConfigGroup for datanode. Override heap size to 1025m instead of 1024m Restart DN.Result: The /var/lib/ambari-agent/data/config.json  has the correct values for the config type:'global': {'2': 'version1392171779044'  'tag': 'version1'} The API call still shows global as default version:http://hostname1:8080/api/v1/clusters/c1/hosts/hostname1/host_components/DATANODE Note:It works after agent is restarted.{global: {overrides: {2: 'version1392171779044'} default: 'version1'}The cause is that hooks aren't executed for the custom_command like RESTART. Since configs for HDFS are generated in hook.py  we must execute hook.py before custom commands or move config generation from hook.py., updat hdf config appli restart step on node cluster creat config group datanod overrid heap size instead restart dn result the var lib ambari agent data config json correct valu config type global version tag version the api call still show global default version http hostnam api v cluster c host hostnam host compon datanod note it work agent restart global overrid version default version the caus hook execut custom command like restart sinc config hdf gener hook py must execut hook py custom command move config gener hook py,0,0,0,0,0,0,
4710,Add unittets for hooks in secured mode., add unittet hook secur mode,,,0,0,0,1,0,0,
4737,Falcon Server can not be restarted, falcon server restart,,,0,0,0,0,0,0,
4741,Alerts for ATS Component, alert at compon,Impl alert for ATS server ATS process (running / not running)Note: this alert should be disabled/removed when ATS gets deleted (when Kerb is enabled)., impl alert at server at process run run note alert disabl remov at get delet kerb enabl,0,0,0,0,0,0,
4745,Value 'storm.zookeeper.servers' not changing after adding new ZK server, valu storm zookeep server chang ad new zk server,After adding new ZK server needs to change value of storm.zookeeper.servers property., after ad new zk server need chang valu storm zookeep server properti,0,0,0,0,0,0,
4750,Tez DAG UI not showing due to changed ATS responses, tez dag ui show due chang at respons,ATS has changed structure of http://server:8188:8188/ws/v1/apptimeline/HIVE_QUERY_ID/&lt;id&gt; where the entire query JSON structure is now represented as a string under 'otherinfo/query'. UI will need to deserialize this string back into JSON and continue., at chang structur http server ws v apptimelin hive queri id lt id gt entir queri json structur repres string otherinfo queri ui need deseri string back json continu,0,0,0,0,0,0,
4764,AmbariManagementControllerTest Test fails with unable to delete the last user., ambari manag control test test fail unabl delet last user,AmbariManagementControllerTest Test fails with unable to delete the last user.Results :Tests in error: testDeleteUsers(org.apache.ambari.server.controller.AmbariManagementControllerTest): Could not remove user user1. System should have at least one user with administrator role. Tests run: 1404  Failures: 0  Errors: 1  Skipped: 7, ambari manag control test test fail unabl delet last user result test error test delet user org apach ambari server control ambari manag control test could remov user user system least one user administr role test run failur error skip,0,0,0,0,0,0,
4772,Security Wizard: History Server should be a different section for MR service., secur wizard histori server differ section mr servic,Earlier History Server was not a different service component and was always co-hosted with JobTracker for HDP-1.x. So we had a same section for Job Tracker and History server in security wizard config page.After AMBARI-2617 and AMBARI-4207 fix  it's possible to have JobTracker and History Server on different host via Ambari web-ui. With this capability it's important to show Job History Server as a different section in Security Wizard MR service config page., earlier histori server differ servic compon alway co host job tracker hdp x so section job tracker histori server secur wizard config page after ambari ambari fix possibl job tracker histori server differ host via ambari web ui with capabl import show job histori server differ section secur wizard mr servic config page,0,0,0,0,0,0,
4777,Restart indicators work incorrectly after adding component, restart indic work incorrectli ad compon,STR: Deploy cluster with DataNodes on 2 from 3 hosts. Change DataNode maximum Java heap size from 1024 to 1025. Check that Restart indicators appeared and 2 DataNodes require restart. Add DataNode on missing host. Change property DataNode maximum Java heap size back to 1024.Result: Those two DataNodes still require restart and added DataNode not.Gluster properties shouldn't be added unless GLUSTERFS is installed., str deploy cluster data node host chang data node maximum java heap size check restart indic appear data node requir restart add data node miss host chang properti data node maximum java heap size back result those two data node still requir restart ad data node gluster properti ad unless glusterf instal,0,0,0,0,0,0,
4779,Add services wizard throw JS exception, add servic wizard throw js except,See attached. During Customize Services.I installed a cluster w/o Storm  and went to add Storm., see attach dure custom servic i instal cluster w storm went add storm,0,0,0,0,0,0,
4787,/var/lib/hadoop-hdfs/ location does not has +x permission for others, var lib hadoop hdf locat x permiss other,The file defined by dfs.domain.socket.path must give +x permission for other user. &lt;property&gt; &lt;name&gt;dfs.domain.socket.path&lt;/name&gt; &lt;value&gt;/var/lib/hadoop-hdfs/dn_socket&lt;/value&gt; &lt;/property&gt;Currently  In ambari installed cluster  /var/lib/hadoop-hdfs does not give +x permission to other user[root@ambari-sec-1392876050-hdfs-re-8 ~]# stat /var/lib/hadoop-hdfs/ File: '/var/lib/hadoop-hdfs/' Size: 4096 Blocks: 8 IO Block: 4096 directoryDevice: 803h/2051d Inode: 1182008 Links: 3Access: (0750/drwxr-x---) Uid: ( 1005/ hdfs) Gid: ( 500/ hadoop)Access: 2014-02-18 18:10:35.000000000 -0800Modify: 2014-02-20 07:50:55.274766162 -0800Change: 2014-02-20 07:50:55.274766162 -0800Due to this Issue  hadoop commands are seeing below WARN messages. 2014-02-18 05:54:32 734|beaver.machine|INFO|RUNNING: /usr/bin/hdfs dfs -tail /user/hrt_qa/hdfsRegressionData/smallFiles/smallRDFile7552014-02-18 05:54:35 528|beaver.machine|INFO|14/02/18 05:54:35 WARN hdfs.BlockReaderLocal: error creating DomainSocket2014-02-18 05:54:35 528|beaver.machine|INFO|java.net.ConnectException: connect(2) error: Permission denied when trying to connect to '/var/lib/hadoop-hdfs/dn_socket'2014-02-18 05:54:35 528|beaver.machine|INFO|at org.apache.hadoop.net.unix.DomainSocket.connect0(Native Method)2014-02-18 05:54:35 529|beaver.machine|INFO|at org.apache.hadoop.net.unix.DomainSocket.connect(DomainSocket.java:250)2014-02-18 05:54:35 529|beaver.machine|INFO|at org.apache.hadoop.hdfs.DomainSocketFactory.createSocket(DomainSocketFactory.java:158)2014-02-18 05:54:35 529|beaver.machine|INFO|at org.apache.hadoop.hdfs.BlockReaderFactory.nextDomainPeer(BlockReaderFactory.java:691)2014-02-18 05:54:35 529|beaver.machine|INFO|at org.apache.hadoop.hdfs.BlockReaderFactory.createShortCircuitReplicaInfo(BlockReaderFactory.java:439)2014-02-18 05:54:35 529|beaver.machine|INFO|at org.apache.hadoop.hdfs.client.ShortCircuitCache.create(ShortCircuitCache.java:669)The expected Permissions on this location is as below.[root@ambari-sec-1392876050-yarn-10 ~]# stat /var/lib/hadoop-hdfs/ File: '/var/lib/hadoop-hdfs/' Size: 4096 Blocks: 8 IO Block: 4096 directoryDevice: 803h/2051d Inode: 1181767 Links: 3Access: (0751/drwxr-x--x) Uid: ( 1005/ hdfs) Gid: ( 500/ hadoop)Access: 2014-02-20 18:00:06.586040913 -0800Modify: 2014-02-20 07:06:28.267889888 -0800Change: 2014-02-20 17:59:56.629052410 -0800, the file defin df domain socket path must give x permiss user lt properti gt lt name gt df domain socket path lt name gt lt valu gt var lib hadoop hdf dn socket lt valu gt lt properti gt current in ambari instal cluster var lib hadoop hdf give x permiss user root ambari sec hdf stat var lib hadoop hdf file var lib hadoop hdf size block io block directori devic h inod link access drwxr x uid hdf gid hadoop access modifi chang due issu hadoop command see warn messag beaver machin info run usr bin hdf df tail user hrt qa hdf regress data small file small rd file beaver machin info warn hdf block reader local error creat domain socket beaver machin info java net connect except connect error permiss deni tri connect var lib hadoop hdf dn socket beaver machin info org apach hadoop net unix domain socket connect nativ method beaver machin info org apach hadoop net unix domain socket connect domain socket java beaver machin info org apach hadoop hdf domain socket factori creat socket domain socket factori java beaver machin info org apach hadoop hdf block reader factori next domain peer block reader factori java beaver machin info org apach hadoop hdf block reader factori creat short circuit replica info block reader factori java beaver machin info org apach hadoop hdf client short circuit cach creat short circuit cach java the expect permiss locat root ambari sec yarn stat var lib hadoop hdf file var lib hadoop hdf size block io block directori devic h inod link access drwxr x x uid hdf gid hadoop access modifi chang,0,0,0,0,0,0,
4788,NameNode fails to start due to 'fs.defaultFS' being null, name node fail start due fs default fs null,,,0,0,0,0,0,0,
4790,Skip Failing tests for now., skip fail test,Skip Failing tests for now., skip fail test,0,0,0,0,0,0,
4794,Reconfiguring memory related properties of a service suffixes 'm' to memory related properties of other service., reconfigur memori relat properti servic suffix memori relat properti servic,,,0,0,0,0,0,0,
4796,Do not automatically put host component in Maintenance Mode upon decommissioning (and out of Maintenance Mode when recommissioning), do automat put host compon mainten mode upon decommiss mainten mode recommiss,We originally wanted to couple decom/recom with putting the host component in / out of maintenance mode.After experimenting  we decided to undo that. This is the JIRA for the Ambari BE changes., we origin want coupl decom recom put host compon mainten mode after experi decid undo thi jira ambari be chang,0,0,0,0,0,0,
4800,Hosts table UI cleanup, host tabl ui cleanup,1) has too much padding (See attached.)2) the sorting carets should have more padding-left so there is a bit more space between the column label3) the checkboxes should have more left padding. They are not balanced.4) The checkboxes and status icons are not vert centered with the hostname text.5) The input field for searching the hostname column should take up more horizontal space. with all that blank  makes it look like there is a missing column., much pad see attach sort caret pad left bit space column label checkbox left pad they balanc the checkbox statu icon vert center hostnam text the input field search hostnam column take horizont space blank make look like miss column,0,0,0,0,0,0,
4809,Allow Falcon to be configured with keytab/security and custom params, allow falcon configur keytab secur custom param,,,0,0,0,0,0,0,
4818,Do not show 'restart' op on non-admin user, do show restart op non admin user,1) As admin  change a config but do not perform the restarts2) Create a test user (non-admin)3) login as test user4) in host details page  operation to restart is shown. Should not be shown, as admin chang config perform restart creat test user non admin login test user host detail page oper restart shown should shown,0,0,0,0,0,0,
4821,Navigation from Hosts page to HostDetailsPage breaks occasionally, navig host page host detail page break occasion,STR: 1. Navigate to hosts page  wait for page loaded. 2. Click specific hosts link. 3. Wait for HostDetails page loaded. 4. Click Back. 5. Iterate. Actual result: sometimes the Hosts page hangs not resulting in presenting a HostDetails page. Speed of attached videos is 5 times faster than real., str navig host page wait page load click specif host link wait host detail page load click back iter actual result sometim host page hang result present host detail page speed attach video time faster real,0,0,0,0,1,0,
4841,Incorrect behavior of HA wizard on second step, incorrect behavior ha wizard second step,STR:1) Deploy cluster by default;2) Go to HA wizard3) Second wizard stepExpected result:1) In 'Additional NameNode' combobox should NOT be ability to choose host where NameNode component already installed.2) In 'JournalNode' comboboxes should NOT be ability to choose more than one JournalNode on one hostActual results:1) In 'Additional NameNode' combobox can be chosen host with already installed NameNode (see first screenshot)2) JournalNode components might be chosen to install for any host  including case when three JournalNode's might be installed on one host (see second screenshot)., str deploy cluster default go ha wizard second wizard step expect result in addit name node combobox not abil choos host name node compon alreadi instal in journal node combobox not abil choos one journal node one host actual result in addit name node combobox chosen host alreadi instal name node see first screenshot journal node compon might chosen instal host includ case three journal node might instal one host see second screenshot,0,0,0,0,0,0,
4842,Update falcon install scripts to recent changes, updat falcon instal script recent chang,Following features must be implemented: /apps/falcon directory on hdfs and owned by falcon user make falcon able to be runned from custom user change alerts text, follow featur must implement app falcon directori hdf own falcon user make falcon abl run custom user chang alert text,0,0,0,0,0,0,
4843,Ambari DDL for MySQL should not create ambarirca database, ambari ddl my sql creat ambarirca databas,Ambari-DDL-MySQL-CREATE.sql includes 'create ambarirca' database.CREATE DATABASE ambarirca;USE ambarirca;1) At minimum  this DDL should not be creating an ambarirca database and instead mix in the RCA tables with core Ambari tables (that's how Oracle DDL does it &#8211; I think). Not ideal but better than creating an ambarirca database in this script (a user would be surprised to see this).2) Alternatively  we need to split out RCA DDL from the core Ambari. We can doc that if you plan to use HDP 1.3.x stack that you need to to setup an RCA database (since RCA is only applicable to those with HDP 1.3.x stack). If we go this route  for consistency  we would need to do the same for the oracle DDL.Note: Right now  looks like the oracle DDL just puts the RCA tables in the same database as ambari core tables (basically #1 above)., ambari ddl my sql creat sql includ creat ambarirca databas creat databas ambarirca use ambarirca at minimum ddl creat ambarirca databas instead mix rca tabl core ambari tabl oracl ddl i think not ideal better creat ambarirca databas script user would surpris see altern need split rca ddl core ambari we doc plan use hdp x stack need setup rca databas sinc rca applic hdp x stack if go rout consist would need oracl ddl note right look like oracl ddl put rca tabl databas ambari core tabl basic,0,0,0,0,0,0,
4872,Nagios alerts are not shown on SUSE, nagio alert shown suse,STR:1. Deploy cluster by default scenario w/o Storm and Falcon.2. Go to HDFS service page.3. Go to Nagios service page.4. Navigate to Nagios Web UI.5. Enter credentials nagiosadmin/passwordActual results: There are no alerts on service page. Web UI is unavailable due to ERROR 403.Screenshots attached., str deploy cluster default scenario w storm falcon go hdf servic page go nagio servic page navig nagio web ui enter credenti nagiosadmin password actual result there alert servic page web ui unavail due error screenshot attach,0,0,0,0,0,0,
4893,Avoid printing stacktrace for state machine exceptions, avoid print stacktrac state machin except,Log gets filled on install failure.13:28:08 856 WARN [qtp615964260-72] HeartBeatHandler:361 - State machine exceptionorg.apache.ambari.server.state.fsm.InvalidStateTransitionException: Invalid event: HOST_SVCCOMP_OP_SUCCEEDED at INSTALL_FAILED at org.apache.ambari.server.state.fsm.StateMachineFactory.doTransition(StateMachineFactory.java:297) at org.apache.ambari.server.state.fsm.StateMachineFactory.access$300(StateMachineFactory.java:39) at org.apache.ambari.server.state.fsm.StateMachineFactory$InternalStateMachine.doTransition(StateMachineFactory.java:440) at org.apache.ambari.server.state.svccomphost.ServiceComponentHostImpl.handleEvent(ServiceComponentHostImpl.java:730) at com.google.inject.persist.jpa.JpaLocalTxnInterceptor.invoke(JpaLocalTxnInterceptor.java:66), log get fill instal failur warn qtp heart beat handler state machin exceptionorg apach ambari server state fsm invalid state transit except invalid event host svccomp op succeed instal fail org apach ambari server state fsm state machin factori transit state machin factori java org apach ambari server state fsm state machin factori access state machin factori java org apach ambari server state fsm state machin factori intern state machin transit state machin factori java org apach ambari server state svccomphost servic compon host impl handl event servic compon host impl java com googl inject persist jpa jpa local txn interceptor invok jpa local txn interceptor java,0,0,0,0,0,0,
4895,License header is repeated in oozie-log4j.properties, licens header repeat oozi log j properti,# Licensed to the Apache Software Foundation (ASF) under one or more contributor license agreements. See the NOTICE file distributed with this work for additional information regarding copyright ownership. The ASF licenses this file to you under the Apache License  Version 2.0 (the 'License'); you may not use this file except in compliance with the License. You may obtain a copy of the License at##http://www.apache.org/licenses/LICENSE-2.0# Unless required by applicable law or agreed to in writing  software distributed under the License is distributed on an 'AS IS' BASIS  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND  either express or implied. See the License for the specific language governing permissions and limitations under the License.# http://www.apache.org/licenses/LICENSE-2.0# Unless required by applicable law or agreed to in writing  software distributed under the License is distributed on an 'AS IS' BASIS  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND  either express or implied. See the License for the specific language governing permissions and limitations under the License. See accompanying LICENSE file.#, licens apach softwar foundat asf one contributor licens agreement see notic file distribut work addit inform regard copyright ownership the asf licens file apach licens version licens may use file except complianc licens you may obtain copi licens http www apach org licens licens unless requir applic law agre write softwar distribut licens distribut as is basi without warranti or condit of ani kind either express impli see licens specif languag govern permiss limit licens http www apach org licens licens unless requir applic law agre write softwar distribut licens distribut as is basi without warranti or condit of ani kind either express impli see licens specif languag govern permiss limit licens see accompani licens file,0,0,0,0,0,0,
4902,Service Check does not work, servic check work,,,0,0,0,0,0,0,
4909,Slave component should include 'restart' command on host details page 'Actions', slave compon includ restart command host detail page action,Slave components (such as DataNode  NodeManager  RegionServer  Supervisor  Ganglia Monitor) should include a 'Restart' command in their 'Actions' menu on the host details page.The Restart option should be shown and enabled when the component is started., slave compon data node node manag region server supervisor ganglia monitor includ restart command action menu host detail page the restart option shown enabl compon start,0,0,0,0,0,0,
4939,Ganglia alerts after adding YARN+MR2, ganglia alert ad yarn mr,STR: Deploy cluster with HDFS+ZK  Nagios  Ganglia. Add YARN+MR2  Tez services.Result: Alerts Ganglia Monitor process for HistoryServer and Ganglia Monitor process for ResourceManager don't dissappear after restarting all services, str deploy cluster hdf zk nagio ganglia add yarn mr tez servic result alert ganglia monitor process histori server ganglia monitor process resourc manag dissappear restart servic,0,0,0,0,0,0,
4947,Change Hive alerts to move away from Hive metadata queries to port checks, chang hive alert move away hive metadata queri port check,1) remove 'Hive Metastore status' alert2) add 'Hive Metastore process' alert3) add 'HiveServer2 process' alert, remov hive metastor statu alert add hive metastor process alert add hive server process alert,0,0,0,0,0,0,
4954,After configuring NNHA  nn process alerts don't work, after configur nnha nn process alert work,1) Configure NN HA2) stack 2.0.6 (but check stack 2.1 as well)3) Two alerts show' check_tcp: Port must be a positive integer'NameNode process on c6401.ambari.apache.orgNameNode process on c6402.ambari.apache.org4) Looked at /etc/nagios/objects/hadoop-services.cfg5) Saw:define service { host_name c6401.ambari.apache.org use hadoop-service service_description NAMENODE::NameNode process on c6401.ambari.apache.org servicegroups HDFS check_command check_tcp_wrapper!//test!-w 1 -c 1 normal_check_interval 0.5 retry_check_interval 0.25 max_check_attempts 3}define service { host_name c6402.ambari.apache.org use hadoop-service service_description NAMENODE::NameNode process on c6402.ambari.apache.org servicegroups HDFS check_command check_tcp_wrapper!//test!-w 1 -c 1 normal_check_interval 0.5 retry_check_interval 0.25 max_check_attempts 3}Notice in the above //test is the name of my nameservice.Attaching screen shot of my config. So looks like it's grabbing port from the wrong prop., configur nn ha stack check stack well two alert show check tcp port must posit integ name node process c ambari apach org name node process c ambari apach org look etc nagio object hadoop servic cfg saw defin servic host name c ambari apach org use hadoop servic servic descript namenod name node process c ambari apach org servicegroup hdf check command check tcp wrapper test w c normal check interv retri check interv max check attempt defin servic host name c ambari apach org use hadoop servic servic descript namenod name node process c ambari apach org servicegroup hdf check command check tcp wrapper test w c normal check interv retri check interv max check attempt notic test name nameservic attach screen shot config so look like grab port wrong prop,0,0,0,0,0,0,
4970,Jobs popup message clicking continues to stay on the same page, job popup messag click continu stay page,1. Click on an app on ATS/jobs page which is running 2. It pops up a window with message 'Tez DAG has no ID associated with name hrt_qa_20140228161212_a5713292-8213-43a3-b61e-06685799a5b3'3. Press OK and the page refreshes and pops up the same window again.4. This goes on forever until the user closes the browser or enters a different URLInstead  the page should be redirected back to &lt;ambari server URL&gt;/#/main/jobs, click app at job page run it pop window messag tez dag id associ name hrt qa b e b press ok page refresh pop window thi goe forev user close browser enter differ url instead page redirect back lt ambari server url gt main job,0,0,0,0,0,0,
4983,During upgrade. migrate decommissioned DN hosts list to the new format, dure upgrad migrat decommiss dn host list new format,The details of how the notion of a decommissioned DN is stored has changed for 1.5.0. Add support to modify persisted data when Ambari is upgraded to 1.5.0., the detail notion decommiss dn store chang add support modifi persist data ambari upgrad,0,0,0,0,0,0,
4986,Reduce loading time of Host Warnings popup, reduc load time host warn popup,Wizard-&gt;Confirm Hosts step:1. Refactor parsing of json response with hosts warnings.2. Reduce latency on opening Host Warnings popup., wizard gt confirm host step refactor pars json respons host warn reduc latenc open host warn popup,0,0,0,0,1,0,
5020,Lost heartbeat on host but ganglia shows a heartbeat lost, lost heartbeat host ganglia show heartbeat lost,3 hosts  third-host has only Ganglia monitor and Datanode.I kill the third machine agent. Hosts page correctly shows heartbeat lost.On the services page  ganglia shows 'yellow'  even though the ganglia server host is fine.Ganglia service should considered started if Ganglia Server is started., host third host ganglia monitor datanod i kill third machin agent host page correctli show heartbeat lost on servic page ganglia show yellow even though ganglia server host fine ganglia servic consid start ganglia server start,0,0,0,0,0,0,
5028,Hive Service Check Failed during Install Wizard, hive servic check fail instal wizard,Hive Service Check Failed during Install Wizard, hive servic check fail instal wizard,0,0,0,0,0,0,
5036,Secured: Start All Services task got stuck forever, secur start all servic task got stuck forev,Deployed 2-node cluster. Added 3rd node. Enabled security.After steps above on all 3 hosts tasks jammed and don't want to perform or fail for a very long time.VMs are alive  ambari-server and all ambari-agents are running.Finally got a reproduce using 2 commandscurl 'http://vm-0.vm:8080/api/v1/clusters/cc/services?params/run_smoke_test=false' -X PUT -H 'X-Requested-By: X-Requested-By' -u admin:admin --data '{'RequestInfo': {'context': 'Start All Services'}  'Body': {'ServiceInfo': {'state': 'STARTED'}}}' ; sleep 3; curl 'http://vm-0.vm:8080/api/v1/clusters/cc/hosts/vm-0.vm/host_components/APP_TIMELINE_SERVER' -X DELETE -H 'X-Requested-By: X-Requested-By' -u admin:adminThe way to reproduce is a bit different compared to an original description (I issue a DELETE request in 3 seconds after START_ALL_SERVICES request has been issued)  but the symptoms are the same: ServiceComponentHostNotFoundException exception is posted to log and operation is stuck on stage that contains 'App Timeline Server Start' command., deploy node cluster ad rd node enabl secur after step host task jam want perform fail long time v ms aliv ambari server ambari agent run final got reproduc use commandscurl http vm vm api v cluster cc servic param run smoke test fals x put h x request by x request by u admin admin data request info context start all servic bodi servic info state start sleep curl http vm vm api v cluster cc host vm vm host compon app timelin server x delet h x request by x request by u admin admin the way reproduc bit differ compar origin descript i issu delet request second start all servic request issu symptom servic compon host not found except except post log oper stuck stage contain app timelin server start command,0,0,0,0,1,0,
5040,2-way auth fails when using jdk7, way auth fail use jdk,Steps to reproduce:On the Ambari Server host  open /etc/ambari-server/conf/ambari.properties with a text editor.Add the following property:security.server.two_way_ssl = trueError messageINFO 2014-03-07 13:57:17 184 security.py:184 - Agent certificate not exists  sending sign requestINFO 2014-03-07 13:57:17 335 security.py:89 - SSL Connect being called.. connecting to the serverERROR 2014-03-07 13:57:17 414 security.py:76 - Two-way SSL authentication failed. Ensure that server and agent certificates were signed by the same CA and restart the agent. In order to receive a new agent certificate  remove existing certificate file from keys directory. As a workaround you can turn off two-way SSL authentication in server configuration(ambari.properties) Exiting.., step reproduc on ambari server host open etc ambari server conf ambari properti text editor add follow properti secur server two way ssl true error messag info secur py agent certif exist send sign request info secur py ssl connect call connect server error secur py two way ssl authent fail ensur server agent certif sign ca restart agent in order receiv new agent certif remov exist certif file key directori as workaround turn two way ssl authent server configur ambari properti exit,0,0,0,0,0,0,
5043,oozie-site.xml defaults need to be updated for 2.1 stack, oozi site xml default need updat stack,oozie-site.xml needs the following two services added to the list of services:org.apache.oozie.service.XLogStreamingService (needed to display logs)org.apache.oozie.service.JobsConcurrencyService (needed to run Recovery Service - for example  this would handle jobs that are dangling and stuck in RUNNING state), oozi site xml need follow two servic ad list servic org apach oozi servic x log stream servic need display log org apach oozi servic job concurr servic need run recoveri servic exampl would handl job dangl stuck run state,0,0,0,0,0,0,
5051,Start all services silently fails when a service is not startable, start servic silent fail servic startabl,I clicked on Start All services button and nothing happened. Turns out that on the API call  the server throws a 500 exception that is silently lost. We should show in a dialog the error response from server. Similarly for Stop All action.PUT http://c6401:8080/api/v1/clusters/c1/services?params/run_smoke_test{'RequestInfo': {'context' :'_PARSE_.START.ALL_SERVICES'}  'Body': {'ServiceInfo': {'state': 'START{ 'status' : 500  'message' : 'org.apache.ambari.server.controller.spi.SystemException: An internal system exception occurred: Invalid transition for servicecomponenthost  clusterName=c1  clusterId=3  serviceName=OOZIE  componentName=OOZIE_SERVER  hostname=c6402.ambari.apache.org  currentState=INSTALL_FAILED  newDesiredState=STARTED'}, i click start all servic button noth happen turn api call server throw except silent lost we show dialog error respons server similarli stop all action put http c api v cluster c servic param run smoke test request info context pars start all servic bodi servic info state start statu messag org apach ambari server control spi system except an intern system except occur invalid transit servicecomponenthost cluster name c cluster id servic name oozi compon name oozi server hostnam c ambari apach org current state instal fail new desir state start,0,0,0,0,0,0,
5060,Security Wizard: enable Kerberos setup for Storm, secur wizard enabl kerbero setup storm,1) Following jaas.conf file needs to be on all storm component hosts:Client { com.sun.security.auth.module.Krb5LoginModule required useKeyTab=true keyTab='$keytab' storeKey=true useTicketCache=false serviceName='zookeeper' principal='$principal';};2) In YAML  following java configurations should have jaas.conf options:nimbus.childopts: '-Djava.security.auth.login.config=/path/to/jaas.conf'ui.childopts: '-Djava.security.auth.login.config=/path/to/jaas.conf'supervisor.childopts: '-Djava.security.auth.login.config=/path/to/jaas.conf', follow jaa conf file need storm compon host client com sun secur auth modul krb login modul requir use key tab true key tab keytab store key true use ticket cach fals servic name zookeep princip princip in yaml follow java configur jaa conf option nimbu childopt djava secur auth login config path jaa conf ui childopt djava secur auth login config path jaa conf supervisor childopt djava secur auth login config path jaa conf,0,0,0,0,0,0,
5097,Retry failure after installation failure triggers start all services request, retri failur instal failur trigger start servic request,Steps to reproduce: Make Install all services request fail. Hit on Retry button. Make retry attempt to install all services fail.This will trigger start all services call and error pop-up will be displayed.Expected behavior: Start all services call should not be called., step reproduc make instal servic request fail hit retri button make retri attempt instal servic fail thi trigger start servic call error pop display expect behavior start servic call call,0,0,0,0,0,0,
5103,Maintenance Mode: maintenance icon changes on Host Details page, mainten mode mainten icon chang host detail page,For host components whose service is in maintenance mode  the maintenance mode icon should be displayed to the right of the service name and we should display the host component health (green / red  etc) on the far left.When the host itself is in maintenance mode  we should not show any maintenance mode icon on the host component (unless the service is in maintenance mode). This allows the UI to distinguish which host components are in service-derived maintenance mode vs host-derived. Also  on this page  host-level operations apply to all but the host components in service-derived maintenance mode (regardless of the host maintenance mode)  so this display is more natural and easier to understand for the end user., for host compon whose servic mainten mode mainten mode icon display right servic name display host compon health green red etc far left when host mainten mode show mainten mode icon host compon unless servic mainten mode thi allow ui distinguish host compon servic deriv mainten mode vs host deriv also page host level oper appli host compon servic deriv mainten mode regardless host mainten mode display natur easier understand end user,0,0,0,0,0,0,
5112,hadoop-mapreduce.jobsummary.log is empty when specified custom YARN Log Dir, hadoop mapreduc jobsummari log empti specifi custom yarn log dir,Reproduced with such preconditions:On Customize Services page specify 'YARN Log Dir Prefix' to some custom dir. After deploying  run MapReduce2 Service check and check that:hadoop-mapreduce.jobsummary.log is empty  but /var/log/hadoop-yarn/yarn/hadoop-mapreduce.jobsummary.log contains jobs records., reproduc precondit on custom servic page specifi yarn log dir prefix custom dir after deploy run map reduc servic check check hadoop mapreduc jobsummari log empti var log hadoop yarn yarn hadoop mapreduc jobsummari log contain job record,0,0,0,0,0,0,
5123,Background Operations window does not appear after triggering Rolling Restart, background oper window appear trigger roll restart,STR: Deploy cluster. Check that flag Do not show the Background Operations dialog when starting an operation is set to false. Click Restart DataNodes in Actions menu of HDFS. Click 'Trigger Restart' in appeared modal window.Result: Background Operations was not appeared., str deploy cluster check flag do show background oper dialog start oper set fals click restart data node action menu hdf click trigger restart appear modal window result background oper appear,0,0,0,0,0,0,
5128,Rolling Restart dialog shows incorrect message that slaves won't be restarted when service is in maintenance mode, roll restart dialog show incorrect messag slave restart servic mainten mode,The fix for this issue would be to display that the slaves whose host is in 'host' maintenance mode will be skipped.For example  we have 3 hosts (host1  host2  and host3) with NodeManager installed on each. Say host3 is in maintenance mode.Rolling Restart dialog should say '1 NodeManager in maintenance mode will not be restarted'., the fix issu would display slave whose host host mainten mode skip for exampl host host host host node manag instal say host mainten mode roll restart dialog say node manag mainten mode restart,0,0,0,0,0,0,
5146,After Ambari is upgraded to 1.5.0  previous JAVA_HOME is overwritten to /usr/jdk64/jdk1.6.0_3, after ambari upgrad previou java home overwritten usr jdk jdk,After upgrading Ambari from 1.2.5 to 1.5.0 and Stack from 1.3.2 to 2.0.10When starting HDFS service  Datanode and SNameNode on the agent host failed to start due to JAVA_HOME=cbin/java is missing. In ambari.properties  java.home=/usr/jdk64/jdk1.6.0_3 after upgrade., after upgrad ambari stack when start hdf servic datanod s name node agent host fail start due java home cbin java miss in ambari properti java home usr jdk jdk upgrad,0,0,0,0,0,0,
5156,Hive CLI using Tez runtime does not start by throwing HDFS exception, hive cli use tez runtim start throw hdf except,In a cluster node we had set the below in /etc/hive/conf/hive-site.xml&lt;property&gt; &lt;name&gt;hive.jar.directory&lt;/name&gt; &lt;value&gt;hdfs:///apps/hive/install&lt;/value&gt;&lt;/property&gt;The HDFS folder has the following contents# hadoop fs -ls -R /apps/hive/drwxr-xr-x - hive hdfs 0 2014-03-19 17:10 /apps/hive/install-rwxr-xr-x 3 hive hdfs 14719276 2014-03-19 17:10 /apps/hive/install/hive-exec.jardrwxrwxrwx - hive hdfs 0 2014-03-19 17:08 /apps/hive/warehouseAs user ambari-qa I run the hive command to hit this exception$ hiveLogging initialized using configuration in file:/etc/hive/conf.dist/hive-log4j.propertiesException in thread 'main' java.lang.RuntimeException: org.apache.hadoop.security.AccessControlException: Permission denied: user=ambari-qa  access=WRITE  inode='/apps/hive/install':hive:hdfs:drwxr-xr-x at org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker.checkFsPermission(FSPermissionChecker.java:265) at org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker.check(FSPermissionChecker.java:251) at org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker.check(FSPermissionChecker.java:232) at org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker.checkPermission(FSPermissionChecker.java:176) at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkPermission(FSNamesystem.java:5481) at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkPermission(FSNamesystem.java:5463) at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkAncestorAccess(FSNamesystem.java:5437) at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.startFileInternal(FSNamesystem.java:2265) at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.startFileInt(FSNamesystem.java:2218) at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.startFile(FSNamesystem.java:2171) at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.create(NameNodeRpcServer.java:517) at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.create(ClientNamenodeProtocolServerSideTranslatorPB.java:354) at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java) at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:585) at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:928) at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2003) at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1999) at java.security.AccessController.doPrivileged(Native Method) at javax.security.auth.Subject.doAs(Subject.java:415) at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1548) at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1997) at org.apache.hadoop.hive.ql.session.SessionState.start(SessionState.java:345) at org.apache.hadoop.hive.cli.CliDriver.run(CliDriver.java:682) at org.apache.hadoop.hive.cli.CliDriver.main(CliDriver.java:626) at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57) at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) at java.lang.reflect.Method.invoke(Method.java:606) at org.apache.hadoop.util.RunJar.main(RunJar.java:212)Caused by: org.apache.hadoop.security.AccessControlException: Permission denied: user=ambari-qa  access=WRITE  inode='/apps/hive/install':hive:hdfs:drwxr-xr-x at org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker.checkFsPermission(FSPermissionChecker.java:265) at org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker.check(FSPermissionChecker.java:251) at org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker.check(FSPermissionChecker.java:232) at org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker.checkPermission(FSPermissionChecker.java:176) at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkPermission(FSNamesystem.java:5481) at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkPermission(FSNamesystem.java:5463) at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkAncestorAccess(FSNamesystem.java:5437) at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.startFileInternal(FSNamesystem.java:2265) at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.startFileInt(FSNamesystem.java:2218) at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.startFile(FSNamesystem.java:2171) at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.create(NameNodeRpcServer.java:517) at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.create(ClientNamenodeProtocolServerSideTranslatorPB.java:354) at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java) at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:585) at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:928) at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2003) at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1999) at java.security.AccessController.doPrivileged(Native Method) at javax.security.auth.Subject.doAs(Subject.java:415) at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1548) at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1997) at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method) at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57) at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45) at java.lang.reflect.Constructor.newInstance(Constructor.java:526) at org.apache.hadoop.ipc.RemoteException.instantiateException(RemoteException.java:106) at org.apache.hadoop.ipc.RemoteException.unwrapRemoteException(RemoteException.java:73) at org.apache.hadoop.hdfs.DFSOutputStream.newStreamForCreate(DFSOutputStream.java:1602) at org.apache.hadoop.hdfs.DFSClient.create(DFSClient.java:1461) at org.apache.hadoop.hdfs.DFSClient.create(DFSClient.java:1386) at org.apache.hadoop.hdfs.DistributedFileSystem$6.doCall(DistributedFileSystem.java:394) at org.apache.hadoop.hdfs.DistributedFileSystem$6.doCall(DistributedFileSystem.java:390) at org.apache.had, in cluster node set etc hive conf hive site xml lt properti gt lt name gt hive jar directori lt name gt lt valu gt hdf app hive instal lt valu gt lt properti gt the hdf folder follow content hadoop fs ls r app hive drwxr xr x hive hdf app hive instal rwxr xr x hive hdf app hive instal hive exec jardrwxrwxrwx hive hdf app hive warehous as user ambari qa i run hive command hit except hive log initi use configur file etc hive conf dist hive log j properti except thread main java lang runtim except org apach hadoop secur access control except permiss deni user ambari qa access write inod app hive instal hive hdf drwxr xr x org apach hadoop hdf server namenod fs permiss checker check fs permiss fs permiss checker java org apach hadoop hdf server namenod fs permiss checker check fs permiss checker java org apach hadoop hdf server namenod fs permiss checker check fs permiss checker java org apach hadoop hdf server namenod fs permiss checker check permiss fs permiss checker java org apach hadoop hdf server namenod fs namesystem check permiss fs namesystem java org apach hadoop hdf server namenod fs namesystem check permiss fs namesystem java org apach hadoop hdf server namenod fs namesystem check ancestor access fs namesystem java org apach hadoop hdf server namenod fs namesystem start file intern fs namesystem java org apach hadoop hdf server namenod fs namesystem start file int fs namesystem java org apach hadoop hdf server namenod fs namesystem start file fs namesystem java org apach hadoop hdf server namenod name node rpc server creat name node rpc server java org apach hadoop hdf protocol pb client namenod protocol server side translat pb creat client namenod protocol server side translat pb java org apach hadoop hdf protocol proto client namenod protocol proto client namenod protocol call block method client namenod protocol proto java org apach hadoop ipc protobuf rpc engin server proto buf rpc invok call protobuf rpc engin java org apach hadoop ipc rpc server call rpc java org apach hadoop ipc server handler run server java org apach hadoop ipc server handler run server java java secur access control privileg nativ method javax secur auth subject as subject java org apach hadoop secur user group inform as user group inform java org apach hadoop ipc server handler run server java org apach hadoop hive ql session session state start session state java org apach hadoop hive cli cli driver run cli driver java org apach hadoop hive cli cli driver main cli driver java sun reflect nativ method accessor impl invok nativ method sun reflect nativ method accessor impl invok nativ method accessor impl java sun reflect deleg method accessor impl invok deleg method accessor impl java java lang reflect method invok method java org apach hadoop util run jar main run jar java caus org apach hadoop secur access control except permiss deni user ambari qa access write inod app hive instal hive hdf drwxr xr x org apach hadoop hdf server namenod fs permiss checker check fs permiss fs permiss checker java org apach hadoop hdf server namenod fs permiss checker check fs permiss checker java org apach hadoop hdf server namenod fs permiss checker check fs permiss checker java org apach hadoop hdf server namenod fs permiss checker check permiss fs permiss checker java org apach hadoop hdf server namenod fs namesystem check permiss fs namesystem java org apach hadoop hdf server namenod fs namesystem check permiss fs namesystem java org apach hadoop hdf server namenod fs namesystem check ancestor access fs namesystem java org apach hadoop hdf server namenod fs namesystem start file intern fs namesystem java org apach hadoop hdf server namenod fs namesystem start file int fs namesystem java org apach hadoop hdf server namenod fs namesystem start file fs namesystem java org apach hadoop hdf server namenod name node rpc server creat name node rpc server java org apach hadoop hdf protocol pb client namenod protocol server side translat pb creat client namenod protocol server side translat pb java org apach hadoop hdf protocol proto client namenod protocol proto client namenod protocol call block method client namenod protocol proto java org apach hadoop ipc protobuf rpc engin server proto buf rpc invok call protobuf rpc engin java org apach hadoop ipc rpc server call rpc java org apach hadoop ipc server handler run server java org apach hadoop ipc server handler run server java java secur access control privileg nativ method javax secur auth subject as subject java org apach hadoop secur user group inform as user group inform java org apach hadoop ipc server handler run server java sun reflect nativ constructor accessor impl new instanc nativ method sun reflect nativ constructor accessor impl new instanc nativ constructor accessor impl java sun reflect deleg constructor accessor impl new instanc deleg constructor accessor impl java java lang reflect constructor new instanc constructor java org apach hadoop ipc remot except instanti except remot except java org apach hadoop ipc remot except unwrap remot except remot except java org apach hadoop hdf df output stream new stream for creat df output stream java org apach hadoop hdf df client creat df client java org apach hadoop hdf df client creat df client java org apach hadoop hdf distribut file system call distribut file system java org apach hadoop hdf distribut file system call distribut file system java org apach,0,0,0,0,1,0,
5173,The status of App Timeline Server affect the health status of YARN service., the statu app timelin server affect health statu yarn servic,When stopping ATS  YARN service indicator blinks red giving the idea that YARN is going down and become solid red after that. YARN service should not be indicated as STOPPED if ATS is down., when stop at yarn servic indic blink red give idea yarn go becom solid red yarn servic indic stop at,0,0,0,0,0,0,
5223,hive-env.sh overwrites user value of HIVE_AUX_JARS_PATH, hive env sh overwrit user valu hive aux jar path,This line at the bottom of hive-env.sh disregards any user-provided values  masking them from the launch script.# Folder containing extra ibraries required for hive compilation/execution can be controlled by:export HIVE_AUX_JARS_PATH=/usr/lib/hcatalog/share/hcatalog/hcatalog-core.jarThis breaks this feature for users of both the environment export HIVE_AUX_JARS_PATH='my custom value' and the command line hive --auxpath 'my custom value' ., thi line bottom hive env sh disregard user provid valu mask launch script folder contain extra ibrari requir hive compil execut control export hive aux jar path usr lib hcatalog share hcatalog hcatalog core jar thi break featur user environ export hive aux jar path custom valu command line hive auxpath custom valu,0,0,0,0,0,0,
5235,Add component of clients in Ambari doesn't work in a secured cluster, add compon client ambari work secur cluster,The reason of bug is wrong query formation while triggering API to create clients on host.The data sent with the POST call to create client components on the host is:{'RequestInfo':{'context':'Install Clients'} 'Body':{'host_components':[{'HostRoles':{'component_name':'CLIENTS'}}]}}This is incorrect. There is no component with name 'CLIENTS'., the reason bug wrong queri format trigger api creat client host the data sent post call creat client compon host request info context instal client bodi host compon host role compon name client thi incorrect there compon name client,0,0,0,0,0,0,
5244,Wizard Step7 JS error on load page (add service wizard), wizard step js error load page add servic wizard,Add Nagios  Ganglia via Add Service WizardProceed to step 'Customize Services'JS-error appears about null-object:/app/controllers/wizard/step7_controller.js : loadServiceTagsSuccess()if (serviceConfigsDef.sites.indexOf(site) &gt; -1)Also  HDFS by default is selected as active tab  but one of the 'new' services (Nagios for example) should be selected., add nagio ganglia via add servic wizard proce step custom servic js error appear null object app control wizard step control js load servic tag success servic config def site index of site gt also hdf default select activ tab one new servic nagio exampl select,0,0,0,0,0,0,
5246,Mistake in title of operation 'Restart APP_TIMELINE_SERVER on ..., mistak titl oper restart app timelin server,We should show displayName  not componentName, we show display name compon name,0,0,0,0,0,0,
5258,Installer: 'Undo' button for repo BaseURL does not work, instal undo button repo base url work,STR: during installer phase go to &lt;cluster&gt;:8080 /#/installer/step1 ; Change/delete any of BaseURL of repos ; Click 'Undo' buttonActual Results:Undo button does not work. Moreover  if we click Undo after any update of text in BaseURL  it leads to cleanup of it value atall., str instal phase go lt cluster gt instal step chang delet base url repo click undo button actual result undo button work moreov click undo updat text base url lead cleanup valu atal,0,0,0,0,0,0,
5264,Ganglia Server goes to 'installed' state after double 'Ganglia rrdcached base directory' config changing, ganglia server goe instal state doubl ganglia rrdcach base directori config chang,STR:1) Deploy cluster by default2) Go to Ganglia service page -&gt; Config tab 3) Change value 'Ganglia rrdcached base directory' (e.g. /var/lib/ganglia/rrd8)4) Restart Ganglia5) Change 'Ganglia rrdcached base directory' back to old value (by default - /var/lib/ganglia/rrds)6) Restart GangliaExpected result: Ganglia should be restarted successfullyCurrent result: Ganglia Server goes to 'installed' state and stuck thereError message from gmetad file:=============================Starting hdp-gmetad...=============================Base directory (-b) resolved via file system links!Please consult rrdcached '-b' documentation!Consider specifying the real directory (/var/lib/ganglia/rrd8)chgrp: cannot access '/var/run/ganglia/hdp/rrdcached.sock': No such file or directorychgrp: cannot access '/var/run/ganglia/hdp/rrdcached.limited.sock': No such file or directoryFailed to start /usr/bin/rrdcachedNot starting /usr/sbin/gmetad because starting /usr/bin/rrdcached failed.root 16990 0.0 0.0 108164 1560 ? S 03:19 0:00 /bin/bash --login -c service hdp-gmetad start &gt;&gt; /tmp/gmetad.log 2&gt;&amp;1 ; /bin/ps auwx | /bin/grep [g]metad &gt;&gt; /tmp/gmetad.log 2&gt;&amp;1, str deploy cluster default go ganglia servic page gt config tab chang valu ganglia rrdcach base directori e g var lib ganglia rrd restart ganglia chang ganglia rrdcach base directori back old valu default var lib ganglia rrd restart ganglia expect result ganglia restart success current result ganglia server goe instal state stuck error messag gmetad file start hdp gmetad base directori b resolv via file system link pleas consult rrdcach b document consid specifi real directori var lib ganglia rrd chgrp cannot access var run ganglia hdp rrdcach sock no file directorychgrp cannot access var run ganglia hdp rrdcach limit sock no file directori fail start usr bin rrdcach not start usr sbin gmetad start usr bin rrdcach fail root s bin bash login c servic hdp gmetad start gt gt tmp gmetad log gt amp bin ps auwx bin grep g metad gt gt tmp gmetad log gt amp,0,0,0,0,0,0,
5274,Supervisor under supervision fails w/o ganglia server, supervisor supervis fail w ganglia server,JMXetricAgent instrumented JVM  see https://github.com/ganglia/jmxetricMar 28  2014 6:40:13 PM info.ganglia.jmxetric.JMXetricAgent premainSEVERE: Exception starting JMXetricAgentjava.net.UnknownHostException: {0}: Name or service not known at java.net.Inet6AddressImpl.lookupAllHostAddr(Native Method) at java.net.InetAddress$1.lookupAllHostAddr(InetAddress.java:901) at java.net.InetAddress.getAddressesFromNameService(InetAddress.java:1293) at java.net.InetAddress.getAllByName0(InetAddress.java:1246) at java.net.InetAddress.getAllByName(InetAddress.java:1162) at java.net.InetAddress.getAllByName(InetAddress.java:1098) at java.net.InetAddress.getByName(InetAddress.java:1048) at info.ganglia.gmetric4j.gmetric.AbstractProtocol.&lt;init&gt;(AbstractProtocol.java:29) at info.ganglia.gmetric4j.gmetric.Protocolv31x.&lt;init&gt;(Protocolv31x.java:34) at info.ganglia.gmetric4j.gmetric.GMetric.&lt;init&gt;(GMetric.java:108) at info.ganglia.jmxetric.XMLConfigurationService.configureGangliaFromXML(XMLConfigurationService.java:165) at info.ganglia.jmxetric.XMLConfigurationService.configure(XMLConfigurationService.java:67) at info.ganglia.jmxetric.JMXetricAgent.premain(JMXetricAgent.java:51) at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57) at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) at java.lang.reflect.Method.invoke(Method.java:606) at sun.instrument.InstrumentationImpl.loadClassAndStartAgent(InstrumentationImpl.java:382) at sun.instrument.InstrumentationImpl.loadClassAndCallPremain(InstrumentationImpl.java:397)The reason here is this config send from ui:...childopts: '-javaagent:/usr/lib/storm/contrib/storm-jmxetric/lib/jmxetric-1.0.4.jar=host={0} ...'it can't find hostname {0}  however this works fine:...childopts: '-javaagent:/usr/lib/storm/contrib/storm-jmxetric/lib/jmxetric-1.0.4.jar=host= port...'So let's leave it as empty if no ganglia server is present, jm xetric agent instrument jvm see http github com ganglia jmxetric mar pm info ganglia jmxetric jm xetric agent premain sever except start jm xetric agentjava net unknown host except name servic known java net inet address impl lookup all host addr nativ method java net inet address lookup all host addr inet address java java net inet address get address from name servic inet address java java net inet address get all by name inet address java java net inet address get all by name inet address java java net inet address get all by name inet address java java net inet address get by name inet address java info ganglia gmetric j gmetric abstract protocol lt init gt abstract protocol java info ganglia gmetric j gmetric protocolv x lt init gt protocolv x java info ganglia gmetric j gmetric g metric lt init gt g metric java info ganglia jmxetric xml configur servic configur ganglia from xml xml configur servic java info ganglia jmxetric xml configur servic configur xml configur servic java info ganglia jmxetric jm xetric agent premain jm xetric agent java sun reflect nativ method accessor impl invok nativ method sun reflect nativ method accessor impl invok nativ method accessor impl java sun reflect deleg method accessor impl invok deleg method accessor impl java java lang reflect method invok method java sun instrument instrument impl load class and start agent instrument impl java sun instrument instrument impl load class and call premain instrument impl java the reason config send ui childopt javaag usr lib storm contrib storm jmxetric lib jmxetric jar host find hostnam howev work fine childopt javaag usr lib storm contrib storm jmxetric lib jmxetric jar host port so let leav empti ganglia server present,0,0,0,0,0,0,
5282,supervisor.enable should be removed from Ambari's Storm Config section, supervisor enabl remov ambari storm config section,Ambari exposes the supervisor.enable property and in some early builds defaults it to true which causes the supervisor's to not launch workers assigned to them. The supervisor's will start up  the assignments are there  but the supervisor's just ignore them. We need to remove the supervisor.enable from Ambari as it seems like a very dangerous switch that doesn't have broad applicability., ambari expos supervisor enabl properti earli build default true caus supervisor launch worker assign the supervisor start assign supervisor ignor we need remov supervisor enabl ambari seem like danger switch broad applic,0,0,0,0,0,0,
5301,Table of Confirm Hosts step is collapsed, tabl confirm host step collaps,Steps to reproduce:1. Run hosts registration2. Switch to Registering category3. Wait till all hosts become registeredResult: Table is collapsed.JS error occured: Uncaught Error: assertion failed: calling set on destroyed object, step reproduc run host registr switch regist categori wait till host becom regist result tabl collaps js error occur uncaught error assert fail call set destroy object,0,0,0,0,0,0,
5310,Don't do server-client version match check if App.version have not been set on ambari-web, don server client version match check app version set ambari web,With the new server-client version match check introduced  it is a bit annoying when building ambari-web locally and testing on the server for e2e testing  as the server and client versions must match exactly or you cannot proceed. We'll disable the check if App.version == ''., with new server client version match check introduc bit annoy build ambari web local test server e e test server client version must match exactli cannot proceed we disabl check app version,0,0,0,0,0,0,
5311,Falcon fails to deploy, falcon fail deploy,Looks like Falcon service definition on the stack has changed by AMBARI-5278.Currently  any new definition of globals require duplicating them in the UI code but that was not done  so this is breaking Falcon installation., look like falcon servic definit stack chang ambari current new definit global requir duplic ui code done break falcon instal,0,0,0,0,0,0,
5313,Icon 'Asterisk' on 'Assign Masters'/-Slaves steps does not display with 8-bit depth, icon asterisk assign master slave step display bit depth,,,0,0,0,0,0,0,
5332,Print better logs for openssl issues on centos/rhel 6.5., print better log openssl issu cento rhel,Print better logs for openssl issues on centos/rhel 6.5., print better log openssl issu cento rhel,0,0,0,0,0,0,
5340,Tez DAG Operator hover text not wrapping wide content, tez dag oper hover text wrap wide content,When a Hive Tez DAG operator has a plan with very wide content  the text is not wrapped and hence bleeds out of the hover box. An additional problem is that this causes the hover to lose focus and hence close - thereby user cannot even see the hover (opens &amp; closes almost instantly)., when hive tez dag oper plan wide content text wrap henc bleed hover box an addit problem caus hover lose focu henc close therebi user cannot even see hover open amp close almost instantli,0,0,0,0,0,0,
5342,Ambari YARN UI - Quick Link - JMX breaks if RM port is changed, ambari yarn ui quick link jmx break rm port chang,If you change RM port from 8088 to 50030 for migration of 1.x to 2.x   JVM metrics and few others are missing which results in yarn service summary page having multiple fields with n/a value.Step1: Change property below yarn.resourcemanager.webapp.address = localhost:50030Step 2: Start the service Click on Dashboard JMX is issue  Quick links is issue. RM gets started successfully., if chang rm port migrat x x jvm metric other miss result yarn servic summari page multipl field n valu step chang properti yarn resourcemanag webapp address localhost step start servic click dashboard jmx issu quick link issu rm get start success,0,0,0,0,0,0,
5344,Error with finding FK constraint, error find fk constraint,Steps: Install Ambari-1.3.2 with Oracle DB. Upgrade ambari to 1.5.0 Run 'ambari-server upgrade' command.Seems like check before execute doesn't work for Oracle. Since we ignore failures this is not block the upgrade.Exception:16:41:36 719 WARN [main] DBAccessorImpl:416 - Error executing query: ALTER TABLE clusterconfigmapping ADD CONSTRAINT FK_clustercfgmap_cluster_id FOREIGN KEY (cluster_id) REFERENCES clusters (cluster_id)java.sql.SQLSyntaxErrorException: ORA-02275: such a referential constraint already exists in the table at oracle.jdbc.driver.T4CTTIoer.processError(T4CTTIoer.java:445) at oracle.jdbc.driver.T4CTTIoer.processError(T4CTTIoer.java:396) at oracle.jdbc.driver.T4C8Oall.processError(T4C8Oall.java:879) at oracle.jdbc.driver.T4CTTIfun.receive(T4CTTIfun.java:450) at oracle.jdbc.driver.T4CTTIfun.doRPC(T4CTTIfun.java:192) at oracle.jdbc.driver.T4C8Oall.doOALL(T4C8Oall.java:531) at oracle.jdbc.driver.T4CStatement.doOall8(T4CStatement.java:193) at oracle.jdbc.driver.T4CStatement.executeForRows(T4CStatement.java:1033) at oracle.jdbc.driver.OracleStatement.doExecuteWithTimeout(OracleStatement.java:1329) at oracle.jdbc.driver.OracleStatement.executeInternal(OracleStatement.java:1909) at oracle.jdbc.driver.OracleStatement.execute(OracleStatement.java:1871) at oracle.jdbc.driver.OracleStatementWrapper.execute(OracleStatementWrapper.java:318) at org.apache.ambari.server.orm.DBAccessorImpl.executeQuery(DBAccessorImpl.java:413) at org.apache.ambari.server.orm.DBAccessorImpl.executeQuery(DBAccessorImpl.java:399) at org.apache.ambari.server.orm.DBAccessorImpl.addFKConstraint(DBAccessorImpl.java:262) at org.apache.ambari.server.upgrade.UpgradeCatalog150.executeDDLUpdates(UpgradeCatalog150.java:375) at org.apache.ambari.server.upgrade.AbstractUpgradeCatalog.upgradeSchema(AbstractUpgradeCatalog.java:177) at org.apache.ambari.server.upgrade.SchemaUpgradeHelper.executeUpgrade(SchemaUpgradeHelper.java:174) at org.apache.ambari.server.upgrade.SchemaUpgradeHelper.main(SchemaUpgradeHelper.java:234)16:41:36 720 WARN [main] DBAccessorImpl:264 - Add FK constraint failed  constraintName = FK_clustercfgmap_cluster_id  tableName = clusterconfigmapping  errorCode = 2275  message = ORA-02275: such a referential constraint already exists in the table, step instal ambari oracl db upgrad ambari run ambari server upgrad command seem like check execut work oracl sinc ignor failur block upgrad except warn main db accessor impl error execut queri alter tabl clusterconfigmap add constraint fk clustercfgmap cluster id foreign key cluster id refer cluster cluster id java sql sql syntax error except ora referenti constraint alreadi exist tabl oracl jdbc driver t ctt ioer process error t ctt ioer java oracl jdbc driver t ctt ioer process error t ctt ioer java oracl jdbc driver t c oall process error t c oall java oracl jdbc driver t ctt ifun receiv t ctt ifun java oracl jdbc driver t ctt ifun rpc t ctt ifun java oracl jdbc driver t c oall oall t c oall java oracl jdbc driver t c statement oall t c statement java oracl jdbc driver t c statement execut for row t c statement java oracl jdbc driver oracl statement execut with timeout oracl statement java oracl jdbc driver oracl statement execut intern oracl statement java oracl jdbc driver oracl statement execut oracl statement java oracl jdbc driver oracl statement wrapper execut oracl statement wrapper java org apach ambari server orm db accessor impl execut queri db accessor impl java org apach ambari server orm db accessor impl execut queri db accessor impl java org apach ambari server orm db accessor impl add fk constraint db accessor impl java org apach ambari server upgrad upgrad catalog execut ddl updat upgrad catalog java org apach ambari server upgrad abstract upgrad catalog upgrad schema abstract upgrad catalog java org apach ambari server upgrad schema upgrad helper execut upgrad schema upgrad helper java org apach ambari server upgrad schema upgrad helper main schema upgrad helper java warn main db accessor impl add fk constraint fail constraint name fk clustercfgmap cluster id tabl name clusterconfigmap error code messag ora referenti constraint alreadi exist tabl,0,0,0,0,0,0,
5359,unittest NagiosPropertyProviderTest fails, unittest nagio properti provid test fail,testNoNagiosServerCompoonent(org.apache.ambari.server.controller.nagios.NagiosPropertyProviderTest): Expected no alertstestNoNagiosService(org.apache.ambari.server.controller.nagios.NagiosPropertyProviderTest): Expected no alerts, test no nagio server compoon org apach ambari server control nagio nagio properti provid test expect alertstest no nagio servic org apach ambari server control nagio nagio properti provid test expect alert,0,0,0,0,0,0,
5362,Automatic bootstrap failed on CentOS 6.5 (No module named common_functions), automat bootstrap fail cent os no modul name common function,SSH bootstrap of agents failed on CentOS 6.5.==========================Copying OS type check script...==========================Could not create directory '/root/.ssh'.Warning: Permanently added 'c6502.ambari.apache.org 192.168.65.102' (RSA) to the list of known hosts.scp /usr/lib/python2.6/site-packages/ambari_server/os_check_type.pyhost=c6502.ambari.apache.org  exitcode=0==========================Running OS type check...==========================Traceback (most recent call last): File '/tmp/os_check_type1396641340.py'  line 22  in &lt;module&gt; from common_functions import OSCheckImportError: No module named common_functionsConnection to c6502.ambari.apache.org closed.SSH command execution finishedhost=c6502.ambari.apache.org  exitcode=1ERROR: Bootstrap of host c6502.ambari.apache.org fails because previous action finished with non-zero exit code (1)ERROR MESSAGE: tcgetattr: Invalid argumentConnection to c6502.ambari.apache.org closed.STDOUT: Traceback (most recent call last): File '/tmp/os_check_type1396641340.py'  line 22  in &lt;module&gt; from common_functions import OSCheckImportError: No module named common_functionsConnection to c6502.ambari.apache.org closed., ssh bootstrap agent fail cent os copi os type check script could creat directori root ssh warn perman ad c ambari apach org rsa list known host scp usr lib python site packag ambari server os check type pyhost c ambari apach org exitcod run os type check traceback recent call last file tmp os check type py line lt modul gt common function import os check import error no modul name common function connect c ambari apach org close ssh command execut finishedhost c ambari apach org exitcod error bootstrap host c ambari apach org fail previou action finish non zero exit code error messag tcgetattr invalid argument connect c ambari apach org close stdout traceback recent call last file tmp os check type py line lt modul gt common function import os check import error no modul name common function connect c ambari apach org close,0,0,0,0,0,0,
5371,Dashboard: dashboard actions do not work for non-admin users, dashboard dashboard action work non admin user,For non-admin users action 'Switch to classic dashboard' returns JS error and 'Reset all widgets to default' action throws to Login page., for non admin user action switch classic dashboard return js error reset widget default action throw login page,0,0,0,0,0,0,
5381,Installer: 'Undo' button for repo BaseURL is unnecessarily present, instal undo button repo base url unnecessarili present,STR:on Installer phase go to &lt;cluster&gt;:8080 /#/installer/step1Do not do any updates of 'Base Url' fieldcheck the 'Undo' button for 2.1 stack.Actual Result:Undo is present for all 3 Os reposExpected Result:Undo should be present only after any update of 'BaseURL' field----------Seems  functionally this does not affect us  but might be confusing for users., str instal phase go lt cluster gt instal step do updat base url fieldcheck undo button stack actual result undo present os repo expect result undo present updat base url field seem function affect us might confus user,0,0,0,0,0,1,
5382,Schema Upgrade failed when upgrading to 1.5.1, schema upgrad fail upgrad,While upgrade  schema upgrade fails.Example error output (Oracle):rg.apache.ambari.server.AmbariException: Current database store version is not compatible with current server version  serverVersion=1.5.1.96  schemaVersion=1.4.1.25 at org.apache.ambari.server.controller.AmbariServer.checkDBVersion(AmbariServer.java:479) at org.apache.ambari.server.controller.AmbariServer.run(AmbariServer.java:149) at org.apache.ambari.server.controller.AmbariServer.main(AmbariServer.java:528)and Postgres 11:39:27 364 ERROR [main] AmbariServer:531 - Failed to run the Ambari Serverorg.apache.ambari.server.AmbariException: Current database store version is not compatible with current server version  serverVersion=1.5.1.96  schemaVersion=1.5.0 at org.apache.ambari.server.controller.AmbariServer.checkDBVersion(AmbariServer.java:479) at org.apache.ambari.server.controller.AmbariServer.run(AmbariServer.java:149) at org.apache.ambari.server.controller.AmbariServer.main(AmbariServer.java:528), while upgrad schema upgrad fail exampl error output oracl rg apach ambari server ambari except current databas store version compat current server version server version schema version org apach ambari server control ambari server check db version ambari server java org apach ambari server control ambari server run ambari server java org apach ambari server control ambari server main ambari server java postgr error main ambari server fail run ambari serverorg apach ambari server ambari except current databas store version compat current server version server version schema version org apach ambari server control ambari server check db version ambari server java org apach ambari server control ambari server run ambari server java org apach ambari server control ambari server main ambari server java,0,0,0,0,0,0,
5386,Deploy stuck during generating tasks on Review page (not always reproduced), deploy stuck gener task review page alway reproduc,After the user clicks 'Deploy' in the Installer Wizard  sometimes an error is shown showing 'java.lang.IllegalArgumentException: Could not access base url'. This happens while the wizard tries to save the user-specified repo base URLs.Installer should not be performing validation during deploy  because it had already been done during Select Stacks., after user click deploy instal wizard sometim error shown show java lang illeg argument except could access base url thi happen wizard tri save user specifi repo base ur ls instal perform valid deploy alreadi done select stack,0,0,0,0,0,0,
5402,Remove classic dashboard view from Ambari, remov classic dashboard view ambari,remove classic dashboard from Ambari  as it's not used any more and doesn't support new services, remov classic dashboard ambari use support new servic,0,0,0,0,0,0,
5406,Pig unit test class named wrong, pig unit test class name wrong,Typo in unit test. File said class was Hcat rather than Pig, typo unit test file said class hcat rather pig,0,1,0,0,0,0,
5412,Operation 'Supervisor start' failed during installation but all supervisors are alive, oper supervisor start fail instal supervisor aliv,STR:1. Deploy Hadoop by default scenario with all services.2. Try to start Storm.Actual results: 'Start All Services' operation failed because of 'Supervisor start' failed. 'Start Storm' operation does not contain 'Supervisor start' popups  but all supervisors are STARTED after it have finished., str deploy hadoop default scenario servic tri start storm actual result start all servic oper fail supervisor start fail start storm oper contain supervisor start popup supervisor start finish,0,0,0,0,0,0,
5413,OS type check for centos 6.5 can fail if the /etc/issue has CentOS Linux release 6.5, os type check cento fail etc issu cent os linux releas,I tried default centos 6.5 it works fine. But if I change /etc/issues and /etc/redhat-release to say:CentOS Linux release 6.5 (Final)then the registration fails with:INFO 2014-04-09 14:39:22 501 security.py:51 - SSL connection established. Two-way SSL authentication is turned off on the server.ERROR 2014-04-09 14:39:22 563 Controller.py:100 - Cannot register host with not supported os type  hostname=c6501.ambari.apache.org  serverOsType=redhat6  agentOstype=centos linux6In the agent logs.By default centos 6.5 has:CentOS release 6.5 (Final)but sometimes can have:CentOS Linux release 6.5 (Final), i tri default cento work fine but i chang etc issu etc redhat releas say cent os linux releas final registr fail info secur py ssl connect establish two way ssl authent turn server error control py cannot regist host support os type hostnam c ambari apach org server os type redhat agent ostyp cento linux in agent log by default cento cent os releas final sometim cent os linux releas final,0,0,0,0,0,0,
5433,Add Host failed on upgraded cluster on Suse, add host fail upgrad cluster suse,This is due to upgrade. We need to change it manually after upgrade://Now we have (redhat  suse  debian  other_detected_by_python)vi /etc/ambari-server/conf/ambari.properties(server.os_type=sles11) --&gt;( server.os_type=suse11)+restart the serverShould upgrade automatically deal with this?old code: os_info = platform.linux_distribution( None  None  None  ['SuSE'  'redhat' ]  0 ) os_name = os_info[0].lower() if os_name == 'suse': os_name = 'sles' os_version = os_info[1].split('.'  1)[0] master_os_type = os_name + os_version write_property(OS_TYPE_PROPERTY  master_os_type), thi due upgrad we need chang manual upgrad now redhat suse debian detect python vi etc ambari server conf ambari properti server os type sle gt server os type suse restart server should upgrad automat deal old code os info platform linux distribut none none none su se redhat os name os info lower os name suse os name sle os version os info split master os type os name os version write properti os type properti master os type,0,0,0,0,0,0,
5445,When new host components are created thru API  some indication should be given that Nagios has to be restarted, when new host compon creat thru api indic given nagio restart,Need to fix this on API side, need fix api side,0,0,0,0,0,0,
5455,Ambari configuration for map join conversion and tez container size seems wrong, ambari configur map join convers tez contain size seem wrong,For hive:hive.auto.convert.join.noconditionaltask.size is set to 1000000000 This should be a fraction (1/3) of the container size.hive.tez.java.opts has '-Xmx1024m'This is different from both map and reduce sizes. Desired values are: map size if map size &gt; 2g else reduce sizemap size is set on the same cluster to ~500mbThe settings as the are will lead to many failed queries because the mapjoin conversion is to aggressive. If we don't change the container sizes based on cluster configs we will see wide spread problems with containers being killed or perf problems., for hive hive auto convert join noconditionaltask size set thi fraction contain size hive tez java opt xmx thi differ map reduc size desir valu map size map size gt g els reduc sizemap size set cluster mb the set lead mani fail queri mapjoin convers aggress if chang contain size base cluster config see wide spread problem contain kill perf problem,0,0,0,0,0,0,
5457,Host Checks: alternatives check results are not surfaced in Host Check popup, host check altern check result surfac host check popup,Ambari Agent  as part of host checks  identifies conflicting 'alternatives' settings and reports back inside the 'last_agent_env' object.UI is not surfacing this in Host Checks popup.We should have a section called 'Alternatives Issues' and list out the alternatives names.For example: 'last_agent_env' : { 'stackFoldersAndFiles' : [ ... ]  'alternatives' : [ { 'name' : 'zookeeper-conf'  'target' : '/etc/zookeeper/conf.dist' }  { 'name' : 'hadoop-conf'  'target' : '/etc/hadoop/conf.dist' }  ]  'existingUsers' : [ .... ]  'existingRepos' : [ ... ]  ...In the above case  we want to highlight the fact that hadoop-conf and zookeeper-conf have conflicts.Alternatives Issues--------The following alternatives should be removedAlternativeshadoop-conf Exists on 3 hostszookeeper-conf Exists on 3 hosts, ambari agent part host check identifi conflict altern set report back insid last agent env object ui surfac host check popup we section call altern issu list altern name for exampl last agent env stack folder and file altern name zookeep conf target etc zookeep conf dist name hadoop conf target etc hadoop conf dist exist user exist repo in case want highlight fact hadoop conf zookeep conf conflict altern issu the follow altern remov alternativeshadoop conf exist hostszookeep conf exist host,0,0,0,0,0,0,
5459,Usability: Improve Stack Definition support for repositories, usabl improv stack definit support repositori,1) Remove HDP-UTILS from Ambari .repo  and move into the HDP Stack Definition (Ambari does not depend on HDP-UTILS so this causes confusion  makes it harder to doc local repo setup  and is unclear on failures if not setup correctly).2) Requires support for multiple repositories in a Stack Definition  and ability to specify multiple repositories from UI., remov hdp util ambari repo move hdp stack definit ambari depend hdp util caus confus make harder doc local repo setup unclear failur setup correctli requir support multipl repositori stack definit abil specifi multipl repositori ui,0,0,0,0,0,0,
5472,Use SchemaTool in Hive for init metastore DB schema, use schema tool hive init metastor db schema,When Ambari create the metastore database in MySQL it uses auto create feature. This does not create the transaction tables  so any ACID operations (including streaming ingest) will not work., when ambari creat metastor databas my sql use auto creat featur thi creat transact tabl acid oper includ stream ingest work,0,0,0,0,0,0,
5511,Misleading hardcoded command in paragraph 2 on step 'Manual commands' of 'Move Master' wizard, mislead hardcod command paragraph step manual command move master wizard,STR:1. Deploy cluster with multiplied NN directory(in our case /grid/0/hadoop/hdfs/namenode  /grid/1/hadoop/hdfs/namenode).2. Start NN moving.3. Reach step 'Manual commands'.Actual results:Paragraph 1 contains information about all dirs we should move.Paragraph 2 contains message 'Login to the target host XXX and change permissons for the NameNode dirs by running:' and only one command with hardcoded NN dir:chown -R hdfs:hadoop /hadoop/hdfs/namenode/Screenshot attached.When there are multiple directories specified  we need to show the actual path (but replace commas with a space).In case of '/grid/0/hadoop/hdfs/namenode /grid/1/hadoop/hdfs/namenode' as in the attached image  we should display:chown -R hdfs:hadoop /grid/0/hadoop/hdfs/namenode /grid/1/hadoop/hdfs/namenode, str deploy cluster multipli nn directori case grid hadoop hdf namenod grid hadoop hdf namenod start nn move reach step manual command actual result paragraph contain inform dir move paragraph contain messag login target host xxx chang permisson name node dir run one command hardcod nn dir chown r hdf hadoop hadoop hdf namenod screenshot attach when multipl directori specifi need show actual path replac comma space in case grid hadoop hdf namenod grid hadoop hdf namenod attach imag display chown r hdf hadoop grid hadoop hdf namenod grid hadoop hdf namenod,1,0,0,0,0,0,
5512,Move wizard and HA wizard gets stuck on any deploy step, move wizard ha wizard get stuck deploy step,Because of JS error (page refresh makes no effect)., becaus js error page refresh make effect,0,0,0,0,0,0,
5524,Unit tests for number_utils  string_utils  validator and misc files., unit test number util string util valid misc file,Create unit tests for following files:utils/misc.jsutils/number_utils.jsutils/string_utils.jsutils/validator.js, creat unit test follow file util misc jsutil number util jsutil string util jsutil valid js,0,0,0,0,0,0,
5531,Switch SQL standard authorization to be off by default., switch sql standard author default,For Ambari 1.5.1 SQL standard authorization was on by default.Users with certification suites are running into problems related to this feature (which were not bugs in the auth systems  rather they were additional requirements to using it). This needs to be turned off by default. The feature is controlled by:hive.security.authorization.enabledWe want the default value to be set to false in Ambari 1.6.0., for ambari sql standard author default user certif suit run problem relat featur bug auth system rather addit requir use thi need turn default the featur control hive secur author enabl we want default valu set fals ambari,0,0,0,0,0,0,
5543,Unit tests for steps 6 (with small refactor), unit test step small refactor,,,0,0,0,0,0,0,
5546,Call for requests with 'page_size' always return 10 most recent, call request page size alway return recent,Problem:Call: /api/v1/clusters/cl1/requests?to=end&amp;page_size=20Actually result: return most recent 10 requestsExpected result: return most recent 20 requests.Call: /api/v1/clusters/cl1/requests?from=start&amp;page_size=3Actually result: return first 3 requests started from the first in most recent 10Expected result: return first 3 requests started from the very first('install services')In this case  we can never get history requests made before most recent 10., problem call api v cluster cl request end amp page size actual result return recent request expect result return recent request call api v cluster cl request start amp page size actual result return first request start first recent expect result return first request start first instal servic in case never get histori request made recent,0,0,0,0,0,0,
5551,Checkbox 'client' without upper case letter, checkbox client without upper case letter,,,0,0,0,0,0,0,
5555,NameNode HA wizard: Review page appears blank, name node ha wizard review page appear blank,,,0,0,0,0,0,0,
5558,Navigating back from Host page to Heatmaps page is broken, navig back host page heatmap page broken,STR: Go to Dashboard page. Switch to 'Heatmaps' tab. Click on first host. On host page click 'Back'.Actual result: Appeared 'Cluster Status and Metrics' tab.Expected result: Should appear 'Heatmaps' tab., str go dashboard page switch heatmap tab click first host on host page click back actual result appear cluster statu metric tab expect result should appear heatmap tab,0,0,0,0,0,0,
5569,Fix UI Unit tests, fix ui unit test,,,0,0,0,0,0,0,
5571,Restart option is enabled for components in 'Decommissioned' state but it should not, restart option enabl compon decommiss state,'Restart' option should not be enabled if a slave component is in 'decommissioned' state  but it is.STR:1) Go to 'Host details' page2) Make any slave component 'Decommissioned'Actual result: 'Restart' option is enabled (see screenshot)Expected result: 'Restart' option should be disabled., restart option enabl slave compon decommiss state str go host detail page make slave compon decommiss actual result restart option enabl see screenshot expect result restart option disabl,0,0,0,0,0,0,
5577,Turn Off Maintenance Mode for HDFS does not work (problems with ambari-agent), turn off mainten mode hdf work problem ambari agent,STR:Turn On Maintenance Mode for HDFSTurn Off Maintenance Mode for HDFSExpected result:Have not problems with ambari-agent.Actual result:Have problems with ambari-agent., str turn on mainten mode hdf turn off mainten mode hdf expect result have problem ambari agent actual result have problem ambari agent,0,0,0,0,0,0,
5603,Ambari version is unknown during installer via UI, ambari version unknown instal via ui,During installer user does not see Ambari Version from UI (see screenshot)  but on monitoring phase it's available in the same Admin -&gt; About, dure instal user see ambari version ui see screenshot monitor phase avail admin gt about,0,0,0,0,0,0,
5605,Usability UX: Default key actions for dialog boxes, usabl ux default key action dialog box,PROBLEM: Default action for dialog boxes in AmbariUSE CASE: Ambari UI doesn't allow you to press enter and trigger default actions. When a default action is high lighted 'green button' you should be able to hit enter and have the action be triggered. In this case add hit enter should make okay button trigger. Also  pressing escape should cancel the dialog box., problem default action dialog box ambari use case ambari ui allow press enter trigger default action when default action high light green button abl hit enter action trigger in case add hit enter make okay button trigger also press escap cancel dialog box,0,0,0,0,0,0,
5607,Yarn Nodemanager Metrics only update every few minutes, yarn nodemanag metric updat everi minut,Yarn Nodemanager Metrics take far too long between updates.To demonstrate:Run Terasort or anything that runs mapreduce:hdfs dfs -mkdir -p benchmarks/terasorthadoop jar /usr/lib/hadoop-mapreduce/hadoop-mapreduce-examples.jar teragen -Dmapred.map.tasks=72 -Dmapred.reduce.tasks=36 1000000 benchmarks/terasort/inputhadoop jar /usr/lib/hadoop-mapreduce/hadoop-mapreduce-examples.jar terasort -Dmapred.map.tasks=72 -Dmapred.reduce.tasks=36 benchmarks/terasort/input benchmarks/terasort/outputhdfs dfs -rm -R -skipTrash benchmarks/terasortThen repeatedly probe the API at:https://&lt;server&gt;:8081/api/v1/clusters/c1/services/YARN/components/NODEMANAGER?fields=host_components/metrics/yarnIt usually takes 2-3 minutes to see the metrics update  very repeatable., yarn nodemanag metric take far long updat to demonstr run terasort anyth run mapreduc hdf df mkdir p benchmark terasorthadoop jar usr lib hadoop mapreduc hadoop mapreduc exampl jar teragen dmapr map task dmapr reduc task benchmark terasort inputhadoop jar usr lib hadoop mapreduc hadoop mapreduc exampl jar terasort dmapr map task dmapr reduc task benchmark terasort input benchmark terasort outputhdf df rm r skip trash benchmark terasort then repeatedli probe api http lt server gt api v cluster c servic yarn compon nodemanag field host compon metric yarn it usual take minut see metric updat repeat,0,0,0,0,1,0,
5612,Unit tests for object_utils  date  ui_effects  updater, unit test object util date ui effect updat,Create unit tests for following files: utils/object.js utils/date_utils.js utils/ui_effects_utils.js utils/updater.js, creat unit test follow file util object js util date util js util ui effect util js util updat js,0,0,0,0,0,0,
5622,'Upgrading schema' failed during upgrading to 1.6.0, upgrad schema fail upgrad,Postgres issue:new restart_required relies on eclipselink default type converters  we avoided this in pastorg.postgresql.util.PSQLException: ERROR: column 'restart_required' is of type boolean but expression is of type integer Hint: You will need to rewrite or cast the expression. Position: 57 at org.postgresql.core.v3.QueryExecutorImpl.receiveErrorResponse(QueryExecutorImpl.java:2161) at org.postgresql.core.v3.QueryExecutorImpl.processResults(QueryExecutorImpl.java:1890) at org.postgresql.core.v3.QueryExecutorImpl.execute(QueryExecutorImpl.java:255) at org.postgresql.jdbc2.AbstractJdbc2Statement.execute(AbstractJdbc2Statement.java:559) at org.postgresql.jdbc2.AbstractJdbc2Statement.executeWithFlags(AbstractJdbc2Statement.java:403) at org.postgresql.jdbc2.AbstractJdbc2Statement.executeUpdate(AbstractJdbc2Statement.java:331) at org.apache.ambari.server.orm.DBAccessorImpl.updateTable(DBAccessorImpl.java:447) at org.apache.ambari.server.orm.DBAccessorImpl.addColumn(DBAccessorImpl.java:371) at org.apache.ambari.server.upgrade.UpgradeCatalog160.executeDDLUpdates(UpgradeCatalog160.java:72) at org.apache.ambari.server.upgrade.AbstractUpgradeCatalog.upgradeSchema(AbstractUpgradeCatalog.java:177) at org.apache.ambari.server.upgrade.SchemaUpgradeHelper.executeUpgrade(SchemaUpgradeHelper.java:176) at org.apache.ambari.server.upgrade.SchemaUpgradeHelper.main(SchemaUpgradeHelper.java:225)MySQL issue:Inreresting MySQL feature  there should be no space between function name and parenthesis18:57:00 629 WARN [main] DBAccessorImpl:469 - Error executing query: insert into request(request_id  cluster_id  request_context  start_time  end_time  create_time) select distinct s.request_id  s.cluster_id  s.request_context  coalesce (cmd.start_time  -1)  coalesce (cmd.end_time  -1)  -1 from (select distinct request_id  cluster_id  request_context from stage ) s left join (select request_id  min(start_time) as start_time  max(end_time) as end_time from host_role_command group by request_id) cmd on s.request_id=cmd.request_idcom.mysql.jdbc.exceptions.jdbc4.MySQLSyntaxErrorException: FUNCTION ambari.coalesce does not exist at sun.reflect.GeneratedConstructorAccessor14.newInstance(Unknown Source) at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45) at java.lang.reflect.Constructor.newInstance(Constructor.java:526) at com.mysql.jdbc.Util.handleNewInstance(Util.java:411) at com.mysql.jdbc.Util.getInstance(Util.java:386) at com.mysql.jdbc.SQLError.createSQLException(SQLError.java:1054) at com.mysql.jdbc.MysqlIO.checkErrorPacket(MysqlIO.java:4237) at com.mysql.jdbc.MysqlIO.checkErrorPacket(MysqlIO.java:4169) at com.mysql.jdbc.MysqlIO.sendCommand(MysqlIO.java:2617) at com.mysql.jdbc.MysqlIO.sqlQueryDirect(MysqlIO.java:2778) at com.mysql.jdbc.ConnectionImpl.execSQL(ConnectionImpl.java:2828) at com.mysql.jdbc.ConnectionImpl.execSQL(ConnectionImpl.java:2777) at com.mysql.jdbc.StatementImpl.execute(StatementImpl.java:949) at com.mysql.jdbc.StatementImpl.execute(StatementImpl.java:795) at org.apache.ambari.server.orm.DBAccessorImpl.executeQuery(DBAccessorImpl.java:466) at org.apache.ambari.server.orm.DBAccessorImpl.executeQuery(DBAccessorImpl.java:452) at org.apache.ambari.server.upgrade.UpgradeCatalog150.executeDDLUpdates(UpgradeCatalog150.java:325) at org.apache.ambari.server.upgrade.AbstractUpgradeCatalog.upgradeSchema(AbstractUpgradeCatalog.java:177) at org.apache.ambari.server.upgrade.SchemaUpgradeHelper.executeUpgrade(SchemaUpgradeHelper.java:176) at org.apache.ambari.server.upgrade.SchemaUpgradeHelper.main(SchemaUpgradeHelper.java:225), postgr issu new restart requir reli eclipselink default type convert avoid pastorg postgresql util psql except error column restart requir type boolean express type integ hint you need rewrit cast express posit org postgresql core v queri executor impl receiv error respons queri executor impl java org postgresql core v queri executor impl process result queri executor impl java org postgresql core v queri executor impl execut queri executor impl java org postgresql jdbc abstract jdbc statement execut abstract jdbc statement java org postgresql jdbc abstract jdbc statement execut with flag abstract jdbc statement java org postgresql jdbc abstract jdbc statement execut updat abstract jdbc statement java org apach ambari server orm db accessor impl updat tabl db accessor impl java org apach ambari server orm db accessor impl add column db accessor impl java org apach ambari server upgrad upgrad catalog execut ddl updat upgrad catalog java org apach ambari server upgrad abstract upgrad catalog upgrad schema abstract upgrad catalog java org apach ambari server upgrad schema upgrad helper execut upgrad schema upgrad helper java org apach ambari server upgrad schema upgrad helper main schema upgrad helper java my sql issu inrerest my sql featur space function name parenthesi warn main db accessor impl error execut queri insert request request id cluster id request context start time end time creat time select distinct request id cluster id request context coalesc cmd start time coalesc cmd end time select distinct request id cluster id request context stage left join select request id min start time start time max end time end time host role command group request id cmd request id cmd request idcom mysql jdbc except jdbc my sql syntax error except function ambari coalesc exist sun reflect gener constructor accessor new instanc unknown sourc sun reflect deleg constructor accessor impl new instanc deleg constructor accessor impl java java lang reflect constructor new instanc constructor java com mysql jdbc util handl new instanc util java com mysql jdbc util get instanc util java com mysql jdbc sql error creat sql except sql error java com mysql jdbc mysql io check error packet mysql io java com mysql jdbc mysql io check error packet mysql io java com mysql jdbc mysql io send command mysql io java com mysql jdbc mysql io sql queri direct mysql io java com mysql jdbc connect impl exec sql connect impl java com mysql jdbc connect impl exec sql connect impl java com mysql jdbc statement impl execut statement impl java com mysql jdbc statement impl execut statement impl java org apach ambari server orm db accessor impl execut queri db accessor impl java org apach ambari server orm db accessor impl execut queri db accessor impl java org apach ambari server upgrad upgrad catalog execut ddl updat upgrad catalog java org apach ambari server upgrad abstract upgrad catalog upgrad schema abstract upgrad catalog java org apach ambari server upgrad schema upgrad helper execut upgrad schema upgrad helper java org apach ambari server upgrad schema upgrad helper main schema upgrad helper java,0,0,0,0,0,0,
5628,Explicitly disabling datanucleus l2 cache for hive, explicitli disabl datanucleu l cach hive,Ambari installations of hive currently do not set any datanucleus related properties. There is such a thing as a datanucleus l2 cache  that is pretty bad for hive in a distributed environment if it is set. (If there is a lone embedded hive instance  with no other codepaths to the db  then it's fine  but that never happens in a distributed environment.)By default  if no setting is present  datanucleus defaults the l2 cache to being on  so hive ups the ante by defaulting to turning it off by default if no other setting is configured.Now  in a war of 'defaults'  the hive default should win  but this is an area where we have had recurring support issues from clients that turn it on expecting improved performance. Thus  I'd like ambari installed hive-site.xml to explicitly have this config parameter turned off  with a comment asking users to not switch it on as it impacts hive negatively.The parameter in question is 'datanucleus.cache.level2.type'   and it's value should be 'none'. (Note that I've seen some older configs that seem to do things like turning datanucleus.cache.level2 = false and stuff like that  that is bogus config and does nothing and should not be assumed to be a catch-all enabler.)As a comment  I'd like the following comment 'Disables datanucleus l2 cache. This must be set to 'none' for hive to work properly' or something to that effect., ambari instal hive current set datanucleu relat properti there thing datanucleu l cach pretti bad hive distribut environ set if lone embed hive instanc codepath db fine never happen distribut environ by default set present datanucleu default l cach hive up ant default turn default set configur now war default hive default win area recur support issu client turn expect improv perform thu i like ambari instal hive site xml explicitli config paramet turn comment ask user switch impact hive neg the paramet question datanucleu cach level type valu none note i seen older config seem thing like turn datanucleu cach level fals stuff like bogu config noth assum catch enabl as comment i like follow comment disabl datanucleu l cach thi must set none hive work properli someth effect,0,0,0,0,0,0,
5633,Start Services command gets stuck for about 30 mins, start servic command get stuck min,On Security wizard Start Services command ZOOKEEPER_SERVER Start task scheduled on ambari-server host remained in QUEUED status for about 30 mins and other non-completed commands were in PENDING status. For this time interval no command was in IN_PROGRESS status.Later executing stop all services command on the same cluster also made ZOOKEEPER_SERVER Stop task scheduled on ambari-server host to remain in QUEUED status for around 20 mins., on secur wizard start servic command zookeep server start task schedul ambari server host remain queu statu min non complet command pend statu for time interv command in progress statu later execut stop servic command cluster also made zookeep server stop task schedul ambari server host remain queu statu around min,0,0,0,0,0,0,
5643,Add Services is disabled after upgrading the stack from HDP-2.0 to HDP-2.1, add servic disabl upgrad stack hdp hdp,Prior to upgrade  launch Add Services wizard using Ambari 1.5.0 (crucial step) Upgrade stack to a stack with new services and Ambari to 1.5.1 Add Services button is disabled  even though there are services that have not been added to the cluster, prior upgrad launch add servic wizard use ambari crucial step upgrad stack stack new servic ambari add servic button disabl even though servic ad cluster,0,0,0,0,0,0,
5646,Jobs dont show up as links for a moment when page is visited, job dont show link moment page visit,I had a couple of finished jobs showing the jobs page. Then I clicked on one job and went back to the jobs page. For a moment both jobs were not shown as links - though they are clickable. This is basically an appearance issue where the job doesnt look like a link for a moment., i coupl finish job show job page then i click one job went back job page for moment job shown link though clickabl thi basic appear issu job doesnt look like link moment,0,0,0,0,0,0,
5656,Views: do not let the user click on the Views icon in the top nav, view let user click view icon top nav,When clicking on the Views icon in the top nav  the page content turns blank.We will show a view index page in a later release  but for now  let's just disable clicking on that icon., when click view icon top nav page content turn blank we show view index page later releas let disabl click icon,0,0,0,0,0,0,
5668,JobsDiagnostic|2.1.1: No job status and end time is shown for interrupted job, job diagnost no job statu end time shown interrupt job,Enabled tez engine for hive;Executed select query;If while job is running I stop it by double pressign CTRL-C in hive shell then job will not have end time shown in jobs table and no failed icon (red X)  but in job details it will have status 'killed' and correct end time.If job is killed with yarn application -kill it will have correct end time and failed icon displayed in jobs table  but in job details it will not have neither status nor end time., enabl tez engin hive execut select queri if job run i stop doubl pressign ctrl c hive shell job end time shown job tabl fail icon red x job detail statu kill correct end time if job kill yarn applic kill correct end time fail icon display job tabl job detail neither statu end time,0,0,0,0,0,0,
5669,Alternatives issues has error message (missing translation), altern issu error messag miss translat,Build 1.6.0-151Missing translation: installer.step3.hostWarningsPopup.alternatives.emptyinstaller.step3.hostWarningsPopup.alternatives.empty, build miss translat instal step host warn popup altern emptyinstal step host warn popup altern empti,0,0,0,0,0,0,
5688,Zookeeper smoke test failed after being triggered after deleting a host  containing ZookeeperServer, zookeep smoke test fail trigger delet host contain zookeep server,1. Stopped all host components on a host ( to be deleted) with ZookeeperServer. 2. Deleted a host from cluster. 3. Ran Zookeeper Service Check.Actual result: failed. Expected: ok., stop host compon host delet zookeep server delet host cluster ran zookeep servic check actual result fail expect ok,0,0,0,0,0,0,
5692,Unit tests for utils/config.js part 1, unit test util config js part,Update unit tests for utils/config.js., updat unit test util config js,0,0,0,0,0,0,
5714,Views list not loading in Ambari Web, view list load ambari web,In build 1.6.0-159  the views call is being made but it's not listing the views in the dropdown.This was due to the API change and Ambari Web hasn't adjusted yet to this introduction of /versions/., in build view call made list view dropdown thi due api chang ambari web adjust yet introduct version,0,0,0,0,0,0,
5716,Disable security fails occasionally, disabl secur fail occasion,There is a possibility that 'Start all Services' command will fail on Disable Security wizard if App.Service DS model was not populated on the load of 'Disable security page' (timing issue). We need to make sure that the Service model has populated before the 'Disable Security page' is rendered., there possibl start servic command fail disabl secur wizard app servic ds model popul load disabl secur page time issu we need make sure servic model popul disabl secur page render,0,0,0,1,0,0,
5720,pig.properties should set pig.location.check.strict to false, pig properti set pig locat check strict fals,pig.properties should set pig.location.check.strict to false, pig properti set pig locat check strict fals,0,0,0,0,0,0,
5722,All Services Fail To Deploy Due To Agent Parsing Exception, all servic fail to deploy due to agent pars except,When deploying a brand new cluster  all services fail to install due to a parsing exception thrown from the Ambari Agents.File '/usr/lib/python2.6/site-packages/ambari_agent/CustomServiceOrchestrator.py'  line 113  in runCommandjson_path = self.dump_command_to_json(command)File '/usr/lib/python2.6/site-packages/ambari_agent/CustomServiceOrchestrator.py'  line 209  in dump_command_to_jsoncommand'clusterHostInfo' = manifestGenerator.decompressClusterHostInfo(command'clusterHostInfo')File '/usr/lib/python2.6/site-packages/ambari_agent/manifestGenerator.py'  line 116  in decompressClusterHostInfoindexes = convertRangeToList(v)File '/usr/lib/python2.6/site-packages/ambari_agent/manifestGenerator.py'  line 57  in convertRangeToListraise AgentException.AgentException('Broken data in given range  expected - ''m-n'' or ''m''  got : ' + str(r))AgentException: 'Broken data in given range  expected - m-n or m  got : -1??he command being sent is{hs_host=[2]  namenode_host=[1]  snamenode_host=[2]  zookeeper_hosts=[0-2]  ganglia_server_host=[1]  nm_hosts=[0]  ganglia_monitor_hosts=[0-2]  all_hosts=[c6403.ambari.apache.org  c6401.ambari.apache.org  c6402.ambari.apache.org]  rm_host=[2]  app_timeline_server_hosts=[2]  slave_hosts=[0]  ambari_server_host=[-1]  nagios_server_host=[1]  all_ping_ports=[8670:0-2]}Notice the ambari-server-host which was added in that commit; it?? value is ??1??which would not parse correctly in manifestGenerator.pyI suspect Git e667dc7c9870864ff537374c819b7c1d1dd88e98 caused this problem.Steps to reproduce:1) Provision 3 c64 hosts2) Wipe your server database and re-create it with the embedded PSQL script3) Attempt to provision a cluster with various services.All services will fail to deploy b/c of the above exception. This was working without issues before the above suspect commit., when deploy brand new cluster servic fail instal due pars except thrown ambari agent file usr lib python site packag ambari agent custom servic orchestr py line run commandjson path self dump command json command file usr lib python site packag ambari agent custom servic orchestr py line dump command jsoncommand cluster host info manifest gener decompress cluster host info command cluster host info file usr lib python site packag ambari agent manifest gener py line decompress cluster host infoindex convert rang to list v file usr lib python site packag ambari agent manifest gener py line convert rang to listrais agent except agent except broken data given rang expect n got str r agent except broken data given rang expect n got command sent hs host namenod host snamenod host zookeep host ganglia server host nm host ganglia monitor host host c ambari apach org c ambari apach org c ambari apach org rm host app timelin server host slave host ambari server host nagio server host ping port notic ambari server host ad commit valu would pars correctli manifest gener py i suspect git e dc c ff c b c dd e caus problem step reproduc provis c host wipe server databas creat embed psql script attempt provis cluster variou servic all servic fail deploy b c except thi work without issu suspect commit,0,0,0,0,0,1,
5726,Adding Oozie failed at service check, ad oozi fail servic check,This happens because HDFS and YARN/MapReduce requires to be restarted for Oozie smoke test to pass successfullyAs a fix to this issue: Don't run smoke test on 'Install  Start and Test' page of the add service wizard. Review page should ask user to restart all stale services., thi happen hdf yarn map reduc requir restart oozi smoke test pass success as fix issu don run smoke test instal start test page add servic wizard review page ask user restart stale servic,0,0,0,0,0,0,
5730,Space Error in container-executor.cfg, space error contain executor cfg,There is a space between 'banned.user' and '=' which make configuration here is ignored by container-executor  so some default banned users works to include hdfs. Space should be deleted between 'banned.user' and '='yarn.nodemanager.local-dirs=/grid/0/hadoop/yarn/local /grid/1/hadoop/yarn/localyarn.nodemanager.log-dirs=/grid/0/hadoop/yarn/log /grid/1/hadoop/yarn/logyarn.nodemanager.linux-container-executor.group=hadoopbanned.users = hdfs yarn mapred binmin.user.id=1000It should be banned.users=hdfs yarn mapred bin, there space ban user make configur ignor contain executor default ban user work includ hdf space delet ban user yarn nodemanag local dir grid hadoop yarn local grid hadoop yarn localyarn nodemanag log dir grid hadoop yarn log grid hadoop yarn logyarn nodemanag linux contain executor group hadoopban user hdf yarn mapr binmin user id it ban user hdf yarn mapr bin,0,0,0,0,0,0,
5735,HDP deployment failed in CentOS5, hdp deploy fail cent os,Fail: Execution of 'mkdir -p /tmp/HDP-artifacts/ ; curl --noproxy hadoop -kf --retry 10 http://hadoop:8080/resources//jdk-7u45-linux-x64.tar.gz -o /tmp/HDP-artifacts//jdk-7u45-linux-x64.tar.gz' returned 2. curl: option --noproxy: is unknownversion of curl that is available at Centos 5 and SLES 11 SP1 seems to have no support for '--noproxy' option.But such workaround works:no_proxy=i.ua curl http://www.i.ua -iI'm going to replace all '--noproxy' invocations with usage of $no_proxy env variable., fail execut mkdir p tmp hdp artifact curl noproxi hadoop kf retri http hadoop resourc jdk u linux x tar gz tmp hdp artifact jdk u linux x tar gz return curl option noproxi unknownvers curl avail cento sle sp seem support noproxi option but workaround work proxi ua curl http www ua i go replac noproxi invoc usag proxi env variabl,0,0,0,0,0,0,
5739,File View Cleanup, file view cleanup,A few items that need to be rectified for the File view submission:1. Modify code to abide by Ambari Coding Standards2. JavaDoc Interfaces and Methods.https://cwiki.apache.org/confluence/display/AMBARI/Coding+Guidelines+for+Ambari., a item need rectifi file view submiss modifi code abid ambari code standard java doc interfac method http cwiki apach org confluenc display ambari code guidelin ambari,0,0,0,0,0,0,
5751,Ambari upgrade to Ambari-1.6.0 from Ambari-1.5.1 logs PSQLException, ambari upgrad ambari ambari log psql except,Following upgrade documentation at http://docs.hortonworks.com/HDPDocuments/Ambari-1.5.1.0/bk_upgrading_Ambari/content/ambari-chap7_2x.html On executing ambari-server upgrade  PSQLException is logged inambari-server.log:21:36:20 498 INFO [main] SchemaUpgradeHelper:211 - Upgrading schema to target version = 1.6.021:36:20 528 INFO [main] SchemaUpgradeHelper:220 - Upgrading schema from source version = 1.5.1.11021:36:20 530 INFO [main] SchemaUpgradeHelper:141 - Upgrade path: [{ org.apache.ambari.server.upgrade.UpgradeCatalog160$$EnhancerByGuice$$ff8a4f66: sourceVersion = null  targetVersion = 1.6.0 }]21:36:20 530 INFO [main] SchemaUpgradeHelper:171 - Executing DDL upgrade...21:36:20 533 INFO [main] DBAccessorImpl:463 - Executing query: ALTER SCHEMA ambari OWNER TO 'ambari';21:36:20 534 INFO [main] DBAccessorImpl:463 - Executing query: ALTER ROLE 'ambari' SET search_path to 'ambari';21:36:20 598 INFO [main] DBAccessorImpl:463 - Executing query: CREATE TABLE hostgroup_configuration (blueprint_name VARCHAR(255) NOT NULL  hostgroup_name VARCHAR(255) NOT NULL  type_name VARCHAR(255) NOT NULL  config_data BYTEA NOT NULL  PRIMARY KEY (blueprint_name  hostgroup_name  type_name))21:36:20 962 INFO [main] DBAccessorImpl:463 - Executing query: CREATE TABLE viewentity (id BIGINT NOT NULL  view_name VARCHAR(255) NOT NULL  view_instance_name VARCHAR(255) NOT NULL  class_name VARCHAR(255) NOT NULL  id_property VARCHAR(255)  PRIMARY KEY (id))21:36:21 010 INFO [main] DBAccessorImpl:463 - Executing query: ALTER TABLE hostcomponentdesiredstate ADD restart_required BOOLEAN21:36:21 078 INFO [main] DBAccessorImpl:463 - Executing query: ALTER TABLE hostgroup_configuration ADD CONSTRAINT FK_hg_config_blueprint_name FOREIGN KEY (blueprint_name) REFERENCES hostgroup (blueprint_name)21:36:21 084 WARN [main] DBAccessorImpl:469 - Error executing query: ALTER TABLE hostgroup_configuration ADD CONSTRAINT FK_hg_config_blueprint_name FOREIGN KEY (blueprint_name) REFERENCES hostgroup (blueprint_name)org.postgresql.util.PSQLException: ERROR: there is no unique constraint matching given keys for referenced table 'hostgroup' at org.postgresql.core.v3.QueryExecutorImpl.receiveErrorResponse(QueryExecutorImpl.java:2161) at org.postgresql.core.v3.QueryExecutorImpl.processResults(QueryExecutorImpl.java:1890) at org.postgresql.core.v3.QueryExecutorImpl.execute(QueryExecutorImpl.java:255) at org.postgresql.jdbc2.AbstractJdbc2Statement.execute(AbstractJdbc2Statement.java:559) at org.postgresql.jdbc2.AbstractJdbc2Statement.executeWithFlags(AbstractJdbc2Statement.java:403) at org.postgresql.jdbc2.AbstractJdbc2Statement.execute(AbstractJdbc2Statement.java:395) at org.apache.ambari.server.orm.DBAccessorImpl.executeQuery(DBAccessorImpl.java:466) at org.apache.ambari.server.orm.DBAccessorImpl.executeQuery(DBAccessorImpl.java:452) at org.apache.ambari.server.orm.DBAccessorImpl.addFKConstraint(DBAccessorImpl.java:337) at org.apache.ambari.server.orm.DBAccessorImpl.addFKConstraint(DBAccessorImpl.java:321) at org.apache.ambari.server.upgrade.UpgradeCatalog160.executeDDLUpdates(UpgradeCatalog160.java:85) at org.apache.ambari.server.upgrade.AbstractUpgradeCatalog.upgradeSchema(AbstractUpgradeCatalog.java:250) at org.apache.ambari.server.upgrade.SchemaUpgradeHelper.executeUpgrade(SchemaUpgradeHelper.java:176) at org.apache.ambari.server.upgrade.SchemaUpgradeHelper.main(SchemaUpgradeHelper.java:225)21:36:21 089 WARN [main] DBAccessorImpl:339 - Add FK constraint failed  constraintName = FK_hg_config_blueprint_name  tableName = hostgroup_configurationorg.postgresql.util.PSQLException: ERROR: there is no unique constraint matching given keys for referenced table 'hostgroup' at org.postgresql.core.v3.QueryExecutorImpl.receiveErrorResponse(QueryExecutorImpl.java:2161) at org.postgresql.core.v3.QueryExecutorImpl.processResults(QueryExecutorImpl.java:1890) at org.postgresql.core.v3.QueryExecutorImpl.execute(QueryExecutorImpl.java:255) at org.postgresql.jdbc2.AbstractJdbc2Statement.execute(AbstractJdbc2Statement.java:559) at org.postgresql.jdbc2.AbstractJdbc2Statement.executeWithFlags(AbstractJdbc2Statement.java:403) at org.postgresql.jdbc2.AbstractJdbc2Statement.execute(AbstractJdbc2Statement.java:395) at org.apache.ambari.server.orm.DBAccessorImpl.executeQuery(DBAccessorImpl.java:466) at org.apache.ambari.server.orm.DBAccessorImpl.executeQuery(DBAccessorImpl.java:452) at org.apache.ambari.server.orm.DBAccessorImpl.addFKConstraint(DBAccessorImpl.java:337) at org.apache.ambari.server.orm.DBAccessorImpl.addFKConstraint(DBAccessorImpl.java:321) at org.apache.ambari.server.upgrade.UpgradeCatalog160.executeDDLUpdates(UpgradeCatalog160.java:85) at org.apache.ambari.server.upgrade.AbstractUpgradeCatalog.upgradeSchema(AbstractUpgradeCatalog.java:250) at org.apache.ambari.server.upgrade.SchemaUpgradeHelper.executeUpgrade(SchemaUpgradeHelper.java:176) at org.apache.ambari.server.upgrade.SchemaUpgradeHelper.main(SchemaUpgradeHelper.java:225), follow upgrad document http doc hortonwork com hdp document ambari bk upgrad ambari content ambari chap x html on execut ambari server upgrad psql except log inambari server log info main schema upgrad helper upgrad schema target version info main schema upgrad helper upgrad schema sourc version info main schema upgrad helper upgrad path org apach ambari server upgrad upgrad catalog enhanc by guic ff f sourc version null target version info main schema upgrad helper execut ddl upgrad info main db accessor impl execut queri alter schema ambari owner to ambari info main db accessor impl execut queri alter role ambari set search path ambari info main db accessor impl execut queri creat tabl hostgroup configur blueprint name varchar not null hostgroup name varchar not null type name varchar not null config data bytea not null primari key blueprint name hostgroup name type name info main db accessor impl execut queri creat tabl viewent id bigint not null view name varchar not null view instanc name varchar not null class name varchar not null id properti varchar primari key id info main db accessor impl execut queri alter tabl hostcomponentdesiredst add restart requir boolean info main db accessor impl execut queri alter tabl hostgroup configur add constraint fk hg config blueprint name foreign key blueprint name refer hostgroup blueprint name warn main db accessor impl error execut queri alter tabl hostgroup configur add constraint fk hg config blueprint name foreign key blueprint name refer hostgroup blueprint name org postgresql util psql except error uniqu constraint match given key referenc tabl hostgroup org postgresql core v queri executor impl receiv error respons queri executor impl java org postgresql core v queri executor impl process result queri executor impl java org postgresql core v queri executor impl execut queri executor impl java org postgresql jdbc abstract jdbc statement execut abstract jdbc statement java org postgresql jdbc abstract jdbc statement execut with flag abstract jdbc statement java org postgresql jdbc abstract jdbc statement execut abstract jdbc statement java org apach ambari server orm db accessor impl execut queri db accessor impl java org apach ambari server orm db accessor impl execut queri db accessor impl java org apach ambari server orm db accessor impl add fk constraint db accessor impl java org apach ambari server orm db accessor impl add fk constraint db accessor impl java org apach ambari server upgrad upgrad catalog execut ddl updat upgrad catalog java org apach ambari server upgrad abstract upgrad catalog upgrad schema abstract upgrad catalog java org apach ambari server upgrad schema upgrad helper execut upgrad schema upgrad helper java org apach ambari server upgrad schema upgrad helper main schema upgrad helper java warn main db accessor impl add fk constraint fail constraint name fk hg config blueprint name tabl name hostgroup configurationorg postgresql util psql except error uniqu constraint match given key referenc tabl hostgroup org postgresql core v queri executor impl receiv error respons queri executor impl java org postgresql core v queri executor impl process result queri executor impl java org postgresql core v queri executor impl execut queri executor impl java org postgresql jdbc abstract jdbc statement execut abstract jdbc statement java org postgresql jdbc abstract jdbc statement execut with flag abstract jdbc statement java org postgresql jdbc abstract jdbc statement execut abstract jdbc statement java org apach ambari server orm db accessor impl execut queri db accessor impl java org apach ambari server orm db accessor impl execut queri db accessor impl java org apach ambari server orm db accessor impl add fk constraint db accessor impl java org apach ambari server orm db accessor impl add fk constraint db accessor impl java org apach ambari server upgrad upgrad catalog execut ddl updat upgrad catalog java org apach ambari server upgrad abstract upgrad catalog upgrad schema abstract upgrad catalog java org apach ambari server upgrad schema upgrad helper execut upgrad schema upgrad helper java org apach ambari server upgrad schema upgrad helper main schema upgrad helper java,0,0,0,0,0,0,
5753,Storm fails to start after disabling security, storm fail start disabl secur,nimbus.childopts  ui.childopts and supervisor.childopts points to sasl configuration files after the security is disabled. web-ui should remove -Djava.security.auth.login.config parameter from these properties while disabling security., nimbu childopt ui childopt supervisor childopt point sasl configur file secur disabl web ui remov djava secur auth login config paramet properti disabl secur,0,0,0,0,0,0,
5761,2000-node cluster testing: during install phase of cluster deployment  install tasks were stuck in PENDING state, node cluster test instal phase cluster deploy instal task stuck pend state,ActionScheduler is stucked when adding tasks to ActionQueue on large clusters (&gt;1000 nodes), action schedul stuck ad task action queue larg cluster gt node,0,0,0,0,1,0,
5778,In some upgrade scenarios  Ambari Web's persist key-value store state causes the UI to act unpredictably, in upgrad scenario ambari web persist key valu store state caus ui act unpredict,During Ambari upgrade  automatically clear the persist state to prevent potential issues with Ambari Web not working properly (this was observed a number of times on upgraded clusters). Currently  we make the following call to get out of the inconsistent state so that Ambari Web works properly:curl -i -u admin:admin -H 'X-Requested-By: ambari' -X POST -d '{ 'CLUSTER_CURRENT_STATUS': '{/'clusterState/':/'CLUSTER_STARTED_5/'}' }' http://localhost:8080/api/v1/persistWe need to do something equivalent during upgrade., dure ambari upgrad automat clear persist state prevent potenti issu ambari web work properli observ number time upgrad cluster current make follow call get inconsist state ambari web work properli curl u admin admin h x request by ambari x post cluster current statu cluster state cluster start http localhost api v persist we need someth equival upgrad,0,0,0,0,0,0,
5779,Recommission a DN fails when https is enabled in Ambari server, recommiss dn fail http enabl ambari server,After https is enable in Ambari server  Recommission a DN will fails with the following error message found in the Ambari-server log:WARN &#91;qtp1103265648-610&#93; nio:651 - javax.net.ssl.SSLException: Received fatal alert: certificate_unknown 00:32:05 458 INFO &#91;ExecutionScheduler_Worker-1&#93; JobRunShell:207 - Job LinearExecutionJobs.BatchRequestJob-2-1 threw a JobExecutionException: org.quartz.JobExecutionException: org.apache.ambari.server.AmbariException: Exception occurred while performing request &#91;See nested exception: org.apache.ambari.server.AmbariException: Exception occurred while performing request&#93; at org.apache.ambari.server.scheduler.AbstractLinearExecutionJob.execute(AbstractLinearExecutionJob.java:94) at org.quartz.core.JobRunShell.run(JobRunShell.java:202) at org.quartz.simpl.SimpleThreadPool$WorkerThread.run(SimpleThreadPool.java:573) Caused by: org.apache.ambari.server.AmbariException: Exception occurred while performing request at org.apache.ambari.server.scheduler.ExecutionScheduleManager.executeBatchRequest(ExecutionScheduleManager.java:479) at org.apache.ambari.server.state.scheduler.BatchRequestJob.doWork(BatchRequestJob.java:77) at org.apache.ambari.server.scheduler.AbstractLinearExecutionJob.execute(AbstractLinearExecutionJob.java:88) ... 2 more Caused by: com.sun.jersey.api.client.ClientHandlerException: javax.net.ssl.SSLHandshakeException: java.security.cert.CertificateException: No name matching localhost found at com.sun.jersey.client.urlconnection.URLConnectionClientHandler.handle(URLConnectionClientHandler.java:149) at com.sun.jersey.api.client.filter.CsrfProtectionFilter.handle(CsrfProtectionFilter.java:97) at org.apache.ambari.server.security.authorization.internal.InternalTokenClientFilter.handle(InternalTokenClientFilter.java:39) at com.sun.jersey.api.client.Client.handle(Client.java:648) at com.sun.jersey.api.client.WebResource.handle(WebResource.java:670) at com.sun.jersey.api.client.WebResource.method(WebResource.java:311) at org.apache.ambari.server.scheduler.ExecutionScheduleManager.performApiRequest(ExecutionScheduleManager.java:619) at org.apache.ambari.server.scheduler.ExecutionScheduleManager.executeBatchRequest(ExecutionScheduleManager.java:469) ... 4 more Caused by: javax.net.ssl.SSLHandshakeException: java.security.cert.CertificateException: No name matching localhost found at sun.security.ssl.Alerts.getSSLException(Alerts.java:192) at sun.security.ssl.SSLSocketImpl.fatal(SSLSocketImpl.java:1884) at sun.security.ssl.Handshaker.fatalSE(Handshaker.java:276) at sun.security.ssl.Handshaker.fatalSE(Handshaker.java:270) at sun.security.ssl.ClientHandshaker.serverCertificate(ClientHandshaker.java:1341) at sun.security.ssl.ClientHandshaker.processMessage(ClientHandshaker.java:153) at sun.security.ssl.Handshaker.processLoop(Handshaker.java:868) at sun.security.ssl.Handshaker.process_record(Handshaker.java:804) at sun.security.ssl.SSLSocketImpl.readRecord(SSLSocketImpl.java:1016) at sun.security.ssl.SSLSocketImpl.performInitialHandshake(SSLSocketImpl.java:1312) at sun.security.ssl.SSLSocketImpl.startHandshake(SSLSocketImpl.java:1339) at sun.security.ssl.SSLSocketImpl.startHandshake(SSLSocketImpl.java:1323) at sun.net.www.protocol.https.HttpsClient.afterConnect(HttpsClient.java:563) at sun.net.www.protocol.https.AbstractDelegateHttpsURLConnection.connect(AbstractDelegateHttpsURLConnection.java:185) at sun.net.www.protocol.http.HttpURLConnection.getOutputStream(HttpURLConnection.java:1091) at sun.net.www.protocol.https.HttpsURLConnectionImpl.getOutputStream(HttpsURLConnectionImpl.java:250), after http enabl ambari server recommiss dn fail follow error messag found ambari server log warn qtp nio javax net ssl ssl except receiv fatal alert certif unknown info execut schedul worker job run shell job linear execut job batch request job threw job execut except org quartz job execut except org apach ambari server ambari except except occur perform request see nest except org apach ambari server ambari except except occur perform request org apach ambari server schedul abstract linear execut job execut abstract linear execut job java org quartz core job run shell run job run shell java org quartz simpl simpl thread pool worker thread run simpl thread pool java caus org apach ambari server ambari except except occur perform request org apach ambari server schedul execut schedul manag execut batch request execut schedul manag java org apach ambari server state schedul batch request job work batch request job java org apach ambari server schedul abstract linear execut job execut abstract linear execut job java caus com sun jersey api client client handler except javax net ssl ssl handshak except java secur cert certif except no name match localhost found com sun jersey client urlconnect url connect client handler handl url connect client handler java com sun jersey api client filter csrf protect filter handl csrf protect filter java org apach ambari server secur author intern intern token client filter handl intern token client filter java com sun jersey api client client handl client java com sun jersey api client web resourc handl web resourc java com sun jersey api client web resourc method web resourc java org apach ambari server schedul execut schedul manag perform api request execut schedul manag java org apach ambari server schedul execut schedul manag execut batch request execut schedul manag java caus javax net ssl ssl handshak except java secur cert certif except no name match localhost found sun secur ssl alert get ssl except alert java sun secur ssl ssl socket impl fatal ssl socket impl java sun secur ssl handshak fatal se handshak java sun secur ssl handshak fatal se handshak java sun secur ssl client handshak server certif client handshak java sun secur ssl client handshak process messag client handshak java sun secur ssl handshak process loop handshak java sun secur ssl handshak process record handshak java sun secur ssl ssl socket impl read record ssl socket impl java sun secur ssl ssl socket impl perform initi handshak ssl socket impl java sun secur ssl ssl socket impl start handshak ssl socket impl java sun secur ssl ssl socket impl start handshak ssl socket impl java sun net www protocol http http client connect http client java sun net www protocol http abstract deleg http url connect connect abstract deleg http url connect java sun net www protocol http http url connect get output stream http url connect java sun net www protocol http http url connect impl get output stream http url connect impl java,0,0,0,0,0,0,
5782,View: Files UI clean-up and adjustments, view file ui clean adjust,,,0,0,0,0,0,0,
5783,Pig fails to install through blueprint, pig fail instal blueprint,During installation through blueprint pig install fails with next exception:Traceback (most recent call last): File '/var/lib/ambari-agent/cache/stacks/HDP/2.0.6/services/PIG/package/scripts/pig_client.py'  line 41  in &lt;module&gt; PigClient().execute() File '/usr/lib/python2.6/site-packages/resource_management/libraries/script/script.py'  line 112  in execute method(env) File '/var/lib/ambari-agent/cache/stacks/HDP/2.0.6/services/PIG/package/scripts/pig_client.py'  line 30  in install self.configure(env) File '/var/lib/ambari-agent/cache/stacks/HDP/2.0.6/services/PIG/package/scripts/pig_client.py'  line 35  in configure pig() File '/var/lib/ambari-agent/cache/stacks/HDP/2.0.6/services/PIG/package/scripts/pig.py'  line 40  in pig properties=params.pig_properties) File '/usr/lib/python2.6/site-packages/resource_management/core/base.py'  line 148  in __init__ self.env.run() File '/usr/lib/python2.6/site-packages/resource_management/core/environment.py'  line 149  in run self.run_action(resource  action) File '/usr/lib/python2.6/site-packages/resource_management/core/environment.py'  line 115  in run_action provider_action() File '/usr/lib/python2.6/site-packages/resource_management/libraries/providers/properties_file.py'  line 48  in action_create mode = self.resource.mode File '/usr/lib/python2.6/site-packages/resource_management/core/base.py'  line 148  in __init__ self.env.run() File '/usr/lib/python2.6/site-packages/resource_management/core/environment.py'  line 149  in run self.run_action(resource  action) File '/usr/lib/python2.6/site-packages/resource_management/core/environment.py'  line 115  in run_action provider_action() File '/usr/lib/python2.6/site-packages/resource_management/core/providers/system.py'  line 96  in action_create content = self._get_content() File '/usr/lib/python2.6/site-packages/resource_management/core/providers/system.py'  line 136  in _get_content return content() File '/usr/lib/python2.6/site-packages/resource_management/core/source.py'  line 47  in __call__ return self.get_content() File '/usr/lib/python2.6/site-packages/resource_management/core/source.py'  line 126  in get_content rendered = self.template.render(self.context) File '/usr/lib/python2.6/site-packages/jinja2/environment.py'  line 891  in render return self.environment.handle_exception(exc_info  True) File '&lt;template&gt;'  line 2  in top-level template code File '/usr/lib/python2.6/site-packages/jinja2/filters.py'  line 176  in do_dictsort return sorted(value.items()  key=sort_func)AttributeError: 'unicode' object has no attribute 'items', dure instal blueprint pig instal fail next except traceback recent call last file var lib ambari agent cach stack hdp servic pig packag script pig client py line lt modul gt pig client execut file usr lib python site packag resourc manag librari script script py line execut method env file var lib ambari agent cach stack hdp servic pig packag script pig client py line instal self configur env file var lib ambari agent cach stack hdp servic pig packag script pig client py line configur pig file var lib ambari agent cach stack hdp servic pig packag script pig py line pig properti param pig properti file usr lib python site packag resourc manag core base py line init self env run file usr lib python site packag resourc manag core environ py line run self run action resourc action file usr lib python site packag resourc manag core environ py line run action provid action file usr lib python site packag resourc manag librari provid properti file py line action creat mode self resourc mode file usr lib python site packag resourc manag core base py line init self env run file usr lib python site packag resourc manag core environ py line run self run action resourc action file usr lib python site packag resourc manag core environ py line run action provid action file usr lib python site packag resourc manag core provid system py line action creat content self get content file usr lib python site packag resourc manag core provid system py line get content return content file usr lib python site packag resourc manag core sourc py line call return self get content file usr lib python site packag resourc manag core sourc py line get content render self templat render self context file usr lib python site packag jinja environ py line render return self environ handl except exc info true file lt templat gt line top level templat code file usr lib python site packag jinja filter py line dictsort return sort valu item key sort func attribut error unicod object attribut item,0,0,0,0,0,0,
5855,Global properties are not being surfaced on service config page, global properti surfac servic config page,Post install  Global properties are not being surfaced on service config page., post instal global properti surfac servic config page,0,0,0,0,0,0,
5866,Populate actions drop down of Slider App details page, popul action drop slider app detail page,Slider App details page should have an actions dropdown with the following actions Freeze: show when status == RUNNING Thaw: show when status == FROZEN Flex: show when status != FINISHED Destroy: show when status == FROZENExcept for Thaw  all actions will show a confirmation dialog. Hitting OK will make the call. Destroy should call DELETE on the app endpoint Freeze and Thaw should call PUT on app endpoint app state being set to FROZEN or RUNNING., slider app detail page action dropdown follow action freez show statu run thaw show statu frozen flex show statu finish destroy show statu frozen except thaw action show confirm dialog hit ok make call destroy call delet app endpoint freez thaw call put app endpoint app state set frozen run,1,0,0,0,0,0,
5886,Implement app_types endpoint to provide app definitions, implement app type endpoint provid app definit,Slider Apps View should provide /api/v1/app_types endpoint to provide definitions of various app_types supported by this Slider Apps View. Only apps in this type can be created through UI., slider app view provid api v app type endpoint provid definit variou app type support slider app view onli app type creat ui,1,0,0,0,0,0,
5894,New slider app wizard should show app-types from /apptypes endpoint, new slider app wizard show app type apptyp endpoint,Currently the new slider app wizard shows hardcoded app-types. We should instead show only those app-types which are returned by http://c6401:8080/api/v1/views/SLIDER/versions/1.0.0/instances/SLIDER_1/apptypes?fields=* endpoint.{ 'href' : 'http://c6401:8080/api/v1/views/SLIDER/versions/1.0.0/instances/SLIDER_1/apptypes?fields=*'  'items' : [ { 'href' : 'http://c6401:8080/api/v1/views/SLIDER/versions/1.0.0/instances/SLIDER_1/apptypes/ACCUMULO'  'id' : 'ACCUMULO'  'instance_name' : 'SLIDER_1'  'typeComponents' : [ { 'id' : 'ACCUMULO_MASTER'  'name' : 'ACCUMULO_MASTER'  'category' : 'MASTER'  'displayName' : 'ACCUMULO_MASTER'  'priority' : 1  'instanceCount' : 0  'maxInstanceCount' : 0  'yarnMemory' : 1024  'yarnCpuCores' : 1 }  { 'id' : 'ACCUMULO_MONITOR'  'name' : 'ACCUMULO_MONITOR'  'category' : 'MASTER'  'displayName' : 'ACCUMULO_MONITOR'  'priority' : 3  'instanceCount' : 0  'maxInstanceCount' : 0  'yarnMemory' : 1024  'yarnCpuCores' : 1 }  { 'id' : 'ACCUMULO_GC'  'name' : 'ACCUMULO_GC'  'category' : 'MASTER'  'displayName' : 'ACCUMULO_GC'  'priority' : 4  'instanceCount' : 0  'maxInstanceCount' : 0  'yarnMemory' : 1024  'yarnCpuCores' : 1 }  { 'id' : 'ACCUMULO_TRACER'  'name' : 'ACCUMULO_TRACER'  'category' : 'MASTER'  'displayName' : 'ACCUMULO_TRACER'  'priority' : 5  'instanceCount' : 0  'maxInstanceCount' : 0  'yarnMemory' : 1024  'yarnCpuCores' : 1 }  { 'id' : 'ACCUMULO_TSERVER'  'name' : 'ACCUMULO_TSERVER'  'category' : 'SLAVE'  'displayName' : 'ACCUMULO_TSERVER'  'priority' : 2  'instanceCount' : 0  'maxInstanceCount' : 0  'yarnMemory' : 1024  'yarnCpuCores' : 1 }  { 'id' : 'ACCUMULO_CLIENT'  'name' : 'ACCUMULO_CLIENT'  'category' : 'CLIENT'  'displayName' : 'ACCUMULO_CLIENT'  'priority' : 0  'instanceCount' : 0  'maxInstanceCount' : 0  'yarnMemory' : 1024  'yarnCpuCores' : 1 } ]  'typeDescription' : 'The Apache Accumulo sorted  distributed key/value store is a robust /n scalable  high performance data storage system that features cell-based/n access control and customizable server-side processing. It is based on/n Google's BigTable design and is built on top of Apache Hadoop /n Zookeeper  and Thrift./n Requirements:/n 1. Ensure parent dir for path (accumulo-site/instance.dfs.dir) is accessible to the App owner.'  'typeName' : 'ACCUMULO'  'typePackageFileName' : 'accumulo_v151.zip'  'typeVersion' : '1.5.1'  'version' : '1.0.0'  'view_name' : 'SLIDER'  'typeConfigs' : { 'agent.conf' : '/slider/agent/conf/agent.ini'  'application.def' : '/slider/accumulo_v151.zip'  'config_types' : 'accumulo-site'  'java_home' : '/usr/jdk64/jdk1.7.0_45'  'package_list' : 'files/accumulo-1.5.1-bin.tar.gz'  'site.accumulo-site.gc.port.client' : '0'  'site.accumulo-site.general.classpaths' : '$ACCUMULO_HOME/lib/accumulo-server.jar /n$ACCUMULO_HOME/lib/accumulo-core.jar /n$ACCUMULO_HOME/lib/accumulo-start.jar /n$ACCUMULO_HOME/lib/accumulo-fate.jar /n$ACCUMULO_HOME/lib/accumulo-proxy.jar /n$ACCUMULO_HOME/lib/[^.].*.jar /n$ZOOKEEPER_HOME/zookeeper[^.].*.jar /n$HADOOP_CONF_DIR /n$HADOOP_PREFIX/[^.].*.jar /n$HADOOP_PREFIX/lib/[^.].*.jar /n$HADOOP_PREFIX/share/hadoop/common/.*.jar /n$HADOOP_PREFIX/share/hadoop/common/lib/.*.jar /n$HADOOP_PREFIX/share/hadoop/hdfs/.*.jar /n$HADOOP_PREFIX/share/hadoop/mapreduce/.*.jar /n$HADOOP_PREFIX/share/hadoop/yarn/.*.jar /n/usr/lib/hadoop/.*.jar /n/usr/lib/hadoop/lib/.*.jar /n/usr/lib/hadoop-hdfs/.*.jar /n/usr/lib/hadoop-mapreduce/.*.jar /n/usr/lib/hadoop-yarn/.*.jar '  'site.accumulo-site.instance.dfs.dir' : '/apps/accumulo/data'  'site.accumulo-site.instance.secret' : 'DEFAULT'  'site.accumulo-site.instance.zookeeper.host' : '${ZK_HOST}'  'site.accumulo-site.master.port.client' : '0'  'site.accumulo-site.monitor.port.client' : '${ACCUMULO_MONITOR.ALLOCATED_PORT}'  'site.accumulo-site.monitor.port.log4j' : '0'  'site.accumulo-site.trace.port.client' : '0'  'site.accumulo-site.trace.token.property.password' : 'secret'  'site.accumulo-site.trace.user' : 'root'  'site.accumulo-site.tserver.cache.data.size' : '7M'  'site.accumulo-site.tserver.cache.index.size' : '20M'  'site.accumulo-site.tserver.memory.maps.max' : '80M'  'site.accumulo-site.tserver.port.client' : '0'  'site.accumulo-site.tserver.sort.buffer.size' : '50M'  'site.accumulo-site.tserver.walog.max.size' : '100M'  'site.global.accumulo_instance_name' : 'instancename'  'site.global.accumulo_root_password' : 'secret'  'site.global.app_install_dir' : '${AGENT_WORK_ROOT}/app/install'  'site.global.app_log_dir' : '${AGENT_LOG_ROOT}/app/log'  'site.global.app_pid_dir' : '${AGENT_WORK_ROOT}/app/run'  'site.global.app_root' : '${AGENT_WORK_ROOT}/app/install/accumulo-1.5.1'  'site.global.app_user' : 'yarn'  'site.global.gc_heapsize' : '64m'  'site.global.hadoop_conf_dir' : '/etc/hadoop/conf'  'site.global.hadoop_prefix' : '/usr/lib/hadoop'  'site.global.master_heapsize' : '128m'  'site.global.monitor_heapsize' : '64m'  'site.global.other_heapsize' : '128m'  'site.global.security_enabled' : 'false'  'site.global.tserver_heapsize' : '128m'  'site.global.user_group' : 'hadoop'  'site.global.zookeeper_home' : '/usr/lib/zookeeper' } }  { 'href' : 'http://c6401:8080/api/v1/views/SLIDER/versions/1.0.0/instances/SLIDER_1/apptypes/HBASE'  'id' : 'HBASE'  'instance_name' : 'SLIDER_1'  'typeComponents' : [ { 'id' : 'HBASE_MASTER'  'name' : 'HBASE_MASTER'  'category' : 'MASTER'  'displayName' : 'HBASE_MASTER'  'priority' : 1  'instanceCount' : 1  'maxInstanceCount' : 2  'yarnMemory' : 1024  'yarnCpuCores' : 1 }  { 'id' : 'HBASE_REGIONSERVER'  'name' : 'HBASE_REGIONSERVER'  'category' : 'SLAVE'  'displayName' : 'HBASE_REGIONSERVER'  'priority' : 2  'instanceCount' : 1  'maxInstanceCount' : 0  'yarnMemory' : 1024  'yarnCpuCores' : 1 }  { 'id' : 'HBASE_CLIENT'  'name' : 'HBASE_CLIENT'  'category' : 'CLIENT'  'displayName' : 'HBASE_CLIENT'  'priority' : 0  'instanceCount' : 0  'maxInstanceCount' : 0  'yarnMemory' : 1024  'yarnCpuCores' : 1 } ]  'typeDescription' : 'Apache HBase is the Hadoop database  a distributed  scalable  big data store./n Requirements:/n 1. Ensure parent dir for path (hbase-site/hbase.rootdir) is accessible to the App owner./n 2. Ensure ZK root (hbase-site/zookeeper.znode.parent) is unique for the App instance.'  'typeName' : 'HBASE'  'typePackageFileName' : 'hbase_v096 (1).zip'  'typeVersion' : '0.96.0.2.1.1'  'version' : '1.0.0'  'view_name' : 'SLIDER'  'typeConfigs' : { 'agent.conf' : '/slider/agent/conf/agent.ini'  'application.def' : '/slider/hbase_v096.zip'  'config_types' : 'core-site hdfs-site hbase-site'  'java_home' : '/usr/jdk64/jdk1.7.0_45'  'package_list' : 'files/hbase-0.96.1-hadoop2-bin.tar.gz'  'site.core-site.fs.defaultFS' : '${NN_URI}'  'site.global.app_install_dir' : '${AGENT_WORK_ROOT}/app/install'  'site.global.app_log_dir' : '${AGENT_LOG_ROOT}/app/log'  'site.global.app_pid_dir' : '${AGENT_WORK_ROOT}/app/run'  'site.global.app_root' : '${AGENT_WORK_ROOT}/app/install/hbase-0.96.1-hadoop2'  'site.global.app_user' : 'yarn'  'site.global.ganglia_server_host' : '${NN_HOST}'  'site.global.ganglia_server_id' : 'Application1'  'site.global.ganglia_server_port' : '8667'  'site.global.hbase_master_heapsize' : '1024m'  'site.global.hbase_regionserver_heapsize' : '1024m'  'site.global.security_enabled' : 'false'  'site.global.user_group' : 'hadoop'  'site.hbase-site.hbase.client.keyvalue.maxsize' : '10485760'  'site.hbase-site.hbase.client.scanner.caching' : '100'  'site.hbase-site.hbase.cluster.distributed' : 'true'  'site.hbase-site.hbase.defaults.for.version.skip' : 'true'  'site.hbase-site.hbase.hregion.majorcompaction' : '86400000'  'site.hbase-site.hbase.hregion.max.filesize' : '10737418240'  'site.hbase-site.hbase.hregion.memstore.block.multiplier' : '2'  'site.hbase-site.hbase.hregion.memstore.flush.size' : '134217728'  'site.hbase-site.hbase.hregion.memstore.mslab.enabled' : 'true'  'site.hbase-site.hbase.hstore.blockingStoreFiles' : '10'  'site.hbase-site.hbase.hstore.compactionThreshold' : '3'  'site.hbase-site.hbase.hstore.flush.retries.number' : '120'  'site.hbase-site.hbase.local.dir' : '${hbase.tmp.dir}/local'  'site.hbase-site.hbase.master.info.port' : '${HBASE_MASTER.ALLOCATED_PORT}'  'site.hbase-site.hbase.regionserver.global.memstore.lowerLimit' : '0.38'  'site.hbase-site.hbase.regionserver.global.memstore.upperLimit' : '0.4'  'site.hbase-site.hbase.regionserver.handler.count' : '60'  'site.hbase-site.hbase.regionserver.info.port' : '0'  'site.hbase-site.hbase.regionserver.port' : '0'  'site.hbase-site.hbase.rootdir' : '${NN_URI}/apps/hbase/data'  'site.hbase-site.hbase.security.authentication' : 'simple'  'site.hbase-site.hbase.security.authorization' : 'false'  'site.hbase-site.hbase.stagingdir' : '${NN_URI}/apps/hbase/staging'  'site.hbase-site.hbase.superuser' : 'yarn'  'site.hbase-site.hbase.tmp.dir' : '${AGENT_WORK_ROOT}/work/app/tmp'  'site.hbase-site.hbase.zookeeper.property.clientPort' : '2181'  'site.hbase-site.hbase.zookeeper.quorum' : '${ZK_HOST}'  'site.hbase-site.hbase.zookeeper.useMulti' : 'true'  'site.hbase-site.hfile.block.cache.size' : '0.40'  'site.hbase-site.zookeeper.session.timeout' : '30000'  'site.hbase-site.zookeeper.znode.parent' : '/hbase-unsecure'  'site.hdfs-site.dfs.namenode.http-address' : '${NN_HOST}:50070'  'site.hdfs-site.dfs.namenode.https-address' : '${NN_HOST}:50470' } }  { 'href' : 'http://c6401:8080/api/v1/views/SLIDER/versions/1.0.0/instances/SLIDER_1/apptypes/STORM'  'id' : 'STORM'  'instance_name' : 'SLIDER_1'  'typeComponents' : [ { 'id' : 'NIMBUS'  'name' : 'NIMBUS'  'category' : 'MASTER'  'displayName' : 'NIMBUS'  'priority' : 1  'instanceCount' : 0  'maxInstanceCount' : 0  'yarnMemory' : 1024  'yarnCpuCores' : 1 }  { 'id' : 'STORM_REST_API'  'name' : 'STORM_REST_API'  'category' : 'MASTER'  'displayName' : 'STORM_REST_API'  'priority' : 2  'instanceCount' : 0  'maxInstanceCount' : 0  'yarnMemory' : 1024  'yarnCpuCores' : 1 }  { 'id' : 'SUPERVISOR'  'name' : 'SUPERVISOR'  'category' : 'SLAVE'  'displayName' : 'SUPERVISOR'  'priority' : 5  'instanceCount' : 0  'maxInstanceCount' : 0  'yarnMemory' : 1024  'yarnCpuCores' : 1 }  { 'id' : 'STORM_UI_SERVER'  'name' : 'STORM_UI_SERVER'  'category' : 'MASTER'  'displayName' : 'STORM_UI_SERVER'  'priority' : 3  'instanceCount' : 0  'maxInstanceCount' : 0  'yarnMemory' : 1024  'yarnCpuCores' : 1 }  { 'id' : 'DRPC_SERVER'  'name' : 'DRPC_SERVER'  'category' : 'MASTER'  'displayName' : 'DRPC_SERVER'  'priority' : 4  'instanceCount' : 0  'maxInstanceCount' : 0  'yarnMemory' : 1024  'yarnCpuCores' : 1 } ]  'typeDescription' : 'Apache Hadoop Stream processing framework'  'typeName' : 'STORM'  'typePackageFileName' : 'storm_v091.zip'  'typeVersion' : '0.9.1.2.1'  'version' : '1.0.0'  'view_name' : 'SLIDER'  'typeConfigs' : { 'agent.conf' : '/slider/agent/conf/agent.ini'  'application.def' : '/slider/storm_v091.zip'  'config_types' : 'storm-site'  'java_home' : '/usr/jdk64/jdk1.7.0_45'  'package_list' : 'files/apache-storm-0.9.1.2.1.1.0-237.tar.gz'  'site.global.app_root' : '${AGENT_WORK_ROOT}/app/install/apache-storm-0.9.1.2.1.1.0-237'  'site.global.app_user' : 'yarn'  'site.global.ganglia_server_host' : '${NN_HOST}'  'site.global.ganglia_server_id' : 'Application2'  'site.global.rest_api_admin_port' : '${STORM_REST_API.ALLOCATED_PORT}'  'site.global.rest_api_port' : '${STORM_REST_API.ALLOCATED_PORT}'  'site.global.security_enabled' : 'false'  'site.global.user_group' : 'hadoop'  'site.storm-site.dev.zookeeper.path' : '${AGENT_WORK_ROOT}/app/tmp/dev-storm-zookeeper'  'site.storm-site.drpc.childopts' : '-Xmx768m'  'site.storm-site.drpc.invocations.port' : '${DRPC_SERVER.ALLOCATED_PORT}'  'site.storm-site.drpc.port' : '${DRPC_SERVER.ALLOCATED_PORT}'  'site.storm-site.drpc.queue.size' : '128'  'site.storm-site.drpc.request.timeout.secs' : '600'  'site.storm-site.drpc.worker.threads' : '64'  'site.storm-site.java.library.path' : '/usr/local/lib:/opt/local/lib:/usr/lib'  'site.storm-site.logviewer.appender.name' : 'A1'  'site.storm-site.logviewer.childopts' : '-Xmx128m'  'site.storm-site.logviewer.port' : '${SUPERVISOR.ALLOCATED_PORT}'  'site.storm-site.nimbus.childopts' : '-Xmx1024m -Djava.security.auth.login.config=/etc/storm/storm_jaas.conf -javaagent:${AGENT_WORK_ROOT}/app/install/apache-storm-0.9.1.2.1.1.0-237/contrib/storm-jmxetric/lib/jmxetric-1.0.4.jar=host={0} port=8669 wireformat31x=true mode=multicast config=${AGENT_WORK_ROOT}/app/install/apache-storm-0.9.1.2.1.1.0-237/contrib/storm-jmxetric/conf/jmxetric-conf.xml process=Nimbus_JVM'  'site.storm-site.nimbus.cleanup.inbox.freq.secs' : '600'  'site.storm-site.nimbus.file.copy.expiration.secs' : '600'  'site.storm-site.nimbus.host' : '${NIMBUS_HOST}'  'site.storm-site.nimbus.inbox.jar.expiration.secs' : '3600'  'site.storm-site.nimbus.monitor.freq.secs' : '10'  'site.storm-site.nimbus.reassign' : 'true'  'site.storm-site.nimbus.supervisor.timeout.secs' : '60'  'site.storm-site.nimbus.task.launch.secs' : '120'  'site.storm-site.nimbus.task.timeout.secs' : '30'  'site.storm-site.nimbus.thrift.max_buffer_size' : '1048576'  'site.storm-site.nimbus.thrift.port' : '${NIMBUS.ALLOCATED_PORT}'  'site.storm-site.nimbus.topology.validator' : 'backtype.storm.nimbus.DefaultTopologyValidator'  'site.storm-site.storm.cluster.mode' : 'distributed'  'site.storm-site.storm.local.dir' : '${AGENT_WORK_ROOT}/app/tmp/storm'  'site.storm-site.storm.local.mode.zmq' : 'false'  'site.storm-site.storm.messaging.netty.buffer_size' : '5242880'  'site.storm-site.storm.messaging.netty.client_worker_threads' : '1'  'site.storm-site.storm.messaging.netty.max_retries' : '30'  'site.storm-site.storm.messaging.netty.max_wait_ms' : '1000'  'site.storm-site.storm.messaging.netty.min_wait_ms' : '100'  'site.storm-site.storm.messaging.netty.server_worker_threads' : '1'  'site.storm-site.storm.messaging.transport' : 'backtype.storm.messaging.netty.Context'  'site.storm-site.storm.thrift.transport' : 'backtype.storm.security.auth.SimpleTransportPlugin'  'site.storm-site.storm.zookeeper.connection.timeout' : '15000'  'site.storm-site.storm.zookeeper.port' : '2181'  'site.storm-site.storm.zookeeper.retry.interval' : '1000'  'site.storm-site.storm.zookeeper.retry.intervalceiling.millis' : '30000'  'site.storm-site.storm.zookeeper.retry.times' : '5'  'site.storm-site.storm.zookeeper.root' : '/storm'  'site.storm-site.storm.zookeeper.servers' : '['${ZK_HOST}']'  'site.storm-site.storm.zookeeper.session.timeout' : '20000'  'site.storm-site.supervisor.childopts' : '-Xmx256m -Djava.security.auth.login.config=/etc/storm/storm_jaas.conf -Dcom.sun.management.jmxremote -Dcom.sun.management.jmxremote.ssl=false -Dcom.sun.management.jmxremote.authenticate=false -Dcom.sun.management.jmxremote.port=${SUPERVISOR.ALLOCATED_PORT} -javaagent:${AGENT_WORK_ROOT}/app/install/apache-storm-0.9.1.2.1.1.0-237/contrib/storm-jmxetric/lib/jmxetric-1.0.4.jar=host={0} port=8669 wireformat31x=true mode=multicast config=${AGENT_WORK_ROOT}/app/install/apache-storm-0.9.1.2.1.1.0-237/contrib/storm-jmxetric/conf/jmxetric-conf.xml process=Supervisor_JVM'  'site.storm-site.supervisor.enable' : 'true'  'site.storm-site.supervisor.heartbeat.frequency.secs' : '5'  'site.storm-site.supervisor.monitor.frequency.secs' : '3'  'site.storm-site.supervisor.slots.ports' : '[${SUPERVISOR.ALLOCATED_PORT}  ${SUPERVISOR.ALLOCATED_PORT}]'  'site.storm-site.supervisor.worker.start.timeout.secs' : '120'  'site.storm-site.supervisor.worker.timeout.secs' : '30'  'site.storm-site.task.heartbeat.frequency.secs' : '3'  'site.storm-site.task.refresh.poll.secs' : '10'  'site.storm-site.topology.acker.executors' : 'null'  'site.storm-site.topology.builtin.metrics.bucket.size.secs' : '60'  'site.storm-site.topology.debug' : 'false'  'site.storm-site.topology.disruptor.wait.strategy' : 'com.lmax.disruptor.BlockingWaitStrategy'  'site.storm-site.topology.enable.message.timeouts' : 'true'  'site.storm-site.topology.error.throttle.interval.secs' : '10'  'site.storm-site.topology.executor.receive.buffer.size' : '1024'  'site.storm-site.topology.executor.send.buffer.size' : '1024'  'site.storm-site.topology.fall.back.on.java.serialization' : 'true'  'site.storm-site.topology.kryo.factory' : 'backtype.storm.serialization.DefaultKryoFactory'  'site.storm-site.topology.max.error.report.per.interval' : '5'  'site.storm-site.topology.max.spout.pending' : 'null'  'site.storm-site.topology.max.task.parallelism' : 'null'  'site.storm-site.topology.message.timeout.secs' : '30'  'site.storm-site.topology.optimize' : 'true'  'site.storm-site.topology.receiver.buffer.size' : '8'  'site.storm-site.topology.skip.missing.kryo.registrations' : 'false'  'site.storm-site.topology.sleep.spout.wait.strategy.time.ms' : '1'  'site.storm-site.topology.spout.wait.strategy' : 'backtype.storm.spout.SleepSpoutWaitStrategy'  'site.storm-site.topology.state.synchronization.timeout.secs' : '60'  'site.storm-site.topology.stats.sample.rate' : '0.05'  'site.storm-site.topology.tick.tuple.freq.secs' : 'null'  'site.storm-site.topology.transfer.buffer.size' : '1024'  'site.storm-site.topology.trident.batch.emit.interval.millis' : '500'  'site.storm-site.topology.tuple.serializer' : 'backtype.storm.serialization.types.ListDelegateSerializer'  'site.storm-site.topology.worker.childopts' : 'null'  'site.storm-site.topology.worker.shared.thread.pool.size' : '4'  'site.storm-site.topology.workers' : '1'  'site.storm-site.transactional.zookeeper.port' : 'null'  'site.storm-site.transactional.zookeeper.root' : '/transactional'  'site.storm-site.transactional.zookeeper.servers' : 'null'  'site.storm-site.ui.port' : '${STORM_UI_SERVER.ALLOCATED_PORT}'  'site.storm-site.worker.childopts' : '-Xmx768m -javaagent:${AGENT_WORK_ROOT}/app/install/apache-storm-0.9.1.2.1.1.0-237/contrib/storm-jmxetric/lib/jmxetric-1.0.4.jar=host={0} port=8669 wireformat31x=true mode=multicast config=${AGENT_WORK_ROOT}/app/install/apache-storm-0.9.1.2.1.1.0-237/contrib/storm-jmxetric/conf/jmxetric-conf.xml process=Worker_%ID%_JVM'  'site.storm-site.worker.heartbeat.frequency.secs' : '1'  'site.storm-site.zmq.hwm' : '0'  'site.storm-site.zmq.linger.millis' : '5000'  'site.storm-site.zmq.threads' : '1' } } ]}, current new slider app wizard show hardcod app type we instead show app type return http c api v view slider version instanc slider apptyp field endpoint href http c api v view slider version instanc slider apptyp field item href http c api v view slider version instanc slider apptyp accumulo id accumulo instanc name slider type compon id accumulo master name accumulo master categori master display name accumulo master prioriti instanc count max instanc count yarn memori yarn cpu core id accumulo monitor name accumulo monitor categori master display name accumulo monitor prioriti instanc count max instanc count yarn memori yarn cpu core id accumulo gc name accumulo gc categori master display name accumulo gc prioriti instanc count max instanc count yarn memori yarn cpu core id accumulo tracer name accumulo tracer categori master display name accumulo tracer prioriti instanc count max instanc count yarn memori yarn cpu core id accumulo tserver name accumulo tserver categori slave display name accumulo tserver prioriti instanc count max instanc count yarn memori yarn cpu core id accumulo client name accumulo client categori client display name accumulo client prioriti instanc count max instanc count yarn memori yarn cpu core type descript the apach accumulo sort distribut key valu store robust n scalabl high perform data storag system featur cell base n access control customiz server side process it base n googl big tabl design built top apach hadoop n zookeep thrift n requir n ensur parent dir path accumulo site instanc df dir access app owner type name accumulo type packag file name accumulo v zip type version version view name slider type config agent conf slider agent conf agent ini applic def slider accumulo v zip config type accumulo site java home usr jdk jdk packag list file accumulo bin tar gz site accumulo site gc port client site accumulo site gener classpath accumulo home lib accumulo server jar n accumulo home lib accumulo core jar n accumulo home lib accumulo start jar n accumulo home lib accumulo fate jar n accumulo home lib accumulo proxi jar n accumulo home lib jar n zookeep home zookeep jar n hadoop conf dir n hadoop prefix jar n hadoop prefix lib jar n hadoop prefix share hadoop common jar n hadoop prefix share hadoop common lib jar n hadoop prefix share hadoop hdf jar n hadoop prefix share hadoop mapreduc jar n hadoop prefix share hadoop yarn jar n usr lib hadoop jar n usr lib hadoop lib jar n usr lib hadoop hdf jar n usr lib hadoop mapreduc jar n usr lib hadoop yarn jar site accumulo site instanc df dir app accumulo data site accumulo site instanc secret default site accumulo site instanc zookeep host zk host site accumulo site master port client site accumulo site monitor port client accumulo monitor alloc port site accumulo site monitor port log j site accumulo site trace port client site accumulo site trace token properti password secret site accumulo site trace user root site accumulo site tserver cach data size m site accumulo site tserver cach index size m site accumulo site tserver memori map max m site accumulo site tserver port client site accumulo site tserver sort buffer size m site accumulo site tserver walog max size m site global accumulo instanc name instancenam site global accumulo root password secret site global app instal dir agent work root app instal site global app log dir agent log root app log site global app pid dir agent work root app run site global app root agent work root app instal accumulo site global app user yarn site global gc heapsiz site global hadoop conf dir etc hadoop conf site global hadoop prefix usr lib hadoop site global master heapsiz site global monitor heapsiz site global heapsiz site global secur enabl fals site global tserver heapsiz site global user group hadoop site global zookeep home usr lib zookeep href http c api v view slider version instanc slider apptyp hbase id hbase instanc name slider type compon id hbase master name hbase master categori master display name hbase master prioriti instanc count max instanc count yarn memori yarn cpu core id hbase regionserv name hbase regionserv categori slave display name hbase regionserv prioriti instanc count max instanc count yarn memori yarn cpu core id hbase client name hbase client categori client display name hbase client prioriti instanc count max instanc count yarn memori yarn cpu core type descript apach h base hadoop databas distribut scalabl big data store n requir n ensur parent dir path hbase site hbase rootdir access app owner n ensur zk root hbase site zookeep znode parent uniqu app instanc type name hbase type packag file name hbase v zip type version version view name slider type config agent conf slider agent conf agent ini applic def slider hbase v zip config type core site hdf site hbase site java home usr jdk jdk packag list file hbase hadoop bin tar gz site core site fs default fs nn uri site global app instal dir agent work root app instal site global app log dir agent log root app log site global app pid dir agent work root app run site global app root agent work root app instal hbase hadoop site global app user yarn site global ganglia server host nn host site global ganglia server id applic site global ganglia server port site global hbase master heapsiz site global hbase regionserv heapsiz site global secur enabl fals site global user group hadoop site hbase site hbase client keyvalu maxsiz site hbase site hbase client scanner cach site hbase site hbase cluster distribut true site hbase site hbase default version skip true site hbase site hbase hregion majorcompact site hbase site hbase hregion max files site hbase site hbase hregion memstor block multipli site hbase site hbase hregion memstor flush size site hbase site hbase hregion memstor mslab enabl true site hbase site hbase hstore block store file site hbase site hbase hstore compact threshold site hbase site hbase hstore flush retri number site hbase site hbase local dir hbase tmp dir local site hbase site hbase master info port hbase master alloc port site hbase site hbase regionserv global memstor lower limit site hbase site hbase regionserv global memstor upper limit site hbase site hbase regionserv handler count site hbase site hbase regionserv info port site hbase site hbase regionserv port site hbase site hbase rootdir nn uri app hbase data site hbase site hbase secur authent simpl site hbase site hbase secur author fals site hbase site hbase stagingdir nn uri app hbase stage site hbase site hbase superus yarn site hbase site hbase tmp dir agent work root work app tmp site hbase site hbase zookeep properti client port site hbase site hbase zookeep quorum zk host site hbase site hbase zookeep use multi true site hbase site hfile block cach size site hbase site zookeep session timeout site hbase site zookeep znode parent hbase unsecur site hdf site df namenod http address nn host site hdf site df namenod http address nn host href http c api v view slider version instanc slider apptyp storm id storm instanc name slider type compon id nimbu name nimbu categori master display name nimbu prioriti instanc count max instanc count yarn memori yarn cpu core id storm rest api name storm rest api categori master display name storm rest api prioriti instanc count max instanc count yarn memori yarn cpu core id supervisor name supervisor categori slave display name supervisor prioriti instanc count max instanc count yarn memori yarn cpu core id storm ui server name storm ui server categori master display name storm ui server prioriti instanc count max instanc count yarn memori yarn cpu core id drpc server name drpc server categori master display name drpc server prioriti instanc count max instanc count yarn memori yarn cpu core type descript apach hadoop stream process framework type name storm type packag file name storm v zip type version version view name slider type config agent conf slider agent conf agent ini applic def slider storm v zip config type storm site java home usr jdk jdk packag list file apach storm tar gz site global app root agent work root app instal apach storm site global app user yarn site global ganglia server host nn host site global ganglia server id applic site global rest api admin port storm rest api alloc port site global rest api port storm rest api alloc port site global secur enabl fals site global user group hadoop site storm site dev zookeep path agent work root app tmp dev storm zookeep site storm site drpc childopt xmx site storm site drpc invoc port drpc server alloc port site storm site drpc port drpc server alloc port site storm site drpc queue size site storm site drpc request timeout sec site storm site drpc worker thread site storm site java librari path usr local lib opt local lib usr lib site storm site logview append name a site storm site logview childopt xmx site storm site logview port supervisor alloc port site storm site nimbu childopt xmx djava secur auth login config etc storm storm jaa conf javaag agent work root app instal apach storm contrib storm jmxetric lib jmxetric jar host port wireformat x true mode multicast config agent work root app instal apach storm contrib storm jmxetric conf jmxetric conf xml process nimbu jvm site storm site nimbu cleanup inbox freq sec site storm site nimbu file copi expir sec site storm site nimbu host nimbu host site storm site nimbu inbox jar expir sec site storm site nimbu monitor freq sec site storm site nimbu reassign true site storm site nimbu supervisor timeout sec site storm site nimbu task launch sec site storm site nimbu task timeout sec site storm site nimbu thrift max buffer size site storm site nimbu thrift port nimbu alloc port site storm site nimbu topolog valid backtyp storm nimbu default topolog valid site storm site storm cluster mode distribut site storm site storm local dir agent work root app tmp storm site storm site storm local mode zmq fals site storm site storm messag netti buffer size site storm site storm messag netti client worker thread site storm site storm messag netti max retri site storm site storm messag netti max wait ms site storm site storm messag netti min wait ms site storm site storm messag netti server worker thread site storm site storm messag transport backtyp storm messag netti context site storm site storm thrift transport backtyp storm secur auth simpl transport plugin site storm site storm zookeep connect timeout site storm site storm zookeep port site storm site storm zookeep retri interv site storm site storm zookeep retri intervalceil milli site storm site storm zookeep retri time site storm site storm zookeep root storm site storm site storm zookeep server zk host site storm site storm zookeep session timeout site storm site supervisor childopt xmx djava secur auth login config etc storm storm jaa conf dcom sun manag jmxremot dcom sun manag jmxremot ssl fals dcom sun manag jmxremot authent fals dcom sun manag jmxremot port supervisor alloc port javaag agent work root app instal apach storm contrib storm jmxetric lib jmxetric jar host port wireformat x true mode multicast config agent work root app instal apach storm contrib storm jmxetric conf jmxetric conf xml process supervisor jvm site storm site supervisor enabl true site storm site supervisor heartbeat frequenc sec site storm site supervisor monitor frequenc sec site storm site supervisor slot port supervisor alloc port supervisor alloc port site storm site supervisor worker start timeout sec site storm site supervisor worker timeout sec site storm site task heartbeat frequenc sec site storm site task refresh poll sec site storm site topolog acker executor null site storm site topolog builtin metric bucket size sec site storm site topolog debug fals site storm site topolog disruptor wait strategi com lmax disruptor block wait strategi site storm site topolog enabl messag timeout true site storm site topolog error throttl interv sec site storm site topolog executor receiv buffer size site storm site topolog executor send buffer size site storm site topolog fall back java serial true site storm site topolog kryo factori backtyp storm serial default kryo factori site storm site topolog max error report per interv site storm site topolog max spout pend null site storm site topolog max task parallel null site storm site topolog messag timeout sec site storm site topolog optim true site storm site topolog receiv buffer size site storm site topolog skip miss kryo registr fals site storm site topolog sleep spout wait strategi time ms site storm site topolog spout wait strategi backtyp storm spout sleep spout wait strategi site storm site topolog state synchron timeout sec site storm site topolog stat sampl rate site storm site topolog tick tupl freq sec null site storm site topolog transfer buffer size site storm site topolog trident batch emit interv milli site storm site topolog tupl serial backtyp storm serial type list deleg serial site storm site topolog worker childopt null site storm site topolog worker share thread pool size site storm site topolog worker site storm site transact zookeep port null site storm site transact zookeep root transact site storm site transact zookeep server null site storm site ui port storm ui server alloc port site storm site worker childopt xmx javaag agent work root app instal apach storm contrib storm jmxetric lib jmxetric jar host port wireformat x true mode multicast config agent work root app instal apach storm contrib storm jmxetric conf jmxetric conf xml process worker id jvm site storm site worker heartbeat frequenc sec site storm site zmq hwm site storm site zmq linger milli site storm site zmq thread,1,0,0,0,0,0,
5895,Suppress any debug and info messages from package managers in setupAgent.py, suppress debug info messag packag manag setup agent py,During bootstrapping of agents  installation retrieve versions of available packages from system package manager. After some test was founded that in different situations package managers can produce specific info/debug output which can be reason of wrong parsing of output.Example:Zypper command:se2mon1400652583-9:/etc/zypp/repos.d # zypper search -s --match-exact ambari-agent Building repository 'Hortonworks Data Platform Utils Version - HDP-UTILS-1.1.0.16' cache &#91;done&#93;Building repository 'Hortonworks Data Platform Utils Version - HDP-UTILS-1.1.0.17' cache &#91;done&#93;Building repository 'Hosted (SLES_11)' cache &#91;done&#93;Building repository 'ambari-1.6.0 - Updates' cache &#91;done&#93;Building repository 'Ambari 1.x' cache &#91;done&#93;Building repository 'PostgreSQL and related packages (SLE_11_SP3)' cache &#91;done&#93;Loading repository data...Reading installed packages...S | Name | Type | Version | Arch | Repository -----------------------------------+---------------------- ambari-agent  package  1.6.0-46  x86_64  ambari-1.6.0 - Updates ambari-agent  package  1.2.0.1-1  x86_64  Ambari 1.xParsed as: Building repository 'ambari-1.6.0 We should suppress any debug/info output in setupAgent.py to avoid any unexpected situation., dure bootstrap agent instal retriev version avail packag system packag manag after test found differ situat packag manag produc specif info debug output reason wrong pars output exampl zypper command se mon etc zypp repo zypper search match exact ambari agent build repositori hortonwork data platform util version hdp util cach done build repositori hortonwork data platform util version hdp util cach done build repositori host sle cach done build repositori ambari updat cach done build repositori ambari x cach done build repositori postgr sql relat packag sle sp cach done load repositori data read instal packag s name type version arch repositori ambari agent packag x ambari updat ambari agent packag x ambari x pars build repositori ambari we suppress debug info output setup agent py avoid unexpect situat,0,0,0,0,0,0,
5915,View: Files UI clean-up and adjustments (PART 2), view file ui clean adjust part,As a part of this ticket 1) Remove rename icon from infront of the file and make it a different column.2) reorder the icon bar in the sequence Download  Move  Rename and Delete, as part ticket remov renam icon infront file make differ column reorder icon bar sequenc download move renam delet,1,0,0,0,0,0,
5922,Predicates don't work on fields with float values, predic work field float valu,API should be able to process filter with predicates(&lt; &gt; =) for fields with float values; For example field: metrics/load/load_one.Currently the Greater than predicate will fail for values in between 0.0 and 1.0, api abl process filter predic lt gt field float valu for exampl field metric load load one current greater predic fail valu,1,0,0,0,0,0,
5930,Some HBase properties are empty  but required to be filled, some h base properti empti requir fill,This HBase properties are empty after install via blueprint: hbase.coprocessor.region.classes hbase.coprocessor.master.classesBut they required to be filled.After enabling security they became filled., thi h base properti empti instal via blueprint hbase coprocessor region class hbase coprocessor master class but requir fill after enabl secur becam fill,0,0,0,0,0,0,
5932,Slider apps table does not remove entry when app is removed, slider app tabl remov entri app remov,Lets say I have a running app. I freeze it and then destroy it. The app goes away from the /apps response. However the UI still continues to show it. I think the mapper is not removing deleted entries from the model.Mapper should remove entries not being sent by /apps., let say i run app i freez destroy the app goe away app respons howev ui still continu show i think mapper remov delet entri model mapper remov entri sent app,1,0,0,0,0,0,
5935,Maintenance state and status commands perf improvements., mainten state statu command perf improv,getEffectiveState should not fetch map of all hosts every time Status commands should not be sent until component is installed, get effect state fetch map host everi time statu command sent compon instal,1,0,0,0,0,0,
5941,Ambari should generate config files in sorted order, ambari gener config file sort order,Ambari should generate config files sorted by key. It would make doing compares between files much easier., ambari gener config file sort key it would make compar file much easier,1,0,0,0,0,0,
5956,DB connection check error if jdk_name does not exist., db connect check error jdk name exist,We can get such situation when user using custom java., we get situat user use custom java,1,0,0,0,0,0,
5957,Bootstrap API call says bootstrap is running even though all agents have installed and registered, bootstrap api call say bootstrap run even though agent instal regist,UI keeps showing that the agents are being installed  because the bootstrap API GET call keeps returning that hostsStatus/status is RUNNINGI suspected time.sleep(1) instruction. If restart takes too long time  situation  when ambari-agent.log has not been created yet is possible. So  tail command returned not 0 retcode and exit from procedure. But in logs I cant see any non-zero retcodes. Thus  too short logs could be explaned by this issue., ui keep show agent instal bootstrap api get call keep return host statu statu runningi suspect time sleep instruct if restart take long time situat ambari agent log creat yet possibl so tail command return retcod exit procedur but log i cant see non zero retcod thu short log could explan issu,0,0,0,0,0,0,
5960,Add support for auth proxy, add support auth proxi,When using a proxy for internet access  add support to set properties for proxy user and password.??Dhttp.proxyUser=someUserName -Dhttp.proxyPassword=somePassword??, when use proxi internet access add support set properti proxi user password dhttp proxi user user name dhttp proxi password password,1,0,0,0,0,0,
5994,Storm UI in Ambari quick link fails when Storm UI server is not co-hosted with nimbus host, storm ui ambari quick link fail storm ui server co host nimbu host,quick link for Storm UI was linked with nimbus server. it should use Storm UI server host., quick link storm ui link nimbu server use storm ui server host,1,0,0,0,0,0,
6003,metrics hostname is not correct for storm, metric hostnam correct storm,Some of the storm configs have extra 'localhost' appended to the host name.'nimbus.childopts' : '-Xmx1024m -Djava.security.auth.login.config=/etc/storm/conf/storm_jaas.conf -javaagent:/usr/lib/storm/contrib/storm-jmxetric/lib/jmxetric-1.0.4.jar=host=c6401.ambari.apache.orglocalhost port=8649 wireformat31x=true mode=multicast config=/usr/lib/storm/contrib/storm-jmxetric/conf/jmxetric-conf.xml process=Nimbus_JVM''supervisor.childopts' : '-Xmx256m -Djava.security.auth.login.config=/etc/storm/conf/storm_jaas.conf -Dcom.sun.management.jmxremote -Dcom.sun.management.jmxremote.ssl=false -Dcom.sun.management.jmxremote.authenticate=false -Dcom.sun.management.jmxremote.port=56431 -javaagent:/usr/lib/storm/contrib/storm-jmxetric/lib/jmxetric-1.0.4.jar=host=c6401.ambari.apache.orglocalhost port=8650 wireformat31x=true mode=multicast config=/usr/lib/storm/contrib/storm-jmxetric/conf/jmxetric-conf.xml process=Supervisor_JVM''worker.childopts' : '-Xmx768m -javaagent:/usr/lib/storm/contrib/storm-jmxetric/lib/jmxetric-1.0.4.jar=host=c6401.ambari.apache.orglocalhost port=8650 wireformat31x=true mode=multicast config=/usr/lib/storm/contrib/storm-jmxetric/conf/jmxetric-conf.xml process=Worker_%ID%_JVM', some storm config extra localhost append host name nimbu childopt xmx djava secur auth login config etc storm conf storm jaa conf javaag usr lib storm contrib storm jmxetric lib jmxetric jar host c ambari apach orglocalhost port wireformat x true mode multicast config usr lib storm contrib storm jmxetric conf jmxetric conf xml process nimbu jvm supervisor childopt xmx djava secur auth login config etc storm conf storm jaa conf dcom sun manag jmxremot dcom sun manag jmxremot ssl fals dcom sun manag jmxremot authent fals dcom sun manag jmxremot port javaag usr lib storm contrib storm jmxetric lib jmxetric jar host c ambari apach orglocalhost port wireformat x true mode multicast config usr lib storm contrib storm jmxetric conf jmxetric conf xml process supervisor jvm worker childopt xmx javaag usr lib storm contrib storm jmxetric lib jmxetric jar host c ambari apach orglocalhost port wireformat x true mode multicast config usr lib storm contrib storm jmxetric conf jmxetric conf xml process worker id jvm,0,0,0,0,0,0,
6014,HDFS alert hangs for a long time after enabling Maintenance mode, hdf alert hang long time enabl mainten mode,STR: Stop HDFS with started Nagios service. Turn on Maintenance mode for HDFS.Result: All alerts dissapear  except 'HDFS capacity utilization' - it hangs for a long time (in my case it was smthng about 10-11 minutes), str stop hdf start nagio servic turn mainten mode hdf result all alert dissapear except hdf capac util hang long time case smthng minut,1,0,0,0,1,0,
6034,Services -> Configs page for Yarn  HIVE  MapReduce services is not displayed, servic config page yarn hive map reduc servic display,Config page for services that use App.YARNDefaultsProvider is not displayed.Following JS error thrown:Uncaught TypeError: Cannot call method 'forEach' of null yarn_defaults_provider.js:291, config page servic use app yarn default provid display follow js error thrown uncaught type error cannot call method each null yarn default provid js,0,0,0,0,0,0,
6044,Issues with jdbc properties, issu jdbc properti,remove dots in --help params description and in warningscopy fails if jdbc selected by --jdbc-driver is already in resourceshide other points of ambari-server setup if jdbc options are passed and server is running, remov dot help param descript warningscopi fail jdbc select jdbc driver alreadi resourceshid point ambari server setup jdbc option pass server run,0,0,0,0,0,0,
6045,Master components are missing, master compon miss,Master components info is missing in service summary., master compon info miss servic summari,1,0,0,0,0,0,
6048,Ambari Agent script should check for running processes before starting, ambari agent script check run process start,PROBLEM: If the Ambari Agent installation fails for whatever reason then a process of ambari-agent is left running. This results in the ambari-agent status to show as it not running. If you then start another ambari-agent it dies because the port is already in use.If the script could check the PID file and check for a running process then it would resolve this issue.BUSINESS IMPACT: Not a huge business impact as the workaround is to kill the running ambari-agent processWorkaround: Kill running ambari-agent process before startingANALYSIS: I cannot reproduce this issue in house and the SE who raised it can not reproduce on demand., problem if ambari agent instal fail whatev reason process ambari agent left run thi result ambari agent statu show run if start anoth ambari agent die port alreadi use if script could check pid file check run process would resolv issu busi impact not huge busi impact workaround kill run ambari agent process workaround kill run ambari agent process start analysi i cannot reproduc issu hous se rais reproduc demand,1,0,0,0,1,0,
6053,Add Host wizard get stuck on Confirm Hosts step, add host wizard get stuck confirm host step,,,1,0,0,0,0,0,
6056,Agent Custom Command Output Coerces Integers to Floats, agent custom command output coerc integ float,When posting a command such as{ 'RequestInfo': { 'action': 'check_host'  'context': 'Check host'  'parameters': { 'check_execute_list': 'host_resolution_check'  'hosts': 'c6401.ambari.apache.org  c6402.ambari.apache.org  c6403.ambari.apache.org  foobar'  'threshold': '20' } }  'Requests/resource_filters': [ { 'hosts': 'c6401.ambari.apache.org c6402.ambari.apache.org' } ]The returned result from the custom action has some integer values coerced into floats: 'structured_out' : { 'host_resolution_check' : { 'exit_code' : '0'  'failed_count' : 0.0  'failures' : [ ]  'message' : 'All hosts resolved to an IP address.'  'success_count' : 4.0 }The structured_output written out to disk does NOT have the float values:{'host_resolution_check': {'failures': []  'message': 'All hosts resolved to an IP address.'  'failed_count': 0  'success_count': 4  'exit_code': '0'}} Therefore this is a problem with the framework and not the command., when post command request info action check host context check host paramet check execut list host resolut check host c ambari apach org c ambari apach org c ambari apach org foobar threshold request resourc filter host c ambari apach org c ambari apach org the return result custom action integ valu coerc float structur host resolut check exit code fail count failur messag all host resolv ip address success count the structur output written disk not float valu host resolut check failur messag all host resolv ip address fail count success count exit code therefor problem framework command,1,0,0,0,0,0,
6059,Add refreshQueues custom command to YARN service, add refresh queue custom command yarn servic,Call refreshqueues from the REST API (replace resource.manager.host and YourClusterName). And be sure to include header 'X-Requested-By' : 'ambari' and set authentication.POST/api/v1/clusters/YourClusterName/requests/{ 'RequestInfo' : { 'command' : 'REFRESHQUEUES'  'context' : 'Refresh YARN Capacity Scheduler' }  'Requests/resource_filters': [{ 'service_name' : 'YARN'  'component_name' : 'RESOURCEMANAGER'  'hosts' : 'resource.manager.host' }]}, call refreshqueu rest api replac resourc manag host your cluster name and sure includ header x request by ambari set authent post api v cluster your cluster name request request info command refreshqueu context refresh yarn capac schedul request resourc filter servic name yarn compon name resourcemanag host resourc manag host,0,0,0,0,0,0,
6063,'ambari-server start command' hangs if was executed via ssh command, ambari server start command hang execut via ssh command,Problem:Ambari Server command 'ssh root@vmhost ambari-server start' hangs on message 'Ambari Server 'start' completed successfully.'. Same command executed successfully on the local console  as a result this behavior can be reproduced only via remote execution of commands.How to reproduce: Deploy server Stop server locally Start server from another host using ssh command, problem ambari server command ssh root vmhost ambari server start hang messag ambari server start complet success same command execut success local consol result behavior reproduc via remot execut command how reproduc deploy server stop server local start server anoth host use ssh command,1,0,0,0,0,0,
6080,Hosts Components filter on Service summary page doesn't work, host compon filter servic summari page work,When user clicks on some component filter on Service summary page  it opens hosts page  but filter is not applied., when user click compon filter servic summari page open host page filter appli,1,0,0,0,0,0,
6087,Multiple ATS appear on YARN summary page, multipl at appear yarn summari page,STR1. Go to add Service Wizard.2. Select some new services and proceed to deploy.3. Close Wizard (Esc-button).4. Wait a little bit (maybe page-refresh needed).5. Go to YARN summary.6. New 'none' components will appear periodically.See screenshot., str go add servic wizard select new servic proceed deploy close wizard esc button wait littl bit mayb page refresh need go yarn summari new none compon appear period see screenshot,1,0,0,0,0,0,
6106,Customize the Hadoop metrics sink to write to MySQL store, custom hadoop metric sink write my sql store,The SqlServerSink should support pushing metrics to MySQL store.This Jira addresses changes needed to support sink to a MySQL store., the sql server sink support push metric my sql store thi jira address chang need support sink my sql store,1,0,0,0,0,0,
6112,Filter by alerts fails on Hosts table, filter alert fail host tabl,Steps to reproduce:1. Go to Hosts page2. Choose filter AlertsResult:Hosts are not filtered by alerts.The request with filter by alerts has incorrect url data., step reproduc go host page choos filter alert result host filter alert the request filter alert incorrect url data,1,0,0,0,0,0,
6113,Nagios install fails on SLES due to php5-json not available, nagio instal fail sle due php json avail,Using SLES 11 SP3 quick-start image on EC2. Doesn't look like php5-json is available  but php53-json is available.WORKAROUND:I modified NAGIOS/metainfo.xml and this worked. &lt;package&gt; &lt;name&gt;php5*-json&lt;/name&gt; &lt;/package&gt;, use sle sp quick start imag ec doesn look like php json avail php json avail workaround i modifi nagio metainfo xml work lt packag gt lt name gt php json lt name gt lt packag gt,1,0,0,0,0,0,
6123,issues with dialog keypresses, issu dialog keypress,1) on 'manage config groups' pressing return does something even though return is not valid (i.e. can't save)2) Once you press return in #1  then you have to press esc twice to close the dialog3) once you open the nested dialog (to add a group)  esc closes the parent  then esc again  closes the nested dialog4) once you open the nested dialog  also notice it doesn't start focus on the name field  you have to click to get that focus., manag config group press return someth even though return valid e save onc press return press esc twice close dialog open nest dialog add group esc close parent esc close nest dialog open nest dialog also notic start focu name field click get focu,0,0,0,0,0,0,
6125,Ambari Groovy client enhancements, ambari groovi client enhanc,Ambari REST client (Groovy) enhancements -refactor - simplified unit tests taking leverage groovy's metaclass capabilities-more REST calls implemented-api compatibility with version 1.6.0, ambari rest client groovi enhanc refactor simplifi unit test take leverag groovi metaclass capabl rest call implement api compat version,0,0,1,0,0,0,
6137,Bulk operations confirmation popup, bulk oper confirm popup,Hosts page  'Actions' menu (dropdown)Each action shows confirmation popup with list of affected hosts.For big cluster this may be 2000+ hosts.If there are more than 3 hosts  then show 'host1  host2  host3  and X more hosts show all', host page action menu dropdown each action show confirm popup list affect host for big cluster may host if host show host host host x host show,0,0,0,0,0,0,
6140,Step3. Hosts checks requests, step host check request,Proceed to step3  wait while registration is complete.Host checks requests are set every second.UI should set next request only when previous is completed., proce step wait registr complet host check request set everi second ui set next request previou complet,0,0,0,0,0,0,
6146,It's not possible to input 'Enter' on 'Target hosts' textarea on 2nd step of Installer wizard', it possibl input enter target host textarea nd step instal wizard,It's not possible to input 'Enter' on 'Target hosts' textarea., it possibl input enter target host textarea,0,0,0,0,0,0,
6147,Ambari Dashboard page  click NameNode link returns wrong page, ambari dashboard page click name node link return wrong page,STR:1. go to dashboard page2. click NameNode link inside HDFS Links widget.instead of go to NameNode host detail page  it returns an empty page, str go dashboard page click name node link insid hdf link widget instead go name node host detail page return empti page,0,0,0,0,0,0,
6155,JS error on POST config group request (step7 installer), js error post config group request step instal,Go to installer step7Click override for some propertySelect 'New Config Group'Click 'OK'JS-error appears - 404 error. Missing clusterName in request URL., go instal step click overrid properti select new config group click ok js error appear error miss cluster name request url,0,0,0,0,0,0,
6162,Behavior change: host filtering no longer handles startsWith matches, behavior chang host filter longer handl start with match,If I have a host that has IP 10.0.2.15  in Ambari 1.6.0 if I start to filter by IP (by typing 10.0....)  the hosts that match 'startsWith' stay displayed.In Ambari 1.6.1  now it only does exact match  so once I start typing  all hosts disappear until I finally type the whole thing in for exact match.Hosts/ip.matches(10), if i host ip ambari i start filter ip type host match start with stay display in ambari exact match i start type host disappear i final type whole thing exact match host ip match,0,0,0,0,0,0,
6169,Installer wizard: ambari web-client issues invalid requests after switching stacks, instal wizard ambari web client issu invalid request switch stack,Steps To Reproduce Select 2.X stack and go ahead to 'Select Services' page Navigate back to 'Select Stack' page and select 1.x stack Go ahead to step-8 'Review' page. On clicking next  API call to create components for HDFS service fails with UI displaying an error message.Invalid Request: Unsupported or invalid component in stack  clusterName=cc  serviceName=HDFS  componentName=JOURNALNODE  stackInfo=HDP-1.3, step to reproduc select x stack go ahead select servic page navig back select stack page select x stack go ahead step review page on click next api call creat compon hdf servic fail ui display error messag invalid request unsupport invalid compon stack cluster name cc servic name hdf compon name journalnod stack info hdp,0,0,0,0,0,0,
6184,Incorrect value for started_count of Datanode component, incorrect valu start count datanod compon,STR:  Installed a 3-node cluster for HDP 1.3 stack HDFS+MapReduce+Nagios+Ganglia+zooKeeper installed with slave components installed on all 3 hosts. Enable security with no kerberos setup On expected failure of security wizard  Disable security. After successfully disabling security  Following API returns incorrect number for started_count of Datanode. It says 0 but Datanode is actually running on all hostshttp://server:8080/api/v1/clusters/c1/components/?ServiceComponentInfo/category.in(SLAVE CLIENT)&amp;fields=ServiceComponentInfo/service_name ServiceComponentInfo/installed_count ServiceComponentInfo/started_count ServiceComponentInfo/total_count&amp;minimal_response=trueReason:During wrong kerberos setup DN processes fail to start  but leave stale pid file owned by root. Next one DN start command starts DN process  but can not override pid file. So the server considers DN as stopped. If we start DN once more  commands fail soon after start (due to lock file at data dir owned by already running DN). Agent reports to server that DN is not running  so server displays a correct information from his point of view., str instal node cluster hdp stack hdf map reduc nagio ganglia zoo keeper instal slave compon instal host enabl secur kerbero setup on expect failur secur wizard disabl secur after success disabl secur follow api return incorrect number start count datanod it say datanod actual run hostshttp server api v cluster c compon servic compon info categori slave client amp field servic compon info servic name servic compon info instal count servic compon info start count servic compon info total count amp minim respons true reason dure wrong kerbero setup dn process fail start leav stale pid file own root next one dn start command start dn process overrid pid file so server consid dn stop if start dn command fail soon start due lock file data dir own alreadi run dn agent report server dn run server display correct inform point view,0,0,0,0,0,0,
6194,Unsuitable height of dropdown menu on metrics page, unsuit height dropdown menu metric page,STR:Delete all widgets from dashboard.Go to Metrics-&gt;Add menu.Result: Appeared inappropriate dropdown: bad_metrics.png, str delet widget dashboard go metric gt add menu result appear inappropri dropdown bad metric png,0,0,0,0,0,0,
6197,Decommissioned running DataNode has 'delete' menu item in action pulldown, decommiss run data node delet menu item action pulldown,Delete operation is not allowed for a hostComponent if it is in STARTED state.Currently delete menu item is shown when a hostComponent is flagged decommissioned and in STARTED state. Performing delete operation in this condition returns API 500 server error. If the hostComponent is brought in INSTALLED state with the decommissioned flag and then delete operation is performed then it happens as expectedDelete menu item should be shown When hostComponent is in INSTALLED state and should be grayed when it is in STARTED state. ambari-web client should not consider decommission status of a hostComponent while validating the required condition to enable/disable 'delete' menu item., delet oper allow host compon start state current delet menu item shown host compon flag decommiss start state perform delet oper condit return api server error if host compon brought instal state decommiss flag delet oper perform happen expect delet menu item shown when host compon instal state gray start state ambari web client consid decommiss statu host compon valid requir condit enabl disabl delet menu item,0,0,0,0,0,0,
6199,ListBoxes with hostnames on 'Select Hosts' page of 'Enable NameNode HA Wizard' do not work, list box hostnam select host page enabl name node ha wizard work,On the second step of HA 'Enable NameNode HA Wizard' do not work listboxes with hostnames. Real additional components position does not depend from values in listboxes (see screenshots).It is possible to choose any host in any listbox  which is incorrect., on second step ha enabl name node ha wizard work listbox hostnam real addit compon posit depend valu listbox see screenshot it possibl choos host listbox incorrect,0,0,0,0,0,0,
6201,Fix sub-resource names in /stacks API, fix sub resourc name stack api,The /stacks api uses sub-resource names such as stackServices and serviceComponents instead of services and components which are the names of the resources specified in the URL. These incorrect resource names would need to be used in any queries for stack resources.For example:To get stack service named HDFS the URL would be:api/v1/stacks/HDP/versions/2.1/services/HDFSBut  if we wanted to do a query of for HDFS services across all versions:api/v1/stacks/HDP/versions?stackServices/StackServices/service_name=HDFSInstead this should be:api/v1/stacks/HDP/versions?services/StackServices/service_name=HDFSFix all sub-resource names that are returned and fix sub-resource names used in queries and partial response., the stack api use sub resourc name stack servic servic compon instead servic compon name resourc specifi url these incorrect resourc name would need use queri stack resourc for exampl to get stack servic name hdf url would api v stack hdp version servic hdf but want queri hdf servic across version api v stack hdp version stack servic stack servic servic name hdf instead api v stack hdp version servic stack servic servic name hdf fix sub resourc name return fix sub resourc name use queri partial respons,0,0,0,0,0,0,
6203,Unable to assign host in HA wizard, unabl assign host ha wizard,STR:1. Open HA wizard2. Proceed to Select Host step3. Try to select host for any master componentResult:nothing changing  js error emerge., str open ha wizard proce select host step tri select host master compon result noth chang js error emerg,0,0,0,0,0,0,
6206,BGO popup: Incorrect number of tasks in category, bgo popup incorrect number task categori,Tasks with 'aborted' status are not included in any 'Aborted' category., task abort statu includ abort categori,0,0,0,0,0,0,
6207,Installer Wizard Step 7-8: namenode_heapsize property incorrect value., instal wizard step namenod heapsiz properti incorrect valu,On the Step 7 namenode_heapsize value is empty and on deploy even if its value filled it saved as 'm' which cause error on NameNode startup.Part of object passed for configuration saving:namenode_heapsize: 'm'namenode_opt_maxnewsize: '200m'namenode_opt_newsize: '200m'nodemanager_heapsize: '1024', on step namenod heapsiz valu empti deploy even valu fill save caus error name node startup part object pass configur save namenod heapsiz namenod opt maxnews namenod opt newsiz nodemanag heapsiz,0,0,0,0,0,0,
6208,Nagios 'Restart all components' button does not work, nagio restart compon button work,On service page for Nagios  under 'Service actions' menu  'Restart all' operation does not work correctly. There is dialog window after pressing  but after confirmation there is not any activity. In API there is not any new request.Note: described situation relates only for Nagios. Other services make restart correctly (including generating new requests in API)., on servic page nagio servic action menu restart oper work correctli there dialog window press confirm activ in api new request note describ situat relat nagio other servic make restart correctli includ gener new request api,0,0,0,0,0,0,
6215,Default add host sequence triggers many unseen before cluster-wide operations, default add host sequenc trigger mani unseen cluster wide oper,I've added 1 host through the add host wizardon deploy step all hosts were present. and install operations were performed on all of them., i ad host add host wizardon deploy step host present instal oper perform,0,0,0,0,0,0,
6221,Ambari Server reset show wrong commands for DB manipulation, ambari server reset show wrong command db manipul,When performing ambari-server reset with external DB  ambari-server does nothing but outputs commands for resetting it manually.But:1. Commands are wrong  at least on Suse:su -postgres --command=psql -f /var/lib/ambari-server/resources/Ambari-DDL-Postgres-DROP.sql -v username=''ambari'' -v password=''bigdata''su: invalid option -- 'o'Try 'su --help' for more information.2. This commands should take in account that external DB can be located on the another host.3. Maybe the best option would be to give user ability to reset automatically  for example via command line switch like ambari-server reset -a, when perform ambari server reset extern db ambari server noth output command reset manual but command wrong least suse su postgr command psql f var lib ambari server resourc ambari ddl postgr drop sql v usernam ambari v password bigdata su invalid option tri su help inform thi command take account extern db locat anoth host mayb best option would give user abil reset automat exampl via command line switch like ambari server reset,0,0,0,0,0,0,
6228,host checks 'Show Report' link is missing from dialog, host check show report link miss dialog,On 'Confirm Hosts' step &gt; Hosts Check popup window  the 'Show Reports' link is missing even if the host check warnings existed.Reason:The newly added hosts checks (jdk  disk  repo and hostNameResolution) dont trigger the ''show reports' link to show up., on confirm host step gt host check popup window show report link miss even host check warn exist reason the newli ad host check jdk disk repo host name resolut dont trigger show report link show,0,0,0,0,0,0,
6234,Security issue - private key password show in logs, secur issu privat key password show log,During generating private key and certificates using openssl password of key shown in logs:11:21:30 735 INFO [main] ShellCommandUtil:44 - Command openssl genrsa -des3 -passout pass:**** -out /var/lib/ambari-server/keys/ca.key 4096 was finished with exit code: 0 - the operation was completely successfully.11:21:30 750 INFO [main] ShellCommandUtil:44 - Command openssl req -passin pass:**** -new -key /var/lib/ambari-server/keys/ca.key -out /var/lib/ambari-server/keys/ca.csr -batch was finished with exit code: 0 - the operation was completely successfully.11:21:30 766 INFO [main] ShellCommandUtil:44 - Command open**** ca -create_serial -out /var/lib/ambari-server/keys/ca.crt -days 365 -keyfile /var/lib/ambari-server/keys/ca.key -key vgGAzzSaCPkI3F7UU7qZZY6CahDUTSnY7B9a8TH0YiGDB10LdJ -selfsign -extensions jdk7_ca -config /var/lib/ambari-server/keys/ca.config -batch -infiles /var/lib/ambari-server/keys/ca.csr was finished with exit code: 0 - the operation was completely successfully.11:21:30 773 INFO [main] ShellCommandUtil:44 - Command openssl pkcs12 -export -in /var/lib/ambari-server/keys/ca.crt -inkey /var/lib/ambari-server/keys/ca.key -certfile /var/lib/ambari-server/keys/ca.crt -out /var/lib/ambari-server/keys/keystore.p12 -password pass:**** -passin pass:**** see '-key vgGAzzSaCPkI3F7UU7qZZY6CahDUTSnY7B9a8TH0YiGDB10LdJ', dure gener privat key certif use openssl password key shown log info main shell command util command openssl genrsa de passout pass var lib ambari server key ca key finish exit code oper complet success info main shell command util command openssl req passin pass new key var lib ambari server key ca key var lib ambari server key ca csr batch finish exit code oper complet success info main shell command util command open ca creat serial var lib ambari server key ca crt day keyfil var lib ambari server key ca key key vg g azz sa c pk i f uu q zzi cah dut sn y b th yi gdb ld j selfsign extens jdk ca config var lib ambari server key ca config batch infil var lib ambari server key ca csr finish exit code oper complet success info main shell command util command openssl pkc export var lib ambari server key ca crt inkey var lib ambari server key ca key certfil var lib ambari server key ca crt var lib ambari server key keystor p password pass passin pass see key vg g azz sa c pk i f uu q zzi cah dut sn y b th yi gdb ld j,0,0,0,1,0,0,
6236,Hosts page: there is no indication that filtering/sorting/paging is happening or not (confusing), host page indic filter sort page happen confus,In 1.6.1  filtering/sorting/paging on the Hosts page has been converted from client-side to server-side in order to address scalability issues.As a result of that  the responsiveness of UI is dependent upon how quickly the server can respond to filtering/sorting/paging calls (and also depends on the network). While UI is waiting for the new table content to come from the server  there should be some indication that work is in progress. For example  we can put an overlay on the table with a spinner (gray out the table with a spinner on top - kind of like when switching filters on JIRA's Agile Board)., in filter sort page host page convert client side server side order address scalabl issu as result respons ui depend upon quickli server respond filter sort page call also depend network while ui wait new tabl content come server indic work progress for exampl put overlay tabl spinner gray tabl spinner top kind like switch filter jira agil board,0,0,0,0,0,0,
6244,Restart icon is present after Service Actions->Restart all button click., restart icon present servic action restart button click,STR:Change property for some service (Hive as example)Save changesClick Service Actions-&gt;Restart all buttonActual result:Service Actions-&gt;Restart all button does not work (do nothing). Restart passed  but restart icon still present.Expected result:Service Actions-&gt;Restart all button works. Restart passed  restart icon is not present after action., str chang properti servic hive exampl save chang click servic action gt restart button actual result servic action gt restart button work noth restart pass restart icon still present expect result servic action gt restart button work restart pass restart icon present action,0,0,0,0,0,0,
6247,Add host stops all services, add host stop servic,STR Install cluster Add hostActual resultAfter adding host all components on all hosts are stoppedExpected resultAll components on all hosts are started, str instal cluster add host actual result after ad host compon host stop expect result all compon host start,0,0,0,0,0,0,
6265,Have spinners instead of charts at Dashboard and Service tabs on IE 11, have spinner instead chart dashboard servic tab ie,STR:1) Deploy cluster with defualt settings.2) Navigate on Dashboard pageExpected result:All charts are present.Actual result:Have spinners instead some charts., str deploy cluster defualt set navig dashboard page expect result all chart present actual result have spinner instead chart,0,0,0,0,0,0,
6266,Get sql metrics logic should be reviewed, get sql metric logic review,,,0,0,0,0,0,0,
6267,Add Services fails with a server error under some conditions, add servic fail server error condit,Upon clicking on 'Deploy' from Review page in Add Services Wizard  sometimes the UI shows a server error saying 'Resource Already Exists'.It looks like the UI is trying to add client components on hosts that already have them.I've seen it for HDFS_CLIENT  MAPREDUCE2_CLIENT  etc.  when trying to add Oozie (for sure)  Storm (I think)  and possibly others., upon click deploy review page add servic wizard sometim ui show server error say resourc alreadi exist it look like ui tri add client compon host alreadi i seen hdf client mapreduc client etc tri add oozi sure storm i think possibl other,0,0,0,0,0,0,
6268,step 6 pagination is slow, step pagin slow,,,0,0,0,0,1,0,
6270,Repoinfo.xml should use family tag rather than type tag, repoinfo xml use famili tag rather type tag,That is a bit confusing we should change that  since we changed the way itworks &lt;os type='redhat6'&gt;should be &lt;os family='redhat6'&gt;, that bit confus chang sinc chang way itwork lt os type redhat gt lt os famili redhat gt,0,0,0,0,0,0,
6273,Manage Config Groups: if Ganglia is not installed config group popup doesn't appear, manag config group ganglia instal config group popup appear,We take some data from Ganglia metrics for hosts and if Ganglia is not installed browser throw js error which blocks popup initializing., we take data ganglia metric host ganglia instal browser throw js error block popup initi,0,0,0,0,0,0,
6276,Prompt to put Service in Maintenance Mode when doing Rolling Restart / Service Stop, prompt put servic mainten mode roll restart servic stop,When initiating Rolling Restart / Service Stop  they would like an option (via a checkbox  for example) to put the service in maintenance mode (if it is not already in MM) to avoid getting a lot of alerts., when initi roll restart servic stop would like option via checkbox exampl put servic mainten mode alreadi mm avoid get lot alert,0,0,0,0,0,0,
6292,Python client caches curl flags between requests causing problems, python client cach curl flag request caus problem,Currently if you use the python client  if you issue a DELETE request followed by a GET request  that GET request becomes a DELETE request. You can imagine why that's undesirable.The problem is that we set the CUSTOMREQUEST field on the DELETE  but we don't unset it on the next GET. pycurl sees it still set and assumes we're still doing a DELETE., current use python client issu delet request follow get request get request becom delet request you imagin undesir the problem set customrequest field delet unset next get pycurl see still set assum still delet,0,0,0,0,0,0,
6295,UI freezes for more than 2 seconds  every 15 seconds  on 2K-node cluster, ui freez second everi second k node cluster,It seems that we call App.componentConfigMapper every 15 seconds.This mapper takes more than 2 seconds to run. While the mapper is running  the entire UI is frozen., it seem call app compon config mapper everi second thi mapper take second run while mapper run entir ui frozen,0,0,0,0,1,0,
6298,Custom Command execution takes too long, custom command execut take long,It takes significant amount of time to populate clusterHostInfo for every Execution Cmd on a 2000 node cluster.org.apache.ambari.server.utils.StageUtils.getClusterHostInfo(Map  Cluster), it take signific amount time popul cluster host info everi execut cmd node cluster org apach ambari server util stage util get cluster host info map cluster,0,0,0,0,0,0,
6299,JMXPropertyProvider makes call to endpoint without checking support for properties, jmx properti provid make call endpoint without check support properti,Causes CPU usage go upto 1500 % and UI becomes unusable.1000's of Exception:00:01:50 666 ERROR [pool-1-thread-17] JMXPropertyProvider:539 - Caught exception getting JMX metrics : Connection refusedAll JMX endpoints called for any JMX metric., caus cpu usag go upto ui becom unus except error pool thread jmx properti provid caught except get jmx metric connect refus all jmx endpoint call jmx metric,0,0,0,0,1,0,
6311,Bulk Ops only targets a maximum of 'page size' hosts, bulk op target maximum page size host,It seems that the max number of target hosts for a bulk operation is capped by the current 'page size' on the Hosts page.For example  on the 2K cluster with 2K DataNodes: Go to Hosts page Click on Actions. Either going to Filtered Hosts (2005) or All Hosts (2005)  select 'DataNodes &gt; Stop' A confirmation popup tells me that 50 DataNodes are about to be stopped. This should have been ~ 2000 DataNodes instead. 50 is the current page size on the Hosts page that I set.Another scenario: Set page size to 50. Select 100 hosts using 'check all'  'next page'  then 'check all' Go to 'Actions &gt; Selected Hosts &gt; DataNodes &gt; Stop'. Again  it tries to perform actions on only 50 DataNodes., it seem max number target host bulk oper cap current page size host page for exampl k cluster k data node go host page click action either go filter host all host select data node gt stop a confirm popup tell data node stop thi data node instead current page size host page i set anoth scenario set page size select host use check next page check go action gt select host gt data node gt stop again tri perform action data node,0,0,0,0,0,0,
6322,Choosing bulk hosts to decommission on 120 node cluster on the hosts page just spins., choos bulk host decommiss node cluster host page spin,Steps to reproduce1. Filter by DataNodes2. Increase the page size to 503. Select all 50 and decommision datanodes.This ends up with a spinner .This might be a blocker. Attaching snapshot., step reproduc filter data node increas page size select decommis datanod thi end spinner thi might blocker attach snapshot,0,0,0,0,0,0,
6327,Rolling Restart: 'only start stale' checkbox not worked functionally, roll restart start stale checkbox work function,Rolling restart page  there is a ' Only restart NodeManagers with stale configs' checkbox  nothing happened no matter if that one was checked or not. 1. On service(HDFS or YARN) actions &gt; restart all DN (NM). All DN(NM) will be restart no matter if the ' Only restart NodeManagers with stale configs' option checked., roll restart page onli restart node manag stale config checkbox noth happen matter one check on servic hdf yarn action gt restart dn nm all dn nm restart matter onli restart node manag stale config option check,0,0,0,0,0,0,
6332,Bulk Decommission: Background Operation Popup content looks broken, bulk decommiss background oper popup content look broken,header in task details popup looks broken when we run decommission for many Datanodes., header task detail popup look broken run decommiss mani datanod,0,0,0,0,0,0,
6335,Add service wizard removes any new property added to core-site and global after cluster installation, add servic wizard remov new properti ad core site global cluster instal,,,0,0,0,0,0,0,
6337,Hosts page. Incorrect total number of hosts after filtering by installed component, host page incorrect total number host filter instal compon,STR1. Go to hosts page2. Filter by Components. Check DATANODE3. At Bottom right corner I see Show 10 | 1-10 of 10, str go host page filter compon check datanod at bottom right corner i see show,0,0,0,0,0,0,
6340,filtering by selected hosts resets pagination, filter select host reset pagin,1. page size 102. select all 10 hosts on first page3. go to page 2  select 2 hosts (total 12 selection)4. click on '12 host selected' and it goes to page 1  1-10 of 12 hosts (correct)5. click to go to page 2 (to see the 2 hosts selected) but it resets to page 1  1-10 of 103, page size select host first page go page select host total select click host select goe page host correct click go page see host select reset page,0,0,0,0,0,0,
6349,After changing property and clicking save button in MapReduce Config Page  no comfirmation popup, after chang properti click save button map reduc config page comfirm popup,STR1. Go to MapReduce Config Page2. Change JobTracker new generation size with a new value3. Clike Save buttonResult no confirmation popup  and save button turns grey  when switching pages the save button in the warning popup is not working neither., str go map reduc config page chang job tracker new gener size new valu clike save button result confirm popup save button turn grey switch page save button warn popup work neither,0,0,0,0,0,0,
6350,Unable to restart all host-components on 110 node cluster, unabl restart host compon node cluster,On a 110 node cluster I went to the hosts page and went into All Hosts &gt; Hosts &gt; Restart All Components. Following that I got an error dialog (image attached) and the below exceptions in the log21:03:20 017 ERROR [qtp1391464722-1188] AmbariJpaLocalTxnInterceptor:114 - [DETAILED ERROR] Rollback reason:Local Exception Stack:Exception [EclipseLink-4002] (Eclipse Persistence Services - 2.4.0.v20120608-r11652): org.eclipse.persistence.exceptions.DatabaseExceptionInternal Exception: java.sql.BatchUpdateException: Batch entry 0 INSERT INTO requestoperationlevel (operation_level_id  cluster_name  host_component_name  host_name  level_name  request_id  service_name) VALUES (2  'Horton'  NULL  'horton-1.c.pramod-thangali.internal horton-10.c.pramod-thangali.internal horton-100.c.pramod-thangali.internal horton-101.c.pramod-thangali.internal horton-103.c.pramod-thangali.internal horton-104.c.pramod-thangali.internal horton-105.c.pramod-thangali.internal horton-106.c.pramod-thangali.internal horton-107.c.pramod-thangali.internal horton-108.c.pramod-thangali.internal horton-109.c.pramod-thangali.internal horton-11.c.pramod-thangali.internal horton-12.c.pramod-thangali.internal horton-13.c.pramod-thangali.internal horton-14.c.pramod-thangali.internal horton-15.c.pramod-thangali.internal horton-16.c.pramod-thangali.internal horton-17.c.pramod-thangali.internal horton-18.c.pramod-thangali.internal horton-19.c.pramod-thangali.internal horton-2.c.pramod-thangali.internal horton-20.c.pramod-thangali.internal horton-21.c.pramod-thangali.internal horton-22.c.pramod-thangali.internal horton-23.c.pramod-thangali.internal horton-24.c.pramod-thangali.internal horton-25.c.pramod-thangali.internal horton-26.c.pramod-thangali.internal horton-27.c.pramod-thangali.internal horton-28.c.pramod-thangali.internal horton-29.c.pramod-thangali.internal horton-3.c.pramod-thangali.internal horton-30.c.pramod-thangali.internal horton-31.c.pramod-thangali.internal horton-32.c.pramod-thangali.internal horton-33.c.pramod-thangali.internal horton-34.c.pramod-thangali.internal horton-35.c.pramod-thangali.internal horton-36.c.pramod-thangali.internal horton-37.c.pramod-thangali.internal horton-38.c.pramod-thangali.internal horton-39.c.pramod-thangali.internal horton-4.c.pramod-thangali.internal horton-40.c.pramod-thangali.internal horton-41.c.pramod-thangali.internal horton-42.c.pramod-thangali.internal horton-43.c.pramod-thangali.internal horton-44.c.pramod-thangali.internal horton-45.c.pramod-thangali.internal horton-46.c.pramod-thangali.internal horton-47.c.pramod-thangali.internal horton-48.c.pramod-thangali.internal horton-49.c.pramod-thangali.internal horton-5.c.pramod-thangali.internal horton-50.c.pramod-thangali.internal horton-51.c.pramod-thangali.internal horton-52.c.pramod-thangali.internal horton-53.c.pramod-thangali.internal horton-54.c.pramod-thangali.internal horton-55.c.pramod-thangali.internal horton-56.c.pramod-thangali.internal horton-57.c.pramod-thangali.internal horton-58.c.pramod-thangali.internal horton-59.c.pramod-thangali.internal horton-6.c.pramod-thangali.internal horton-60.c.pramod-thangali.internal horton-61.c.pramod-thangali.internal horton-62.c.pramod-thangali.internal horton-63.c.pramod-thangali.internal horton-64.c.pramod-thangali.internal horton-65.c.pramod-thangali.internal horton-66.c.pramod-thangali.internal horton-67.c.pramod-thangali.internal horton-68.c.pramod-thangali.internal horton-69.c.pramod-thangali.internal horton-7.c.pramod-thangali.internal horton-70.c.pramod-thangali.internal horton-71.c.pramod-thangali.internal horton-72.c.pramod-thangali.internal horton-73.c.pramod-thangali.internal horton-74.c.pramod-thangali.internal horton-75.c.pramod-thangali.internal horton-76.c.pramod-thangali.internal horton-77.c.pramod-thangali.internal horton-78.c.pramod-thangali.internal horton-79.c.pramod-thangali.internal horton-8.c.pramod-thangali.internal horton-80.c.pramod-thangali.internal horton-81.c.pramod-thangali.internal horton-82.c.pramod-thangali.internal horton-83.c.pramod-thangali.internal horton-84.c.pramod-thangali.internal horton-85.c.pramod-thangali.internal horton-86.c.pramod-thangali.internal horton-87.c.pramod-thangali.internal horton-88.c.pramod-thangali.internal horton-9.c.pramod-thangali.internal horton-90.c.pramod-thangali.internal horton-91.c.pramod-thangali.internal horton-92.c.pramod-thangali.internal horton-93.c.pramod-thangali.internal horton-94.c.pramod-thangali.internal horton-95.c.pramod-thangali.internal horton-96.c.pramod-thangali.internal horton-97.c.pramod-thangali.internal horton-98.c.pramod-thangali.internal horton-99.c.pramod-thangali.internal horton-master-1.c.pramod-thangali.internal horton-master-2.c.pramod-thangali.internal horton-master-3.c.pramod-thangali.internal'  'Host'  25  NULL) was aborted. Call getNextException to see the cause.Error Code: 0Call: INSERT INTO requestoperationlevel (operation_level_id  cluster_name  host_component_name  host_name  level_name  request_id  service_name) VALUES (?  ?  ?  ?  ?  ?  ?) bind =&gt; [7 parameters bound]Query: InsertObjectQuery(org.apache.ambari.server.orm.entities.RequestResourceFilterEntity@2c6ae575) at org.eclipse.persistence.exceptions.DatabaseException.sqlException(DatabaseException.java:333) at org.eclipse.persistence.internal.databaseaccess.DatabaseAccessor.processExceptionForCommError(DatabaseAccessor.java:1501) at org.eclipse.persistence.internal.databaseaccess.DatabaseAccessor.executeJDK12BatchStatement(DatabaseAccessor.java:875) at org.eclipse.persistence.internal.databaseaccess.ParameterizedSQLBatchWritingMechanism.executeBatchedStatements(ParameterizedSQLBatchWritingMechanism.java:145) at org.eclipse.persistence.internal.databaseaccess.ParameterizedSQLBatchWritingMechanism.appendCall(ParameterizedSQLBatchWritingMechanism.java:88) at org.eclipse.persistence.internal.databaseaccess.DatabaseAccessor.basicExecuteCall(DatabaseAccessor.java:571) at org.eclipse.persistence.internal.databaseaccess.DatabaseAccessor.executeCall(DatabaseAccessor.java:537) at org.eclipse.persistence.internal.sessions.AbstractSession.basicExecuteCall(AbstractSession.java:1800) at org.eclipse.persistence.sessions.server.ClientSession.executeCall(ClientSession.java:286) at org.eclipse.persistence.internal.queries.DatasourceCallQueryMechanism.executeCall(DatasourceCallQueryMechanism.java:207) at org.eclipse.persistence.internal.queries.DatasourceCallQueryMechanism.executeCall(DatasourceCallQueryMechanism.java:193) at org.eclipse.persistence.internal.queries.DatasourceCallQueryMechanism.insertObject(DatasourceCallQueryMechanism.java:342) at org.eclipse.persistence.internal.queries.StatementQueryMechanism.insertObject(StatementQueryMechanism.java:162) at org.eclipse.persistence.internal.queries.StatementQueryMechanism.insertObject(StatementQueryMechanism.java:177) at org.eclipse.persistence.internal.queries.DatabaseQueryMechanism.insertObjectForWrite(DatabaseQueryMechanism.java:471) at org.eclipse.persistence.queries.InsertObjectQuery.executeCommit(InsertObjectQuery.java:80) at org.eclipse.persistence.queries.InsertObjectQuery.executeCommitWithChangeSet(InsertObjectQuery.java:90) at org.eclipse.persistence.internal.queries.DatabaseQueryMechanism.executeWriteWithChangeSet(DatabaseQueryMechanism.java:286) at org.eclipse.persistence.queries.WriteObjectQuery.executeDatabaseQuery(WriteObjectQuery.java:58) at org.eclipse.persistence.queries.DatabaseQuery.execute(DatabaseQuery.java:852) at org.eclipse.persistence.queries.DatabaseQuery.executeInUnitOfWork(DatabaseQuery.java:751) at org.eclipse.persistence.queries.ObjectLevelModifyQuery.executeInUnitOfWorkObjectLevelModifyQuery(ObjectLevelModifyQuery.java:108) at org.eclipse.persistence.queries.ObjectLevelModifyQuery.executeInUnitOfWork(ObjectLevelModifyQuery.java:85) at org.eclipse.persistence.internal.sessions.UnitOfWorkImpl.internalExecuteQuery(UnitOfWorkImpl.java:2875) at org.eclipse.persistence.internal.sessions.AbstractSession.executeQuery(AbstractSession.java:1602) at org.eclipse.persistence.internal.sessions.AbstractSession.executeQuery(AbstractSession.java:1584) at org.eclipse.persistence.internal.sessions.AbstractSession.executeQuery(AbstractSession.java:1535) at org.eclipse.persistence.internal.sessions.CommitManager.commitNewObjectsForClassWithChangeSet(CommitManager.java:224) at org.eclipse.persistence.internal.sessions.CommitManager.commitAllObjectsForClassWithChangeSet(CommitManager.java:191) at org.eclipse.persistence.internal.sessions.CommitManager.commitAllObjectsWithChangeSet(CommitManager.java:136) at org.eclipse.persistence.internal.sessions.AbstractSession.writeAllObjectsWithChangeSet(AbstractSession.java:3914) at org.eclipse.persistence.internal.sessions.UnitOfWorkImpl.commitToDatabase(UnitOfWorkImpl.java:1419) at org.eclipse.persistence.internal.sessions.RepeatableWriteUnitOfWork.commitToDatabase(RepeatableWriteUnitOfWork.java:634) at org.eclipse.persistence.internal.sessions.UnitOfWorkImpl.commitToDatabaseWithChangeSet(UnitOfWorkImpl.java:1509) at org.eclipse.persistence.internal.sessions.RepeatableWriteUnitOfWork.commitRootUnitOfWork(RepeatableWriteUnitOfWork.java:266) at org.eclipse.persistence.internal.sessions.UnitOfWorkImpl.commitAndResume(UnitOfWorkImpl.java:1147) at org.eclipse.persistence.internal.jpa.transaction.EntityTransactionImpl.commitInternal(EntityTransactionImpl.java:84) at org.eclipse.persistence.internal.jpa.transaction.EntityTransactionImpl.commit(EntityTransactionImpl.java:63) at org.apache.ambari.server.orm.AmbariJpaLocalTxnInterceptor.invoke(AmbariJpaLocalTxnInterceptor.java:91) at com.google.inject.internal.InterceptorStackCallback$InterceptedMethodInvocation.proceed(InterceptorStackCallback.java:72) at com.google.inject.internal.InterceptorStackCallback.intercept(InterceptorStackCallback.java:52) at org.apache.ambari.server.actionmanager.ActionDBAccessorImpl$$EnhancerByGuice$$ddd0d231.persistActions(&lt;generated&gt;) at org.apache.ambari.server.actionmanager.ActionManager.sendActions(ActionManager.java:95) at org.apache.ambari.server.actionmanager.ActionManager.sendActions(ActionManager.java:84) at org.apache.ambari.server.controller.AmbariManagementControllerImpl.createAction(AmbariManagementControllerImpl.java:2583) at org.apache.ambari.server.controller.internal.RequestResourceProvider$1.invoke(RequestResourceProvider.java:124) at org.apache.ambari.server.controller.internal.RequestResourceProvider$1.invoke(RequestResourceProvider.java:121) at org.apache.ambari.server.controller.internal.AbstractResourceProvider.createResources(AbstractResourceProvider.java:237) at org.apache.ambari.server.controller.internal.RequestResourceProvider.createResources(RequestResourceProvider.java:121) at org.apache.ambari.server.controller.internal.ClusterControllerImpl.createResources(ClusterControllerImpl.java:274) at org.apache.ambari.server.api.services.persistence.PersistenceManagerImpl.create(PersistenceManagerImpl.java:75) at org.apache.ambari.server.api.handlers.CreateHandler.persist(CreateHandler.java:36) at org.apache.ambari.server.api.handlers.BaseManagementHandler.handleRequest(BaseManagementHandler.java:72) at org.apache.ambari.server.api.services.BaseRequest.process(BaseRequest.java:135) at org.apache.ambari.server.api.services.BaseService.handleRequest(BaseService.java:103) at org.apache.ambari.server.api.services.BaseService.handleRequest(BaseService.java:72) at org.apache.ambari.server.api.services.RequestService.createRequests(RequestService.java:119) at sun.reflect.GeneratedMethodAccessor113.invoke(Unknown Source), on node cluster i went host page went all host gt host gt restart all compon follow i got error dialog imag attach except log error qtp ambari jpa local txn interceptor detail error rollback reason local except stack except eclips link eclips persist servic v r org eclips persist except databas except intern except java sql batch updat except batch entri insert into requestoperationlevel oper level id cluster name host compon name host name level name request id servic name valu horton null horton c pramod thangali intern horton c pramod thangali intern horton c pramod thangali intern horton c pramod thangali intern horton c pramod thangali intern horton c pramod thangali intern horton c pramod thangali intern horton c pramod thangali intern horton c pramod thangali intern horton c pramod thangali intern horton c pramod thangali intern horton c pramod thangali intern horton c pramod thangali intern horton c pramod thangali intern horton c pramod thangali intern horton c pramod thangali intern horton c pramod thangali intern horton c pramod thangali intern horton c pramod thangali intern horton c pramod thangali intern horton c pramod thangali intern horton c pramod thangali intern horton c pramod thangali intern horton c pramod thangali intern horton c pramod thangali intern horton c pramod thangali intern horton c pramod thangali intern horton c pramod thangali intern horton c pramod thangali intern horton c pramod thangali intern horton c pramod thangali intern horton c pramod thangali intern horton c pramod thangali intern horton c pramod thangali intern horton c pramod thangali intern horton c pramod thangali intern horton c pramod thangali intern horton c pramod thangali intern horton c pramod thangali intern horton c pramod thangali intern horton c pramod thangali intern horton c pramod thangali intern horton c pramod thangali intern horton c pramod thangali intern horton c pramod thangali intern horton c pramod thangali intern horton c pramod thangali intern horton c pramod thangali intern horton c pramod thangali intern horton c pramod thangali intern horton c pramod thangali intern horton c pramod thangali intern horton c pramod thangali intern horton c pramod thangali intern horton c pramod thangali intern horton c pramod thangali intern horton c pramod thangali intern horton c pramod thangali intern horton c pramod thangali intern horton c pramod thangali intern horton c pramod thangali intern horton c pramod thangali intern horton c pramod thangali intern horton c pramod thangali intern horton c pramod thangali intern horton c pramod thangali intern horton c pramod thangali intern horton c pramod thangali intern horton c pramod thangali intern horton c pramod thangali intern horton c pramod thangali intern horton c pramod thangali intern horton c pramod thangali intern horton c pramod thangali intern horton c pramod thangali intern horton c pramod thangali intern horton c pramod thangali intern horton c pramod thangali intern horton c pramod thangali intern horton c pramod thangali intern horton c pramod thangali intern horton c pramod thangali intern horton c pramod thangali intern horton c pramod thangali intern horton c pramod thangali intern horton c pramod thangali intern horton c pramod thangali intern horton c pramod thangali intern horton c pramod thangali intern horton c pramod thangali intern horton c pramod thangali intern horton c pramod thangali intern horton c pramod thangali intern horton c pramod thangali intern horton c pramod thangali intern horton c pramod thangali intern horton c pramod thangali intern horton c pramod thangali intern horton c pramod thangali intern horton c pramod thangali intern horton c pramod thangali intern horton c pramod thangali intern horton c pramod thangali intern horton c pramod thangali intern horton c pramod thangali intern horton c pramod thangali intern horton c pramod thangali intern horton master c pramod thangali intern horton master c pramod thangali intern horton master c pramod thangali intern host null abort call get next except see caus error code call insert into requestoperationlevel oper level id cluster name host compon name host name level name request id servic name valu bind gt paramet bound queri insert object queri org apach ambari server orm entiti request resourc filter entiti c ae org eclips persist except databas except sql except databas except java org eclips persist intern databaseaccess databas accessor process except for comm error databas accessor java org eclips persist intern databaseaccess databas accessor execut jdk batch statement databas accessor java org eclips persist intern databaseaccess parameter sql batch write mechan execut batch statement parameter sql batch write mechan java org eclips persist intern databaseaccess parameter sql batch write mechan append call parameter sql batch write mechan java org eclips persist intern databaseaccess databas accessor basic execut call databas accessor java org eclips persist intern databaseaccess databas accessor execut call databas accessor java org eclips persist intern session abstract session basic execut call abstract session java org eclips persist session server client session execut call client session java org eclips persist intern queri datasourc call queri mechan execut call datasourc call queri mechan java org eclips persist intern queri datasourc call queri mechan execut call datasourc call queri mechan java org eclips persist intern queri datasourc call queri mechan insert object datasourc call queri mechan java org eclips persist intern queri statement queri mechan insert object statement queri mechan java org eclips persist intern queri statement queri mechan insert object statement queri mechan java org eclips persist intern queri databas queri mechan insert object for write databas queri mechan java org eclips persist queri insert object queri execut commit insert object queri java org eclips persist queri insert object queri execut commit with chang set insert object queri java org eclips persist intern queri databas queri mechan execut write with chang set databas queri mechan java org eclips persist queri write object queri execut databas queri write object queri java org eclips persist queri databas queri execut databas queri java org eclips persist queri databas queri execut in unit of work databas queri java org eclips persist queri object level modifi queri execut in unit of work object level modifi queri object level modifi queri java org eclips persist queri object level modifi queri execut in unit of work object level modifi queri java org eclips persist intern session unit of work impl intern execut queri unit of work impl java org eclips persist intern session abstract session execut queri abstract session java org eclips persist intern session abstract session execut queri abstract session java org eclips persist intern session abstract session execut queri abstract session java org eclips persist intern session commit manag commit new object for class with chang set commit manag java org eclips persist intern session commit manag commit all object for class with chang set commit manag java org eclips persist intern session commit manag commit all object with chang set commit manag java org eclips persist intern session abstract session write all object with chang set abstract session java org eclips persist intern session unit of work impl commit to databas unit of work impl java org eclips persist intern session repeat write unit of work commit to databas repeat write unit of work java org eclips persist intern session unit of work impl commit to databas with chang set unit of work impl java org eclips persist intern session repeat write unit of work commit root unit of work repeat write unit of work java org eclips persist intern session unit of work impl commit and resum unit of work impl java org eclips persist intern jpa transact entiti transact impl commit intern entiti transact impl java org eclips persist intern jpa transact entiti transact impl commit entiti transact impl java org apach ambari server orm ambari jpa local txn interceptor invok ambari jpa local txn interceptor java com googl inject intern interceptor stack callback intercept method invoc proceed interceptor stack callback java com googl inject intern interceptor stack callback intercept interceptor stack callback java org apach ambari server actionmanag action db accessor impl enhanc by guic ddd persist action lt gener gt org apach ambari server actionmanag action manag send action action manag java org apach ambari server actionmanag action manag send action action manag java org apach ambari server control ambari manag control impl creat action ambari manag control impl java org apach ambari server control intern request resourc provid invok request resourc provid java org apach ambari server control intern request resourc provid invok request resourc provid java org apach ambari server control intern abstract resourc provid creat resourc abstract resourc provid java org apach ambari server control intern request resourc provid creat resourc request resourc provid java org apach ambari server control intern cluster control impl creat resourc cluster control impl java org apach ambari server api servic persist persist manag impl creat persist manag impl java org apach ambari server api handler creat handler persist creat handler java org apach ambari server api handler base manag handler handl request base manag handler java org apach ambari server api servic base request process base request java org apach ambari server api servic base servic handl request base servic java org apach ambari server api servic base servic handl request base servic java org apach ambari server api servic request servic creat request request servic java sun reflect gener method accessor invok unknown sourc,0,0,0,0,0,0,
6359,Hosts page : mysterious 'selected hosts' keep coming back, host page mysteri select host keep come back,See attached video.Somehow  there are always 10 hosts selected. After explicitly hitting 'clear selection'  page refresh somehow reverts back to 10 hosts being selected., see attach video somehow alway host select after explicitli hit clear select page refresh somehow revert back host select,0,0,0,0,1,0,
6369,hostname wrapping with icon asterisks on assign slaves, hostnam wrap icon asterisk assign slave,see attached., see attach,0,0,0,0,0,0,
6376,Excessive requests are sending on hosts page, excess request send host page,When we open hosts page  we can see  that we have excessive requests as spinner appears two times one after another., when open host page see excess request spinner appear two time one anoth,0,0,0,0,0,0,
6379,In Host Detailed Page  decommissioned NM is labeled as STOPPED, in host detail page decommiss nm label stop,In Yarn Service page  NodeManagers Status: 3 active / 0 lost / 0 unhealthy / 0 rebooted / 1 decommissionedIn Host Detailed Page of that decommissioned NM  the state of the NM is labeled as 'stopped'  which is not consistent comparing to Service page.API call:'href' : 'http://172.18.145.115:8080/api/v1/clusters/cl1/hosts/us3mon1404188570-3.cs1cloud.internal/host_components/NODEMANAGER'  'HostRoles' : { 'cluster_name' : 'cl1'  'component_name' : 'NODEMANAGER'  'desired_admin_state' : 'DECOMMISSIONED'  'desired_stack_id' : 'HDP-2.1'  'desired_state' : 'STARTED'  'host_name' : 'us3mon1404188570-3.cs1cloud.internal'  'maintenance_state' : 'OFF'  'service_name' : 'YARN'  'stack_id' : 'HDP-2.1'  'stale_configs' : false  'state' : 'INSTALLED' As a result  user cannot start the NodeManger as it is considered to be decommissioned by ResourceManger., in yarn servic page node manag statu activ lost unhealthi reboot decommiss in host detail page decommiss nm state nm label stop consist compar servic page api call href http api v cluster cl host us mon cs cloud intern host compon nodemanag host role cluster name cl compon name nodemanag desir admin state decommiss desir stack id hdp desir state start host name us mon cs cloud intern mainten state off servic name yarn stack id hdp stale config fals state instal as result user cannot start node manger consid decommiss resourc manger,0,0,0,0,0,0,
6402,Many services failed to start when using custom user names and groups, mani servic fail start use custom user name group,On Customize services page set next values for users and groups:Name: ValueProxy group for Hive WebHCat Oozie and Falcon: custom-usersgrHDFS User: custom-hdfsMapReduce User: custom-mapredYARN User: custom-yarnHBase User: custom-hbaseHive User: custom-hiveHCat User: custom-hcatWebHCat User: custom-hcatOozie User: custom-oozieFalcon User: custom-falconStorm User: custom-stormZooKeeper User: custom-zookeeperGanglia User: custom-nobodyNagios User: custom-nagiosNagios Group: custom-nagiosgrSmoke Test User: ambari-qaTez User: custom-tezHadoop Group: custom-hadoopgrSkip group modifications during install: falseAfter deploy many services fail to start:MapReduce  Hbase  Hive  WebHcat  Falcon  OozieSame error for all:Call From ins1404365245-9.cs1cloud.internal/172.18.145.154 to localhost:8020 failed on connection exception: java.net.ConnectException: Connection refused; For more details see: http://wiki.apache.org/hadoop/ConnectionRefused, on custom servic page set next valu user group name valu proxi group hive web h cat oozi falcon custom usersgr hdf user custom hdf map reduc user custom mapr yarn user custom yarn h base user custom hbase hive user custom hive h cat user custom hcat web h cat user custom hcat oozi user custom oozi falcon user custom falcon storm user custom storm zoo keeper user custom zookeep ganglia user custom nobodi nagio user custom nagio nagio group custom nagiosgr smoke test user ambari qa tez user custom tez hadoop group custom hadoopgr skip group modif instal fals after deploy mani servic fail start map reduc hbase hive web hcat falcon oozi same error call from in cs cloud intern localhost fail connect except java net connect except connect refus for detail see http wiki apach org hadoop connect refus,0,0,0,0,0,0,
6405,Move Wizard: get stuck if wizard is running second time for the same component without page refresh, move wizard get stuck wizard run second time compon without page refresh,STR:1. Run Move Wizard for some master component.2. Do not click Complete button after all operation will be finished. Close wizard (confirm closing).3. Without page refresh open wizard for the same master component and try it to reassign to another host (ex. back to the previous host).Wizard will get stuck  as source host for master component will be calculated incorrectly.This issue is related to host components mapper. After first running of move wizard in the model there were 2 masters one with host before moving and the new one. Old host component was deleted on server but UI model still contains it. And it brakes algorithm of calculating source host on assign master step., str run move wizard master compon do click complet button oper finish close wizard confirm close without page refresh open wizard master compon tri reassign anoth host ex back previou host wizard get stuck sourc host master compon calcul incorrectli thi issu relat host compon mapper after first run move wizard model master one host move new one old host compon delet server ui model still contain and brake algorithm calcul sourc host assign master step,0,0,0,0,1,0,
6406,Move Wizard: assign master step next button is not disabled  when host input has error, move wizard assign master step next button disabl host input error,When Move Wizard is running on cluster with a large number of hosts  assign master step has input to enter hostname instead of select dropdown. If this dropdown is empty  it shows an error. But if in the same time target host was selected correctly the next button is enabled., when move wizard run cluster larg number host assign master step input enter hostnam instead select dropdown if dropdown empti show error but time target host select correctli next button enabl,0,0,0,0,0,0,
6409,Postgres create script generates errors for clusterconfig, postgr creat script gener error clusterconfig,Manually running the postgres create sql script encountered errors.Errors: psql:Ambari-DDL-Postgres-CREATE.sql:22: ERROR: column 'config_attributes' specified more than once psql:Ambari-DDL-Postgres-CREATE.sql:103: ERROR: relation 'clusterconfig' does not exist psql:Ambari-DDL-Postgres-CREATE.sql:126: ERROR: relation 'clusterconfig' does not existRepro Steps:1. Spin up a CentOS 6.4 VM vagrant up c6401 vagrant ssh c6401 sudo su - wget http://public-repo-1.hortonworks.com/ambari/centos6/1.x/updates/1.6.0/ambari.repo cp ambari.repo /etc/yum.repos.d yum install ambari-server -y2. Setup a postgres database vi /etc/yum.repos.d/CentOS-Base.repo edit the &#91;base&#93; and &#91;updates&#93; sections by adding the following line without quotes  'exclude=postgresql*' yum localinstall http://yum.postgresql.org/9.3/redhat/rhel-6-x86_64/pgdg-centos93-9.3-1.noarch.rpm yum install postgresql-server cd /etc/yum.repos.d/ service postgresql initdb chkconfig postgresql on service postgresql start3. Run the script cp /vagrant/Ambari-DDL-Postgres-CREATE.sql /var/lib/ambari-server/resources/ (assuming you have the latest file from trunk) cp /var/lib/ambari-server/resources/Ambari-DDL-Postgres-CREATE.sql /var/lib/pgsql/ su - postgres psql Follow step #1 at http://docs.hortonworks.com/HDPDocuments/Ambari-1.6.0.0/bk_ambari_reference/content/nndb-using-ambari-postgresql.html where $AMBARIDATABASE is ambari  $AMBARIUSER is postgres  and $AMBARISCHEMA is ambari. /i Ambari-DDL-Postgres-CREATE.sql;Then verify the errors. When done  issue /q on the psql command line to exit., manual run postgr creat sql script encount error error psql ambari ddl postgr creat sql error column config attribut specifi psql ambari ddl postgr creat sql error relat clusterconfig exist psql ambari ddl postgr creat sql error relat clusterconfig exist repro step spin cent os vm vagrant c vagrant ssh c sudo su wget http public repo hortonwork com ambari cento x updat ambari repo cp ambari repo etc yum repo yum instal ambari server setup postgr databas vi etc yum repo cent os base repo edit base updat section ad follow line without quot exclud postgresql yum localinstal http yum postgresql org redhat rhel x pgdg cento noarch rpm yum instal postgresql server cd etc yum repo servic postgresql initdb chkconfig postgresql servic postgresql start run script cp vagrant ambari ddl postgr creat sql var lib ambari server resourc assum latest file trunk cp var lib ambari server resourc ambari ddl postgr creat sql var lib pgsql su postgr psql follow step http doc hortonwork com hdp document ambari bk ambari refer content nndb use ambari postgresql html ambaridatabas ambari ambarius postgr ambarischema ambari ambari ddl postgr creat sql then verifi error when done issu q psql command line exit,0,0,0,0,0,0,
6418,Config pages load slowly on 2k-node cluster for some services, config page load slowli k node cluster servic,On the 2k-node cluster  service config pages load slowly (though the load time has improved significantly from before).The load time depends on the service.Here are some sample load times:HDFS: 5sYARN: 17sMR2: 16sTEZ: 10sHBASE: 3sHIVE: 13sWEBHCAT: 3sFALCON: 3sSTORM: 17sOOZIE: 11sGANGLIA: 1sNAGIOS: 1sZOOKEEPER: 2sPIG: 2sSQOOP: n/a (no config page), on k node cluster servic config page load slowli though load time improv significantli the load time depend servic here sampl load time hdf yarn mr tez hbase hive webhcat falcon storm oozi ganglia nagio zookeep pig sqoop n config page,0,0,0,0,1,0,
6426,Link to ExtJS license in 'Choose Services' page has moved, link ext js licens choos servic page move,Step 4 of the Ambari installer has a broken link.When 'Choosing Services'  the link for the 'ExtJS' library license under the 'Oozie' service has moved from http://www.sencha.com/products/extjs/license/It should be updated to the URL below since it covers that Ext JS is available under GPL v3 license:http://www.sencha.com/legal/open-source-faq/, step ambari instal broken link when choos servic link ext js librari licens oozi servic move http www sencha com product extj licens it updat url sinc cover ext js avail gpl v licens http www sencha com legal open sourc faq,0,0,0,0,0,1,
6439,Filter state does not clear in Background Operation popup and causes a lot of confusion, filter state clear background oper popup caus lot confus,In the Background Operations popup  the filter state persists at the Host and Task levels  even after closing the popup (the filter should be cleared once the user goes back up one level or closes the popup).This causes a lot of confusion because the user will expect these filters to be cleared but instead cannot see the hosts and tasks that they are expecting., in background oper popup filter state persist host task level even close popup filter clear user goe back one level close popup thi caus lot confus user expect filter clear instead cannot see host task expect,0,0,0,0,0,0,
6440,Unable to move NameNode, unabl move name node,STR: Enabled NNHA.Active NameNode is on suse1101 hostStandby NameNode is on suse1102 host Stopped SNN. Opened move NN wizardResult: Next button is active when I choose existing topology (suse1101 and suse1102)Next button is inactive when I choose custom topology (suse1103 and suse1101)UPD: Reproduced also on non-HA cluster., str enabl nnha activ name node suse host standbi name node suse host stop snn open move nn wizard result next button activ i choos exist topolog suse suse next button inact i choos custom topolog suse suse upd reproduc also non ha cluster,0,0,0,0,0,0,
6442,JS error occurs periodically when Job Details page is opened, js error occur period job detail page open,'Uncaught TypeError: Cannot read property 'properties' of undefined'at /ambari-web/app/views/common/quick_view_link_view.js:246, uncaught type error cannot read properti properti undefin ambari web app view common quick view link view js,0,0,0,0,1,0,
6471,Add Services Wizard can corrupt config files unpredictably, add servic wizard corrupt config file unpredict,Testing using the 1.6.1 RC bits (branch-1.6.1  hash=ffb702b252a8b979529864ec5579465a122060b5) revealed that Add Services can corrupt config files in an unpredictable manner.We have seen: adding Pig resulted in core-site dropping all existing properties except for 'hadoop.proxyuser.falcon.hosts and hadoop.proxyuser.falcon.groups' add Storm resulted in core-site retaining all existing properties but dropping hadoop.proxyuser.* settings for all services except Falcon adding Storm dropped a number of existing properties in yarn-site (and NodeManagers can no longer restart after that)The bottomline here is that behavior seems pretty random (adding the same service on different clusters results in different behavior).Also  when adding a service  certain configs (like hdfs-site) were not cloned while most other configs (like yarn-site  mapred-site) get cloned even if no property changes are made regardless of which service is added.Update: changes to hdfs-site during Add Services Wizard never persists., test use rc bit branch hash ffb b b ec b reveal add servic corrupt config file unpredict manner we seen ad pig result core site drop exist properti except hadoop proxyus falcon host hadoop proxyus falcon group add storm result core site retain exist properti drop hadoop proxyus set servic except falcon ad storm drop number exist properti yarn site node manag longer restart the bottomlin behavior seem pretti random ad servic differ cluster result differ behavior also ad servic certain config like hdf site clone config like yarn site mapr site get clone even properti chang made regardless servic ad updat chang hdf site add servic wizard never persist,0,0,0,0,0,0,
6477,AddHost Wizard: error for existing host is not present, add host wizard error exist host present,STR: Click AddHost Type existing host Click 'Register and Confirm' buttonActual result: There present only message about 'SSH Private Key is required'Excpected Result: SHould be also present message like 'These hosts are already exists'.---------------------------------Note: the excpected message only present if we type/select ss key and will click 'Register and Confirm', str click add host type exist host click regist confirm button actual result there present messag ssh privat key requir excpect result s hould also present messag like these host alreadi exist note excpect messag present type select ss key click regist confirm,0,0,0,0,0,0,
6483,JS errors on Jobs page if ATS is stopped, js error job page at stop,If ATS stopped  Jobs page continuously produces following js errors:NetworkError: 400 Bad Request - http://172.18.145.185:8080/proxy?url=http://us1mon1402550931-re-5.cs1cloud.internal:8188/ws/v1/timeline/HIVE_QUERY_ID?fields=events primaryfilters otherinfo&amp;secondaryFilter=tez:true&amp;limit=11&amp;_=1402578530622'proxy?...8530622'NetworkError: 400 Bad Request - http://172.18.145.185:8080/proxy?url=http://us1mon1402550931-re-5.cs1cloud.internal:8188/ws/v1/timeline/HIVE_QUERY_ID?limit=1&amp;secondaryFilter=tez:true&amp;_=1402578531674'proxy?...8531674'NetworkError: 400 Bad Request - http://172.18.145.185:8080/proxy?url=http://us1mon1402550931-re-5.cs1cloud.internal:8188/ws/v1/timeline/HIVE_QUERY_ID?limit=1&amp;secondaryFilter=tez:true&amp;_=1402578537654', if at stop job page continu produc follow js error network error bad request http proxi url http us mon cs cloud intern ws v timelin hive queri id field event primaryfilt otherinfo amp secondari filter tez true amp limit amp proxi network error bad request http proxi url http us mon cs cloud intern ws v timelin hive queri id limit amp secondari filter tez true amp proxi network error bad request http proxi url http us mon cs cloud intern ws v timelin hive queri id limit amp secondari filter tez true amp,1,0,0,0,0,0,
6491,Default config group name contains 'undefined' instead of service name in Manage Config Groups popup, default config group name contain undefin instead servic name manag config group popup,,,0,0,0,0,0,0,
6499,Unable to proceed from step 4 of installer if YARN-dependent services are selected but YARN isn't (HDP 2.0), unabl proceed step instal yarn depend servic select yarn hdp,STROn step 4 of Install Wizard  select any of the services dependent on YARN (Piq  Oozie  Hive) but don't select YARN itself. Press 'Next' button.Expected resultPopup with the info about YARN dependence appears.Actual resultNothing happens. JS error occurs:Uncaught TypeError: Cannot read property 'get' of undefinedatapp/controllers/wizard/step4_controller.js:180, str on step instal wizard select servic depend yarn piq oozi hive select yarn press next button expect result popup info yarn depend appear actual result noth happen js error occur uncaught type error cannot read properti get undefinedatapp control wizard step control js,0,0,0,0,0,0,
6504,Incorrect errors count for YARN on step 7 of Install Wizard (HDP 2.0), incorrect error count yarn step instal wizard hdp,'Customize Services' step indicates that 8 YARN properties need revision but no one is highlighted as incorrect., custom servic step indic yarn properti need revis one highlight incorrect,0,0,0,0,0,0,
6521,Hostname case sensitivity, hostnam case sensit,Hostname CaSe SeNsiTiVity causes hostnames don't match error.'in the install wizard  the hostnames MUST be lower case in the user input  otherwise the 'hostnames don't match' during the install and it fails. Ambari should tolower() both hosts before comparing.'Effected Install/Add Host Wizard., hostnam ca se se nsi ti viti caus hostnam match error instal wizard hostnam must lower case user input otherwis hostnam match instal fail ambari tolow host compar effect instal add host wizard,0,0,0,0,0,0,
6525,Unable to install cluster after coming back to step 1 and selecting different stack version, unabl instal cluster come back step select differ stack version,STRProceed to step 7 of Install Wizard. Return to step 1 and choose a different stack version. Go forward.ResultUnable to proceed from step 3. JS error occurs:Uncaught Error: &lt;DS.StateManager:ember26192&gt; could not respond to event didChangeData in state rootState.loaded.updated.uncommitted.WorkaroundTo log out and log in again before perforfing installation with different stack version., str proce step instal wizard return step choos differ stack version go forward result unabl proceed step js error occur uncaught error lt ds state manag ember gt could respond event chang data state root state load updat uncommit workaround to log log perforf instal differ stack version,0,0,0,0,0,0,
6527,Ambari support on CentOS7 /RHEL7 - I, ambari support cent os rhel i,Background : Centos7 is very different from centos6.5 and currently Ambari &amp; HDP are not supported on Centos7Supporting Ambari on CentOS7/RHEL7 has multiple elements :1)mvn build of Ambari should be successful. document required steps in wiki.2)Ambari server &amp; agent should be modified to support centos7/redhat7. following changes are necessary :2.1)/usr/lib/python2.6 symbolic link .2.2)code changes to add redhat7 as a new OS family.3)HDP centos7 rpms + new repo to be made available to deploy different services on centos7/redhat7 .Currently centos7 is not supported by the Ambari/HDP build team.This JIRA captures 2.2), background cento differ cento current ambari amp hdp support cento support ambari cent os rhel multipl element mvn build ambari success document requir step wiki ambari server amp agent modifi support cento redhat follow chang necessari usr lib python symbol link code chang add redhat new os famili hdp cento rpm new repo made avail deploy differ servic cento redhat current cento support ambari hdp build team thi jira captur,1,0,0,0,0,0,
6539,Create main page with table of jobs, creat main page tabl job,,,1,0,0,0,0,0,
6544,Add ZKFC component to summary page (when NNHA enabled), add zkfc compon summari page nnha enabl,Add ZKFC component on summary page. Each Zkfc should under related Namenode  with a smaller size., add zkfc compon summari page each zkfc relat namenod smaller size,1,0,0,0,0,0,
6558,FE allows add host on host that is in UNKNOWN state (agent was stopped), fe allow add host host unknown state agent stop,added a host to a cluster (manually installed and started an agent)http://c6403.ambari.apache.org:8080/api/v1/hosts/c6401.ambari.apache.org{ 'href' : 'http://c6403.ambari.apache.org:8080/api/v1/hosts/c6401.ambari.apache.org'  'Hosts' : { 'host_health_report' : ''  'host_name' : 'c6401.ambari.apache.org'  'host_state' : 'HEALTHY'  'host_status' : 'HEALTHY' }}After the agent is stopped for the host{ 'href' : 'http://c6403.ambari.apache.org:8080/api/v1/hosts/c6401.ambari.apache.org'  'Hosts' : { 'host_health_report' : ''  'host_name' : 'c6401.ambari.apache.org'  'host_state' : 'HEARTBEAT_LOST'  'host_status' : 'UNKNOWN' }}At this point I can perform AddHost and start the wizard. But AddHost will eventually fail - although now I see it just waiting.We need some form of error indication when host is not lost., ad host cluster manual instal start agent http c ambari apach org api v host c ambari apach org href http c ambari apach org api v host c ambari apach org host host health report host name c ambari apach org host state healthi host statu healthi after agent stop host href http c ambari apach org api v host c ambari apach org host host health report host name c ambari apach org host state heartbeat lost host statu unknown at point i perform add host start wizard but add host eventu fail although i see wait we need form error indic host lost,0,0,0,0,0,0,
6568,Service pluggability: New added services in the stack has Add Property link in log-4j and env categories, servic pluggabl new ad servic stack add properti link log j env categori,,,1,0,0,0,0,0,
6570,Fast user can skip 'Customize Services' step (add service wizard) while configs are not loaded, fast user skip custom servic step add servic wizard config load,STR Install cluster Go to Add Service Wizard Proceed to step 'Customize services' While configs are loading click 'Next'.If user tried to install services with configs that should be provided manually using UI (like Nagios)  he will get JS-error on next step., str instal cluster go add servic wizard proce step custom servic while config load click next if user tri instal servic config provid manual use ui like nagio get js error next step,0,0,0,0,0,0,
6571,'all/none' links affect already installed services on 'Select Services' step, none link affect alreadi instal servic select servic step,,,0,0,0,0,0,0,
6575,Linked Ganglia charts from Host Details page don't show any graphs (except on NameNode host), link ganglia chart host detail page show graph except name node host,The UI links to Ganglia Web to display the graphs for a specific host as http://&lt;ganglia-host&gt;/ganglia/mobile_helper.php?show_host_metrics=1&amp;h=&lt;target-fqdn&gt;&amp;c=HDPNameNode&amp;r=hour&amp;cs=&amp;ce=Note that c=HDPNameNode is static.I believe this worked before as the NameNode gmond was repeated on all the hosts (we should not have and this has been fixed but now it's causing this issue).For this ticket  let's change the call the UI makes from 'HDPNameNode' to 'HDPSlaves' and verify that host-level Ganglia links work for all hosts on a multi-node cluster (including hosts with clients only)., the ui link ganglia web display graph specif host http lt ganglia host gt ganglia mobil helper php show host metric amp h lt target fqdn gt amp c hdp name node amp r hour amp cs amp ce note c hdp name node static i believ work name node gmond repeat host fix caus issu for ticket let chang call ui make hdp name node hdp slave verifi host level ganglia link work host multi node cluster includ host client,1,0,0,0,0,0,
6585,Deleted hosts come back to life after ambari-server restart, delet host come back life ambari server restart,When attempting to delete a host through the UI  and then re-add it  the re-add operation fails because a record already exists in the clusterhostmapping table.This can be reproduced as follows (host names will change of course) 1. Create a cluster and add a host so that it is populated in the clusterhostmapping table.2. Make sure the agent is running.3. On the server  run ambari-server restart  and immediately run the following repeatedly in another terminal window before the restart finishes  curl --write-out %{http_code} --show-error -u admin:admin -H 'X-Requested-By:1' -i -X DELETE http://c6404.ambari.apache.org:8080/api/v1/clusters/dev/hosts/c6407.ambari.apache.orgHTTP/1.1 200 OKSet-Cookie: AMBARISESSIONID=z91px2l41uc6dwjv52zl2mcu;Path=/Expires: Thu  01 Jan 1970 00:00:00 GMTContent-Type: text/plainContent-Length: 0Server: Jetty(7.6.7.v20120910)4. Quickly verify that the host name is removed from the clusterhostmapping table.5. On the agent  run ambari-agent restart  and repeatedly requery the clusterhostmapping table  until the record is reinserted (should take no more than 30 seconds to appear).6. Run the curl command to attempt to re-add the host  and receive the error message curl --write-out %{http_code} --show-error -u admin:admin -H 'X-Requested-By:1' -i POST http://c6404.ambari.apache.org:8080/api/v1/clusters/dev/hosts/c6407.ambari.apache.orgHTTP/1.1 500 Exception [EclipseLink-4002] (Eclipse Persistence Services - 2.4.0.v20120608-r11652): org.eclipse.persistence.exceptions.DatabaseException Internal Exception: java.sql.BatchUpdateException: Batch entry 0 INSERT INTO ClusterHostMapping (cluster_id  host_name) VALUES (2  'c6407.ambari.apache.org') was aborted. Call getNextException to see the cause. Error Code: 0 Call: INSERT INTO ClusterHostMapping (cluster_id  host_name) VALUES (?  ?) bind =&gt; [2 parameters bound]Set-Cookie: AMBARISESSIONID=1je1wahcml82f11gjrserxgdyl;Path=/Content-Type: text/plain;charset=ISO-8859-1Content-Length: 530Server: Jetty(7.6.7.v20120910){ 'status': 500  'message': 'Exception [EclipseLink-4002] (Eclipse Persistence Services - 2.4.0.v20120608-r11652): org.eclipse.persistence.exceptions.DatabaseException/nInternal Exception: java.sql.BatchUpdateException: Batch entry 0 INSERT INTO ClusterHostMapping (cluster_id  host_name) VALUES (2  /u0027c6407.ambari.apache.org/u0027) was aborted. Call getNextException to see the cause./nError Code: 0/nCall: INSERT INTO ClusterHostMapping (cluster_id  host_name) VALUES (?  ?)/n/tbind /u003d/u003e [2 parameters bound]'At this point  here is the state of the tables.select * from clusterhostmapping where host_name = 'c6407.ambari.apache.org'; cluster_id | host_name------------+------------------------- 2 | c6407.ambari.apache.orgselect * from hoststate where host_name = 'c6407.ambari.apache.org'; agent_version | available_mem | current_state | health_status | host_name | time_in_state | maintenance_state---------------------+---------------+---------------+----------------------------------------------+-------------------------+---------------+------------------- {'version':'1.6.0'} | 250232 | INIT | {'healthStatus':'HEALTHY' 'healthReport':''} | c6407.ambari.apache.org | 1405718796141 | {'2':'ON'}I then deleted both records  restarted the server  and was then able to add the host successfully.This is a bug in the persistence layer., when attempt delet host ui add add oper fail record alreadi exist clusterhostmap tabl thi reproduc follow host name chang cours creat cluster add host popul clusterhostmap tabl make sure agent run on server run ambari server restart immedi run follow repeatedli anoth termin window restart finish curl write http code show error u admin admin h x request by x delet http c ambari apach org api v cluster dev host c ambari apach org http ok set cooki ambarisessionid z px l uc dwjv zl mcu path expir thu jan gmt content type text plain content length server jetti v quickli verifi host name remov clusterhostmap tabl on agent run ambari agent restart repeatedli requeri clusterhostmap tabl record reinsert take second appear run curl command attempt add host receiv error messag curl write http code show error u admin admin h x request by post http c ambari apach org api v cluster dev host c ambari apach org http except eclips link eclips persist servic v r org eclips persist except databas except intern except java sql batch updat except batch entri insert into cluster host map cluster id host name valu c ambari apach org abort call get next except see caus error code call insert into cluster host map cluster id host name valu bind gt paramet bound set cooki ambarisessionid je wahcml f gjrserxgdyl path content type text plain charset iso content length server jetti v statu messag except eclips link eclips persist servic v r org eclips persist except databas except n intern except java sql batch updat except batch entri insert into cluster host map cluster id host name valu u c ambari apach org u abort call get next except see caus n error code n call insert into cluster host map cluster id host name valu n tbind u u e paramet bound at point state tabl select clusterhostmap host name c ambari apach org cluster id host name c ambari apach orgselect hoststat host name c ambari apach org agent version avail mem current state health statu host name time state mainten state version init health statu healthi health report c ambari apach org on i delet record restart server abl add host success thi bug persist layer,1,0,0,0,0,0,
6587,Views: exception on server restart after creating view instance, view except server restart creat view instanc,1) installed the files view (built from source)2) I created an instance of the viewPOSThttp://c6401.ambari.apache.org:8080/api/v1/views/FILES/versions/0.1.0/instances/MyFiles[ {'ViewInstanceInfo' : { 'properties' : { 'dataworker.defaultFs' : 'webhdfs://c6401.ambari.apache.org:50070'  'dataworker.username' : 'ambari-qa' } }} ]3) I restart ambari-server and get this exception  so ambari-server can't start up. If I delete the view jar and restart  then I can get ambari-server to start.00:41:59 433 INFO [main] Server:266 - jetty-7.6.7.v2012091000:41:59 914 WARN [main] WebAppContext:489 - Failed startup of context o.e.j.w.WebAppContext{/views/FILES/0.1.0/MyFiles file:/var/lib/ambari-server/resources/views/work/FILES%7B0.1.0%7D/} /var/lib/ambari-server/resources/views/work/FILES{0.1.0}java.util.zip.ZipException: invalid entry size (expected 12027 but got 11985 bytes) at java.util.zip.ZipInputStream.readEnd(ZipInputStream.java:403) at java.util.zip.ZipInputStream.read(ZipInputStream.java:195), instal file view built sourc i creat instanc view po thttp c ambari apach org api v view file version instanc my file view instanc info properti datawork default fs webhdf c ambari apach org datawork usernam ambari qa i restart ambari server get except ambari server start if i delet view jar restart i get ambari server start info main server jetti v warn main web app context fail startup context e j w web app context view file my file file var lib ambari server resourc view work file b d var lib ambari server resourc view work file java util zip zip except invalid entri size expect got byte java util zip zip input stream read end zip input stream java java util zip zip input stream read zip input stream java,1,0,0,0,0,0,
6589,Management Console: UI Layout  Basic Routing and Create User Management Page (with mock data), manag consol ui layout basic rout creat user manag page mock data,,,1,0,0,0,0,0,
6603,Install wizard should have ability to load default 'final' configs  and save them, instal wizard abil load default final config save,In Ambari if a stack service has configs with final=true  then during install wizard these configs should have the final-checkbox checked. Also  if the user changes these checkboxes  the values should be stored in the properties_attributes of configs when Deploy is hit., in ambari stack servic config final true instal wizard config final checkbox check also user chang checkbox valu store properti attribut config deploy hit,0,0,0,0,0,0,
6609,Ambari-DDL-Postgres-CREATE.sql fix for CLUSTER.OPERATE, ambari ddl postgr creat sql fix cluster oper,,,0,0,0,0,0,0,
6622,BlueprintResourceProviderTest fails on exception message assertion, blueprint resourc provid test fail except messag assert,testDecidePopulationStrategy_unsupportedSchema(org.apache.ambari.server.controller.internal.BlueprintResourceProviderTest) Time elapsed: 0.02 sec &lt;&lt;&lt; FAILURE!java.lang.AssertionError:Expected: (exception with message a string containing 'Configuration definition schema is not supported' and an instance of java.lang.IllegalArgumentException) got: &lt;java.lang.IllegalArgumentException: 'Configuration format provided in Blueprint is not supported'&gt;, test decid popul strategi unsupport schema org apach ambari server control intern blueprint resourc provid test time elaps sec lt lt lt failur java lang assert error expect except messag string contain configur definit schema support instanc java lang illeg argument except got lt java lang illeg argument except configur format provid blueprint support gt,0,0,0,0,0,0,
6623,Fix unit tests that are broken after AMBARI-6533, fix unit test broken ambari,Unit tests are broken after commit f3aab68ec417d8e2c39c216962e1e1d47d56d401The root cause is the properties in InMemoryDefaultTestModule.java., unit test broken commit f aab ec e c c e e the root caus properti in memori default test modul java,0,0,0,0,0,0,
6626,HDP-1.3 stack shows HUE in select services page, hdp stack show hue select servic page,HUE is defined in HDP-1.3.2 stack with configuration sites but there is no agent scripts defining configure/stop/start/status functions for the service.HUE is not supported to work with Ambari. In that case we should not expose HUE via stack definition API as supported service of HDP-1.x stack., hue defin hdp stack configur site agent script defin configur stop start statu function servic hue support work ambari in case expos hue via stack definit api support servic hdp x stack,0,0,0,0,0,0,
6628,Cannot start ambari-server due to empty metainfo table, cannot start ambari server due empti metainfo tabl,Trying to deploy latest trunk build ambari-server-1.7.0-70 and hit the following when running ambari-server startambari-server.log01:30:53 074 INFO [main] AmbariServer:157 - ********* Meta Info initialized **********01:30:53 086 INFO [main] ClustersImpl:103 - Initializing the ClustersImpl01:30:53 679 INFO [main] AmbariManagementControllerImpl:230 - Initializing the AmbariManagementControllerImpl01:30:53 894 INFO [main] AmbariServer:487 - Checking DB store version01:30:53 894 WARN [main] AmbariServer:502 - Current database store version is not compatible with current server version  serverVersion=null  schemaVersion=null01:30:53 895 ERROR [main] AmbariServer:592 - Failed to run the Ambari Serverorg.apache.ambari.server.AmbariException: Current database store version is not compatible with current server version  serverVersion=null  schemaVersion=null at org.apache.ambari.server.controller.AmbariServer.checkDBVersion(AmbariServer.java:503) at org.apache.ambari.server.controller.AmbariServer.run(AmbariServer.java:164) at org.apache.ambari.server.controller.AmbariServer.main(AmbariServer.java:589)The metainfo table is empty with no values indicating version of Ambari installed., tri deploy latest trunk build ambari server hit follow run ambari server startambari server log info main ambari server meta info initi info main cluster impl initi cluster impl info main ambari manag control impl initi ambari manag control impl info main ambari server check db store version warn main ambari server current databas store version compat current server version server version null schema version null error main ambari server fail run ambari serverorg apach ambari server ambari except current databas store version compat current server version server version null schema version null org apach ambari server control ambari server check db version ambari server java org apach ambari server control ambari server run ambari server java org apach ambari server control ambari server main ambari server java the metainfo tabl empti valu indic version ambari instal,0,0,0,0,0,0,
6630,Add Service wizard: Retry installation functionality does not work, add servic wizard retri instal function work,API triggered on clicking Retry button results in failure with 400 status code, api trigger click retri button result failur statu code,0,0,0,0,1,0,
6638,Config categories aren't displayed on 'Configure Services' step of Secrity Wizard, config categori display configur servic step secriti wizard,See the screenshot attached.JS error Uncaught TypeError: Cannot read property 'filterProperty' of undefined at ambari-web/app/views/common/configs/services_config.js:338, see screenshot attach js error uncaught type error cannot read properti filter properti undefin ambari web app view common config servic config js,1,0,0,0,0,0,
6640,Memory leaks during tabs switching on 'Customize Services' page, memori leak tab switch custom servic page,Steps:Go to 'Customize services page'.Switch to other tab many a time (50 switches = about 100MB).Result: firefox browser gets 1.2 GB in memory., step go custom servic page switch tab mani time switch mb result firefox browser get gb memori,1,0,0,0,0,0,
6664,Oozie service can not be started on Ambari server with external Postgres, oozi servic start ambari server extern postgr,STR: Install single node cluster with Postgres 9 server as Ambari and Oozie DB (Stack - 2.0). Postgres server should be on dedicated host; Start all services.Actual result: Oozie server will not start.Error message for Oozie:2014-07-23 08:42:57 947 - Error while executing command 'start':Traceback (most recent call last): File '/usr/lib/python2.6/site-packages/resource_management/libraries/script/script.py'  line 111  in execute method(env) File '/var/lib/ambari-agent/cache/stacks/HDP/2.0.6/services/OOZIE/package/scripts/oozie_server.py'  line 43  in start oozie_service(action='start') File '/var/lib/ambari-agent/cache/stacks/HDP/2.0.6/services/OOZIE/package/scripts/oozie_service.py'  line 43  in oozie_service Execute( db_connection_check_command  tries=5  try_sleep=10) File '/usr/lib/python2.6/site-packages/resource_management/core/base.py'  line 148  in __init__ self.env.run() File '/usr/lib/python2.6/site-packages/resource_management/core/environment.py'  line 149  in run self.run_action(resource  action) File '/usr/lib/python2.6/site-packages/resource_management/core/environment.py'  line 115  in run_action provider_action() File '/usr/lib/python2.6/site-packages/resource_management/core/providers/system.py'  line 239  in action_run raise exFail: Execution of '/usr/jdk64/jdk1.7.0_45/bin/java -cp /usr/lib/ambari-agent/DBConnectionVerification.jar:/usr/lib/oozie/libserver/postgresql-9.0-801.jdbc4.jar org.apache.ambari.server.DBConnectionVerification jdbc:postgresql://&lt;host&gt;:5432/ooziedb oozieuser [PROTECTED] org.postgresql.Driver' returned 1. ERROR: Unable to connect to the DB. Please check DB connection properties.java.lang.ClassNotFoundException: org.postgresql.Driver, str instal singl node cluster postgr server ambari oozi db stack postgr server dedic host start servic actual result oozi server start error messag oozi error execut command start traceback recent call last file usr lib python site packag resourc manag librari script script py line execut method env file var lib ambari agent cach stack hdp servic oozi packag script oozi server py line start oozi servic action start file var lib ambari agent cach stack hdp servic oozi packag script oozi servic py line oozi servic execut db connect check command tri tri sleep file usr lib python site packag resourc manag core base py line init self env run file usr lib python site packag resourc manag core environ py line run self run action resourc action file usr lib python site packag resourc manag core environ py line run action provid action file usr lib python site packag resourc manag core provid system py line action run rais ex fail execut usr jdk jdk bin java cp usr lib ambari agent db connect verif jar usr lib oozi libserv postgresql jdbc jar org apach ambari server db connect verif jdbc postgresql lt host gt ooziedb oozieus protect org postgresql driver return error unabl connect db pleas check db connect properti java lang class not found except org postgresql driver,1,0,0,0,0,0,
6667,Unit test failures on jenkins for Ambari 1.7.0 related to alerts., unit test failur jenkin ambari relat alert,Tests run locally and pass 100% consistently.These test results are fishy; they randomly fail on different OS deployments. Even the simple logic ones fail. There are only 5 alert targets ever created  yet there are 9 returned sometimes. I'm wondering if this is because we tried to load some data in the @BeforeClass instead of @Before - maybe there's a weird Guice/JUnit race condition going on.I can't say I can fix this with confidence since I can't reproduce it. I'm going to move some things to @Before and hope it helps.java.lang.AssertionError: expected:&lt;5&gt; but was:&lt;9&gt; at org.junit.Assert.fail(Assert.java:93) at org.junit.Assert.failNotEquals(Assert.java:647) at org.junit.Assert.assertEquals(Assert.java:128) at org.junit.Assert.assertEquals(Assert.java:472) at org.junit.Assert.assertEquals(Assert.java:456) at org.apache.ambari.server.orm.dao.AlertDispatchDAOTest.testFindAllTargets(AlertDispatchDAOTest.java:117), test run local pass consist these test result fishi randomli fail differ os deploy even simpl logic one fail there alert target ever creat yet return sometim i wonder tri load data befor class instead befor mayb weird guic j unit race condit go i say i fix confid sinc i reproduc i go move thing befor hope help java lang assert error expect lt gt lt gt org junit assert fail assert java org junit assert fail not equal assert java org junit assert assert equal assert java org junit assert assert equal assert java org junit assert assert equal assert java org apach ambari server orm dao alert dispatch dao test test find all target alert dispatch dao test java,1,0,0,0,0,0,
6672,Oozie fails for stack 2.0 and 2.1, oozi fail stack,,,1,0,0,0,0,0,
6716,'Refresh configs' action doesn't work for Flume, refresh config action work flume,STR: Deployed cluster with Flume  without Flume Agents. Added Flume agent on each host. Configured 7 agents  assigned to different hosts. Changed config of one Flume agent. Clicked 'Refresh configs' action.Result: Nothing happened., str deploy cluster flume without flume agent ad flume agent host configur agent assign differ host chang config one flume agent click refresh config action result noth happen,0,0,0,0,0,0,
6721,Capacity Scheduler config cannot be saved, capac schedul config cannot save,STR: Go to YARN &gt; Config Under 'Scheduler' section  modify 'Capacity Scheduler' configs. Hit Save. The page reloads. The modifications are reverted back., str go yarn gt config under schedul section modifi capac schedul config hit save the page reload the modif revert back,0,0,0,0,0,0,
6729,RM HA wizard is experimental but shouldn't be, rm ha wizard experiment,Installed 1.7.0 build and RM HA is not available unless I go enable #experimental.Should not be experimental. RM HA is available with HDP 2.1+ Stack (but not with HDP 2.0.* stack)., instal build rm ha avail unless i go enabl experiment should experiment rm ha avail hdp stack hdp stack,1,0,0,0,0,0,
6735,Once RM HA is config'd  none of the RM summary info or RM dashboard widgets show data, onc rm ha config none rm summari info rm dashboard widget show data,Configure RM HA. None of the summary info shows in Services &gt; YARN &gt; Summary All of the RM Dashboard widgets show N/A, configur rm ha none summari info show servic gt yarn gt summari all rm dashboard widget show n a,0,0,0,0,0,0,
6741,On HDFS config page edit boxes with memory size values have incorrect behavior., on hdf config page edit box memori size valu incorrect behavior,Go to HDFS config page.Change Name Node java heap size.Save configs.'Successful' message appears  but on config page edit boxes have wrong values.After clicking 'OK' fields get right value again.Same issue reproduced on Nano but it looks like there it fails to save configs., go hdf config page chang name node java heap size save config success messag appear config page edit box wrong valu after click ok field get right valu same issu reproduc nano look like fail save config,1,0,0,0,0,0,
6744,'Select Host' page on Resource Manager HA Enabling wizard does not save selected values after next/back steps, select host page resourc manag ha enabl wizard save select valu next back step,STR:1) Deploy cluster2) Go to Enable Resourse Manager Enable HA wizard3) Go to 'Select Host' page4) Select some different from default value in combobox 'Additional Resourse manager'5) Click 'Next' and go to 'Review' page6) Click 'Back' and again go to 'Select Host' page.Actual result: value in 'Additional Resourse Manager' is default againExpected result: value in 'Additional Resourse Manager' is the same with choosen previously., str deploy cluster go enabl resours manag enabl ha wizard go select host page select differ default valu combobox addit resours manag click next go review page click back go select host page actual result valu addit resours manag default expect result valu addit resours manag choosen previous,1,0,0,0,1,0,
6749,Flume service should be in STARTED state when no agents configured, flume servic start state agent configur,When installing Flume service by default if we dont configure any agents  we end up with Flume being the only service shown in a red STOPPED state. For the case where there are no agents  this should be set to STARTED., when instal flume servic default dont configur agent end flume servic shown red stop state for case agent set start,1,0,0,0,0,0,
6751,HostNames and Slaves broken in two lines on Assign Slaves step, host name slave broken two line assign slave step,On Assign Slaves step  elements in the table got broken in two lines.see attached., on assign slave step element tabl got broken two line see attach,0,0,0,0,0,0,
6754,Resource Manager HA: after enabling RM HA  UI does not display standby and active Resource Managers on Summary tab, resourc manag ha enabl rm ha ui display standbi activ resourc manag summari tab,STR:1) Deploy 3-node cluster2) Enable RM HA3) Go to YARN Service page  Summary tabResult: There are not Resource Managers on Summary tab4) Select config tab and after that return back to Summary tabResult: Summary tab is as expected, str deploy node cluster enabl rm ha go yarn servic page summari tab result there resourc manag summari tab select config tab return back summari tab result summari tab expect,1,0,0,0,0,0,
6769,Add security configs on Add service wizard, add secur config add servic wizard,add service wizard doesn't add secure cinfigs, add servic wizard add secur cinfig,1,0,0,0,0,0,
6782,Wizard: Adding Master component does not create and install the master host component, wizard ad master compon creat instal master host compon,STR: On 3-node cluster  Go to Assign Masters page. Add another HBase Master. (Note: HBase Master and ZK server are only addable components.) Go forward to the review page. Review page will show correct information that 2 HBase master are selected for installation Eventually on deploying the cluster installation  HBase Master is only created and installed on the host that was a default selection on Assign Master page, str on node cluster go assign master page add anoth h base master note h base master zk server addabl compon go forward review page review page show correct inform h base master select instal eventu deploy cluster instal h base master creat instal host default select assign master page,0,0,0,0,0,0,
6787,Cleanup of Cluster > Admin tab, cleanup cluster admin tab,1. Remove 'Users' and 'Access' category from 'Admin' tab on the main menu.2. Rename 'Clusters' --&gt; 'Repositories'3. Rename 'misc' --&gt; 'Service Accounts', remov user access categori admin tab main menu renam cluster gt repositori renam misc gt servic account,1,0,0,0,0,0,
6788,Ambari installation webhcat templeton.hive.properties set thrift host name to be localhost, ambari instal webhcat templeton hive properti set thrift host name localhost,PROBLEM:default installation of Ambari sets webhcat-site.xmltempleton.hive.properties=hive.metastore.local=false  hive.metastore.uris=thrift://localhost:9933  hive.metastore.sasl.enabled=falsethe proper value should be the FQDN of the thrift host name  plus hive.metastore.execute.setugi=truefor example:hive.metastore.local=false  hive.metastore.uris=thrift://this.fqdn.com:9933  hive.metastore.sasl.enabled=false hive.metastore.execute.setugi=true, problem default instal ambari set webhcat site xmltempleton hive properti hive metastor local fals hive metastor uri thrift localhost hive metastor sasl enabl falseth proper valu fqdn thrift host name plu hive metastor execut setugi truefor exampl hive metastor local fals hive metastor uri thrift fqdn com hive metastor sasl enabl fals hive metastor execut setugi true,0,0,0,0,0,0,
6793,Need ResourceManager UI Quicklinks for active / standby, need resourc manag ui quicklink activ standbi,Similar to NN HA., similar nn ha,0,0,0,0,0,0,
